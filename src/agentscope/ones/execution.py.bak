# -*- coding: utf-8 -*-
"""Execution loop tying all sections together (III)."""
from __future__ import annotations

import re
from dataclasses import dataclass, field
import asyncio
from datetime import datetime
from typing import Any, Callable, Dict, TYPE_CHECKING

from .intent import (
    AcceptanceCriteria,
    AssistantOrchestrator,
    IntentRequest,
    StrategyPlan,
)
from .kpi import KPIRecord, KPITracker
from .memory import (
    DecisionCategory,
    MemoryEntry,
    MemoryPool,
    ProjectDescriptor,
    ProjectMemory,
    ProjectPool,
    ResourceLibrary,
)
from .task_graph import TaskGraphBuilder, TaskGraph
from ._task_validator import TaskLevelValidator, TaskValidationResult
from .task_board import ProjectTaskBoard, TaskItem, TaskItemStatus
from .msghub import MsgHubBroadcaster, RoundUpdate
from .artifacts import ArtifactDeliveryManager, ArtifactDeliveryResult
from .repair_engine import RepairEngine, RepairAction, FilePatch
from .dependency_sync import DependencySynchronizer, DependencyInfo, SyncResult
from .blueprint_enhancer import BlueprintEnhancer, EnhancedBlueprint, RequiredFunction
from ..message import Msg
from ..pipeline import MsgHub
from ..memory import MemoryBase
from .._logging import logger
import shortuuid
import time as _time_module


def _log_execution(
    msg: str,
    *,
    level: str = "info",
    prefix: str = "[ExecutionLoop]",
    hub: "Any | None" = None,
    event_type: str | None = None,
    metadata: dict | None = None,
    project_id: str | None = None,
) -> None:
    """Log execution progress with immediate flush for observability.

    Args:
        msg: Message to log.
        level: Log level (info, warning, error).
        prefix: Log prefix (default: [ExecutionLoop]).
        hub: Optional ObservabilityHub instance for timeline tracking.
        event_type: Optional timeline event type for ObservabilityHub.
        metadata: Optional metadata for timeline event.
        project_id: Optional project ID for timeline event.
    """
    timestamp = _time_module.strftime("%H:%M:%S")
    formatted = f"{timestamp} | {prefix} {msg}"

    # Print with flush for immediate visibility
    print(formatted, flush=True)

    # Also log via logger for file logging
    if level == "warning":
        logger.warning(msg)
    elif level == "error":
        logger.error(msg)
    else:
        logger.info(msg)

    # Record to ObservabilityHub if provided
    if hub is not None and event_type is not None:
        try:
            from datetime import datetime
            from ..observability._types import TimelineEvent

            event = TimelineEvent(
                timestamp=datetime.now(),
                event_type=event_type,  # type: ignore[arg-type]
                project_id=project_id,
                agent_id="execution_loop",
                metadata={"message": msg, **(metadata or {})},
            )
            hub.record_timeline_event(event)
        except Exception:
            pass  # Don't fail on observability errors


if TYPE_CHECKING:
    from .acceptance_agent import AcceptanceAgent, AcceptanceResult
    from .project_context import ProjectContext, RoundFeedback
    from .manifest import AgentManifest


@dataclass
class ExecutionEnhancer:
    """Execution enhancer that integrates repair, dependency, and blueprint tools.

    This class wraps the RepairEngine, DependencySynchronizer, and BlueprintEnhancer
    to provide enhanced execution capabilities based on agent manifests.

    Attributes:
        repair_engine (`RepairEngine | None`):
            Engine for analyzing and repairing errors.
        dependency_sync (`DependencySynchronizer | None`):
            Synchronizer for managing dependencies.
        blueprint_enhancer (`BlueprintEnhancer | None`):
            Enhancer for blueprint generation.
    """

    repair_engine: RepairEngine | None = None
    dependency_sync: DependencySynchronizer | None = None
    blueprint_enhancer: BlueprintEnhancer | None = None

    @classmethod
    def from_manifest(cls, manifest: "AgentManifest") -> "ExecutionEnhancer":
        """Create an ExecutionEnhancer from an agent manifest.

        Args:
            manifest: The agent manifest with configuration.

        Returns:
            Configured ExecutionEnhancer instance.
        """
        enhancer = cls()

        # Initialize components based on manifest configuration
        if manifest.repair_config:
            enhancer.repair_engine = RepairEngine(manifest)

        if manifest.dependency_config:
            enhancer.dependency_sync = DependencySynchronizer(manifest)

        if manifest.blueprint_config:
            enhancer.blueprint_enhancer = BlueprintEnhancer(manifest)

        return enhancer

    def analyze_error(self, error_message: str) -> list[RepairAction]:
        """Analyze an error and return repair actions.

        Args:
            error_message: The error message to analyze.

        Returns:
            List of repair actions.
        """
        if self.repair_engine:
            return self.repair_engine.analyze_error(error_message)
        return []

    def build_repair_prompt(
        self,
        error_message: str,
        actions: list[RepairAction],
        affected_files: dict[str, str] | None = None,
    ) -> str:
        """Build a repair prompt for the LLM.

        Args:
            error_message: The error message.
            actions: Repair actions from analysis.
            affected_files: Optional affected file contents.

        Returns:
            Prompt string for repair.
        """
        if self.repair_engine:
            return self.repair_engine.build_repair_prompt(
                error_message, actions, affected_files
            )
        return f"ËØ∑‰øÆÂ§ç‰ª•‰∏ãÈîôËØØ:\n\n```\n{error_message}\n```"

    def extract_dependencies(
        self,
        code_content: str,
        language: str | None = None,
    ) -> list[DependencyInfo]:
        """Extract dependencies from code content.

        Args:
            code_content: The source code.
            language: Optional language hint.

        Returns:
            List of extracted dependencies.
        """
        if self.dependency_sync:
            return self.dependency_sync.extract_dependencies_from_code(
                code_content, language
            )
        return []

    def find_missing_dependencies(
        self,
        required: list[DependencyInfo],
        dep_file_content: str,
    ) -> list[DependencyInfo]:
        """Find missing dependencies.

        Args:
            required: Required dependencies.
            dep_file_content: Current dependency file content.

        Returns:
            List of missing dependencies.
        """
        if self.dependency_sync:
            return self.dependency_sync.find_missing_dependencies(
                required, dep_file_content
            )
        return []

    async def sync_dependencies(
        self,
        code_files: dict[str, str],
        workspace: Any,
    ) -> SyncResult:
        """Synchronize dependencies from code to dependency file.

        Args:
            code_files: Dict of file paths to content.
            workspace: The workspace to update.

        Returns:
            Sync result with details.
        """
        if self.dependency_sync:
            return await self.dependency_sync.sync_dependencies(code_files, workspace)
        return SyncResult(success=False, errors=["No dependency synchronizer configured"])

    async def enhance_blueprint(
        self,
        requirement: str,
        criteria: list,
        original_blueprint: dict[str, Any] | None = None,
    ) -> EnhancedBlueprint:
        """Enhance a blueprint with required functions.

        Args:
            requirement: The requirement description.
            criteria: List of acceptance criteria.
            original_blueprint: Optional original blueprint.

        Returns:
            Enhanced blueprint with function requirements.
        """
        if self.blueprint_enhancer:
            return await self.blueprint_enhancer.analyze_and_enhance(
                requirement, criteria, original_blueprint
            )
        return EnhancedBlueprint()

    def build_enhanced_prompt(
        self,
        requirement: str,
        criteria: list,
        enhanced: EnhancedBlueprint,
    ) -> str:
        """Build an enhanced generation prompt.

        Args:
            requirement: The requirement.
            criteria: Acceptance criteria.
            enhanced: Enhanced blueprint.

        Returns:
            Enhanced prompt string.
        """
        if self.blueprint_enhancer:
            return self.blueprint_enhancer.build_enhanced_prompt(
                requirement, criteria, enhanced
            )
        return requirement


@dataclass
class AgentOutput:
    """Captures agent execution result for context passing.

    Attributes:
        agent_id (`str`):
            The agent ID that produced this output.
        node_id (`str`):
            The task node ID.
        content (`str`):
            The output content.
        success (`bool`):
            Whether execution succeeded.
        execution_id (`str | None`):
            The execution ID for timeline tracking.
        start_time (`datetime | None`):
            When execution started.
        end_time (`datetime | None`):
            When execution ended.
        duration_ms (`float | None`):
            Duration in milliseconds.
    """

    agent_id: str
    node_id: str
    content: str
    success: bool = True
    execution_id: str | None = None
    start_time: datetime | None = None
    end_time: datetime | None = None
    duration_ms: float | None = None


@dataclass
class ExecutionContext:
    """Accumulated context from previous agent executions."""

    intent_utterance: str
    agent_outputs: list[AgentOutput] = field(default_factory=list)
    shared_artifacts: dict[str, str] = field(default_factory=dict)
    project_memory: ProjectMemory | None = None
    round_feedback: "RoundFeedback | None" = None

    def add_output(self, output: AgentOutput) -> None:
        """Add an agent output to the context."""
        self.agent_outputs.append(output)

    def set_round_feedback(self, feedback: "RoundFeedback") -> None:
        """Set feedback from the previous round.

        Args:
            feedback: The round feedback to include in prompts.
        """
        self.round_feedback = feedback

    def build_prompt(self, current_node_id: str, requirement_desc: str) -> str:
        """Build a prompt with accumulated context for the next agent.

        This method constructs a comprehensive prompt that includes:
        1. User requirement
        2. Project memory (technology decisions and constraints)
        3. Previous round feedback (if available)
        4. Previous agent outputs
        5. Current task description

        The project memory section is critical for maintaining consistency
        across multiple rounds of code generation.
        """
        lines = [
            f"## Áî®Êà∑ÈúÄÊ±Ç\n{self.intent_utterance}",
        ]

        # Add project memory context (critical for consistency)
        if self.project_memory:
            memory_context = self.project_memory.get_context_for_prompt()
            if memory_context.strip():
                lines.append(f"\n{memory_context}")

        # Add previous round feedback (critical for iterative improvement)
        if self.round_feedback and not self.round_feedback.is_successful():
            feedback_prompt = self.round_feedback.build_feedback_prompt()
            if feedback_prompt.strip():
                lines.append(f"\n{feedback_prompt}")

        lines.append(
            f"\n## ÂΩìÂâç‰ªªÂä°\n‰ªªÂä°ID: {current_node_id}\nË¶ÅÊ±Ç: {requirement_desc}"
        )

        if self.agent_outputs:
            lines.append("\n## ÂâçÂ∫è‰ªªÂä°ËæìÂá∫")
            for output in self.agent_outputs:
                status = "‚úì" if output.success else "‚úó"
                lines.append(f"\n### [{status}] {output.node_id} ({output.agent_id})")
                lines.append(output.content)

        if self.shared_artifacts:
            lines.append("\n## ÂÖ±‰∫´‰∫ßÁâ©")
            for name, value in self.shared_artifacts.items():
                lines.append(f"- {name}: {value}")

        lines.append(
            "\n## Êåá‰ª§\n"
            "ËØ∑Âü∫‰∫é‰ª•‰∏ä‰∏ä‰∏ãÊñáÂÆåÊàêÂΩìÂâç‰ªªÂä°ÔºåËæìÂá∫ÁªìÊûÑÂåñÁªìÊûú„ÄÇ\n\n"
            "### È™åËØÅË¶ÅÊ±ÇÔºàÈáçË¶ÅÔºâ\n"
            "ÂÆåÊàê‰ª£Á†ÅÁºñÂÜôÂêéÔºå‰Ω†**ÂøÖÈ°ª**‰∏ªÂä®È™åËØÅ‰ª£Á†ÅÁöÑÊ≠£Á°ÆÊÄßÔºö\n"
            "1. ËøêË°åÈÄÇÂΩìÁöÑÂëΩ‰ª§È™åËØÅ‰ª£Á†ÅÂèØ‰ª•Ë¢´Ê≠£Á°ÆÂä†ËΩΩ/ÁºñËØëÔºàÂ¶ÇÂØºÂÖ•ÊµãËØï„ÄÅÊûÑÂª∫ÂëΩ‰ª§Á≠âÔºâ\n"
            "2. Â¶ÇÊûúÈ°πÁõÆÊúâÊµãËØïÊ°ÜÊû∂ÔºåËøêË°åÁõ∏ÂÖ≥ÊµãËØï\n"
            "3. Â¶ÇÊûúÈ™åËØÅÂ§±Ë¥•ÔºåÁ´ãÂç≥‰øÆÂ§çÈóÆÈ¢òÂêéÂÜçÊ¨°È™åËØÅ\n"
            "4. Âè™ÊúâÈ™åËØÅÈÄöËøáÂêéÊâçÁÆó‰ªªÂä°ÂÆåÊàê\n\n"
            "È™åËØÅÊñπÂºèÁî±‰Ω†Ê†πÊçÆÈ°πÁõÆÁ±ªÂûãËá™Ë°åÂÜ≥ÂÆöÔºåÁ°Æ‰øù‰ª£Á†ÅÂèØ‰ª•Ê≠£Â∏∏ËøêË°å„ÄÇ"
        )
        return "\n".join(lines)


@dataclass
class ExecutionReport:
    project_id: str | None
    accepted: bool
    kpi: KPIRecord
    task_status: dict[str, str]
    plan: StrategyPlan
    deliverable: ArtifactDeliveryResult | None = None
    agent_outputs: list[AgentOutput] = field(default_factory=list)
    acceptance_result: "AcceptanceResult | None" = None  # LLM-driven validation


class ExecutionLoop:
    """Execution loop with agent interaction support.

    This class orchestrates multi-agent task execution with:
    - Context passing: Previous agent outputs are passed to subsequent agents
    - MsgHub integration: Agents can broadcast and observe messages
    - Shared memory: Optional shared memory for cross-agent state
    """

    def __init__(
        self,
        *,
        project_pool: ProjectPool,
        memory_pool: MemoryPool,
        resource_library: ResourceLibrary,
        orchestrator: AssistantOrchestrator,
        task_graph_builder: TaskGraphBuilder,
        kpi_tracker: KPITracker,
        msg_hub_factory: Callable[[str], MsgHubBroadcaster] | None = None,
        delivery_manager: ArtifactDeliveryManager | None = None,
        max_rounds: int = 3,
        shared_memory: MemoryBase | None = None,
        enable_agent_msghub: bool = True,
        acceptance_agent: "AcceptanceAgent | None" = None,
        workspace_dir: str = "/workspace",
        enable_project_context: bool = True,
        enable_parallel_execution: bool = False,
        parallel_timeout_seconds: float = 300.0,
        enable_task_validation: bool = True,
        max_task_retries: int = 2,
        task_validation_timeout: float = 30.0,
        enable_active_validation: bool = True,
        active_validation_timeout: float = 120.0,
    ) -> None:
        self.project_pool = project_pool
        self.memory_pool = memory_pool
        self.resource_library = resource_library
        self.orchestrator = orchestrator
        self.task_graph_builder = task_graph_builder
        self.kpi_tracker = kpi_tracker
        self.msg_hub_factory = msg_hub_factory
        self.delivery_manager = delivery_manager
        self.max_rounds = max_rounds
        self.shared_memory = shared_memory
        self.enable_agent_msghub = enable_agent_msghub
        self.acceptance_agent = acceptance_agent
        self.workspace_dir = workspace_dir
        self.enable_project_context = enable_project_context
        self.enable_parallel_execution = enable_parallel_execution
        self.parallel_timeout_seconds = parallel_timeout_seconds
        # Task-level validation settings
        self.enable_task_validation = enable_task_validation
        self.max_task_retries = max_task_retries
        self.task_validation_timeout = task_validation_timeout
        # Active validation: call Claude Code to verify task results
        self.enable_active_validation = enable_active_validation
        self.active_validation_timeout = active_validation_timeout
        # Task-level validator (created lazily)
        self._task_validator: TaskLevelValidator | None = None
        # Project task boards for tracking agent-level progress
        self._project_task_boards: Dict[str, "ProjectTaskBoard"] = {}
        # Project memories for cross-agent context sharing
        self._project_memories: Dict[str, ProjectMemory] = {}
        # Enhanced project contexts for file tracking and validation
        self._project_contexts: Dict[str, "ProjectContext"] = {}
        # Execution enhancers per agent (keyed by agent_id)
        self._agent_enhancers: Dict[str, ExecutionEnhancer] = {}
        # Agent cache to avoid dynamic re-creation (NEW)
        self._agent_cache: Dict[str, Any] = {}

    def get_project_memory(self, project_id: str) -> ProjectMemory:
        """Get or create project memory for a project.

        Args:
            project_id: The project identifier.

        Returns:
            ProjectMemory instance for the project.
        """
        if project_id not in self._project_memories:
            self._project_memories[project_id] = ProjectMemory(
                project_id=project_id,
                workspace_dir=self.workspace_dir,
            )
        return self._project_memories[project_id]

    def get_project_context(self, project_id: str) -> "ProjectContext":
        """Get or create enhanced project context for a project.

        The ProjectContext provides file tracking, dependency validation,
        and structured feedback generation.

        Args:
            project_id: The project identifier.

        Returns:
            ProjectContext instance for the project.
        """
        if project_id not in self._project_contexts:
            from .project_context import create_project_context

            project_memory = self.get_project_memory(project_id)
            self._project_contexts[project_id] = create_project_context(
                project_id=project_id,
                workspace_dir=self.workspace_dir,
                project_memory=project_memory,
            )
        return self._project_contexts[project_id]

    def register_agent_enhancer(
        self,
        agent_id: str,
        manifest: "AgentManifest",
    ) -> ExecutionEnhancer:
        """Register an execution enhancer for an agent.

        Creates an ExecutionEnhancer from the agent's manifest and stores
        it for use during execution. The enhancer provides repair, dependency
        sync, and blueprint enhancement capabilities.

        Args:
            agent_id: The agent identifier.
            manifest: The agent's manifest with configuration.

        Returns:
            The created ExecutionEnhancer.
        """
        enhancer = ExecutionEnhancer.from_manifest(manifest)
        self._agent_enhancers[agent_id] = enhancer
        logger.debug(
            "Registered enhancer for agent %s (repair=%s, deps=%s, blueprint=%s)",
            agent_id,
            enhancer.repair_engine is not None,
            enhancer.dependency_sync is not None,
            enhancer.blueprint_enhancer is not None,
        )
        return enhancer

    def get_agent_enhancer(self, agent_id: str) -> ExecutionEnhancer | None:
        """Get the execution enhancer for an agent.

        Args:
            agent_id: The agent identifier.

        Returns:
            The ExecutionEnhancer if registered, None otherwise.
        """
        return self._agent_enhancers.get(agent_id)

    def get_task_validator(self) -> TaskLevelValidator:
        """Get or create the task-level validator.

        Returns:
            TaskLevelValidator instance for immediate task validation.
        """
        if self._task_validator is None:
            # Try to get container context if available
            container_id = None
            container_workspace = "/workspace/working"
            try:
                from agentscope.scripts._claude_code import get_container_context

                container_id, container_workspace = get_container_context()
            except ImportError:
                pass

            self._task_validator = TaskLevelValidator(
                workspace_dir=self.workspace_dir,
                container_id=container_id,
                container_workspace=container_workspace,
                validation_timeout=self.task_validation_timeout,
            )
        return self._task_validator

    async def _handle_execution_error(
        self,
        agent_id: str,
        error_message: str,
        project_context: "ProjectContext | None",
    ) -> str | None:
        """Handle an execution error using the agent's repair engine.

        This method analyzes the error using the agent's configured repair
        patterns and generates a repair prompt with specific hints.

        Args:
            agent_id: The agent that produced the error.
            error_message: The error message.
            project_context: Optional project context for file access.

        Returns:
            A repair prompt if the error can be analyzed, None otherwise.
        """
        enhancer = self.get_agent_enhancer(agent_id)
        if enhancer is None or enhancer.repair_engine is None:
            return None

        # Analyze the error
        actions = enhancer.analyze_error(error_message)
        if not actions:
            logger.debug("No repair actions found for error: %s", error_message[:100])
            return None

        _log_execution(
            f"Found {len(actions)} repair actions for error in agent {agent_id}",
            prefix="[RepairEngine]",
        )

        # Get affected files if project context is available
        affected_files: dict[str, str] = {}
        if project_context:
            for action in actions:
                for sync_file in action.sync_files:
                    content = project_context.get_file_content(sync_file)
                    if content:
                        affected_files[sync_file] = content

        # Build repair prompt
        return enhancer.build_repair_prompt(error_message, actions, affected_files)

    async def _sync_project_dependencies(
        self,
        project_id: str,
        agent_id: str,
        project_context: "ProjectContext",
    ) -> SyncResult | None:
        """Synchronize dependencies for a project.

        Extracts imports from generated code files and ensures they are
        present in the dependency file (requirements.txt, package.json, etc).

        Args:
            project_id: The project identifier.
            agent_id: The agent that generated the code.
            project_context: The project context with file tracking.

        Returns:
            SyncResult if synchronization was attempted, None otherwise.
        """
        enhancer = self.get_agent_enhancer(agent_id)
        if enhancer is None or enhancer.dependency_sync is None:
            return None

        # Gather all code files from project context
        code_files: dict[str, str] = {}
        for path in project_context.list_files():
            content = project_context.get_file_content(path)
            if content and (
                path.endswith(".py") or
                path.endswith(".js") or
                path.endswith(".ts") or
                path.endswith(".go")
            ):
                code_files[path] = content

        if not code_files:
            return None

        # Create a simple workspace wrapper for file operations
        class WorkspaceWrapper:
            def __init__(self, ctx: "ProjectContext"):
                self._ctx = ctx

            def read_file(self, path: str) -> str:
                return self._ctx.get_file_content(path) or ""

            def write_file(self, path: str, content: str) -> None:
                self._ctx.register_file(
                    path=path,
                    content=content,
                    created_by="DependencySynchronizer",
                    description="Auto-generated dependency update",
                )

        workspace = WorkspaceWrapper(project_context)
        result = await enhancer.sync_dependencies(code_files, workspace)

        if result.added:
            _log_execution(
                f"Added {len(result.added)} dependencies to {result.dependency_file}: {[d.name for d in result.added]}",
                prefix="[DependencySync]",
            )

        return result

    def _parse_agent_output_files(
        self,
        output: AgentOutput,
        project_context: "ProjectContext",
    ) -> list[str]:
        """Parse agent output to extract and register generated files.

        This method detects file content blocks in agent output and
        registers them with the ProjectContext for tracking and validation.

        Common output patterns supported:
        - ```filename.ext\\n...content...\\n```
        - **File: path/to/file**\\n```lang\\n...content...\\n```
        - <file path="...">...content...</file>
        - JSON format: {"files": [{"path": "...", "content": "..."}]}

        Args:
            output: The agent output to parse.
            project_context: The project context to register files with.

        Returns:
            List of file paths that were registered.
        """
        registered_files = []
        content = output.content

        # Pattern 1: **File: path/to/file**\n```lang\ncontent\n```
        file_block_pattern = re.compile(
            r'\*\*File:\s*([^\*\n]+)\*\*\s*\n```(?:\w+)?\n(.*?)```',
            re.DOTALL
        )
        for match in file_block_pattern.finditer(content):
            path = match.group(1).strip()
            file_content = match.group(2)
            if path and file_content:
                project_context.register_file(
                    path=path,
                    content=file_content,
                    created_by=output.agent_id,
                    description=f"Generated in node {output.node_id}",
                )
                registered_files.append(path)

        # Pattern 2: <file path="...">content</file>
        xml_file_pattern = re.compile(
            r'<file\s+path=["\']([^"\']+)["\']>(.*?)</file>',
            re.DOTALL
        )
        for match in xml_file_pattern.finditer(content):
            path = match.group(1).strip()
            file_content = match.group(2)
            if path and file_content and path not in registered_files:
                project_context.register_file(
                    path=path,
                    content=file_content,
                    created_by=output.agent_id,
                    description=f"Generated in node {output.node_id}",
                )
                registered_files.append(path)

        # Pattern 3: JSON format with files array
        try:
            import json
            # Look for JSON blocks in the content
            json_pattern = re.compile(r'```json\n(.*?)```', re.DOTALL)
            for match in json_pattern.finditer(content):
                try:
                    data = json.loads(match.group(1))
                    files = data.get("files", [])
                    for file_info in files:
                        path = file_info.get("path", "")
                        file_content = file_info.get("content", "")
                        if path and file_content and path not in registered_files:
                            project_context.register_file(
                                path=path,
                                content=file_content,
                                created_by=output.agent_id,
                                description=file_info.get(
                                    "description",
                                    f"Generated in node {output.node_id}"
                                ),
                            )
                            registered_files.append(path)
                except json.JSONDecodeError:
                    continue
        except Exception:
            pass

        # Pattern 4: Simple code blocks with filename comment
        # ```python\n# filename: path/to/file.py\ncontent\n```
        simple_pattern = re.compile(
            r'```(\w+)\n#\s*(?:filename|file):\s*([^\n]+)\n(.*?)```',
            re.DOTALL
        )
        for match in simple_pattern.finditer(content):
            path = match.group(2).strip()
            file_content = match.group(3)
            if path and file_content and path not in registered_files:
                project_context.register_file(
                    path=path,
                    content=file_content,
                    created_by=output.agent_id,
                    description=f"Generated in node {output.node_id}",
                )
                registered_files.append(path)

        if registered_files:
            _log_execution(
                f"Registered {len(registered_files)} files from agent {output.agent_id}: {', '.join(registered_files[:5])}",
                prefix="[FileRegistry]",
            )

        return registered_files

    def _register_dependency_file(
        self,
        path: str,
        content: str,
        project_context: "ProjectContext",
    ) -> None:
        """Register a dependency file (package.json, requirements.txt, etc.).

        Args:
            path: The file path.
            content: The file content.
            project_context: The project context.
        """
        dependency_files = {
            "package.json",
            "requirements.txt",
            "pyproject.toml",
            "Pipfile",
            "Cargo.toml",
            "go.mod",
        }
        filename = path.split("/")[-1]
        if filename in dependency_files:
            project_context.declare_dependencies_from_file(path, content)
            _log_execution(
                f"Declared dependencies from {path}",
                prefix="[DependencySync]",
            )

    def _get_runtime_agents(self) -> dict[str, object]:
        """Get all registered runtime agents."""
        return getattr(self.orchestrator, "_runtime_agents", {})

    def _get_workspace_for_collaborative(self) -> Any:
        """Get RuntimeWorkspaceWithPR instance for collaborative execution.

        Collaborative execution requires a workspace with git worktree support.
        This method tries to find an existing workspace or returns None.

        Returns:
            RuntimeWorkspaceWithPR instance or None if not available.
        """
        # Try to get workspace from orchestrator or context
        workspace = getattr(self.orchestrator, "_workspace", None)

        if workspace is not None:
            # Check if it's a RuntimeWorkspaceWithPR with git support
            if hasattr(workspace, "init_git_repo") and hasattr(
                workspace, "create_agent_worktree"
            ):
                return workspace

        # Try to create one if we have the necessary info
        try:
            from agentscope.scripts._runtime_workspace import RuntimeWorkspaceWithPR

            # Check if there's a container_id available
            container_id = getattr(self.orchestrator, "_container_id", None)
            if container_id:
                workspace = RuntimeWorkspaceWithPR(
                    base_workspace_dir=self.workspace_dir,
                    enable_pr_mode=True,
                )
                workspace.container_id = container_id
                return workspace
        except ImportError:
            pass

        return None

    def get_task_board(self, project_id: str) -> ProjectTaskBoard:
        """Get or create project task board.

        Args:
            project_id: The project identifier.

        Returns:
            The project's task board.
        """
        if project_id not in self._project_task_boards:
            self._project_task_boards[project_id] = ProjectTaskBoard(
                project_id=project_id
            )
        return self._project_task_boards[project_id]

    def _init_task_board_from_plan(
        self,
        project_id: str,
        plan: StrategyPlan,
        acceptance: AcceptanceCriteria,
    ) -> ProjectTaskBoard:
        """Initialize task board from strategy plan.

        Args:
            project_id: The project identifier.
            plan: The strategy plan with agent assignments.
            acceptance: Acceptance criteria for the project.

        Returns:
            Initialized project task board.
        """
        board = self.get_task_board(project_id)
        board.set_acceptance_criteria(acceptance)

        # Create tasks from plan rankings
        for node_id, ranking in plan.rankings.items():
            agent_id = ranking.profile.agent_id
            agent_name = ranking.profile.name

            # Create task item
            task = TaskItem(
                task_id=node_id,
                description=f"Task for {node_id}",
                assigned_to=agent_id,
            )

            # Add to board and assign to agent
            board.assign_task_to_agent(
                task=task,
                agent_id=agent_id,
                agent_name=agent_name,
                role=node_id,
            )

        return board

    def _update_task_board_status(
        self,
        project_id: str,
        task_status: Dict[str, str],
    ) -> None:
        """Update task board from execution results.

        Args:
            project_id: The project identifier.
            task_status: Dict of task_id -> status string.
        """
        board = self.get_task_board(project_id)
        board.sync_from_task_graph(task_status)

        # Also update agent task boards
        for task_id, status_str in task_status.items():
            if task_id in board.global_tasks:
                task = board.global_tasks[task_id]
                agent_id = task.assigned_to
                if agent_id and agent_id in board.agent_boards:
                    agent_board = board.agent_boards[agent_id]
                    if status_str == "running":
                        agent_board.start_task(task_id)
                    elif status_str == "completed":
                        agent_board.complete_task(task_id)
                    elif status_str == "failed":
                        agent_board.fail_task(task_id)

    def _update_agent_scores(
        self,
        plan: "StrategyPlan",
        task_status: dict[str, str],
        pass_ratio: float,
    ) -> None:
        """Update agent scores based on task completion results.

        This implements the cold-start learning mechanism where agent
        performance scores are updated based on actual task outcomes.

        Args:
            plan: The strategy plan containing agent rankings.
            task_status: Status of each task (completed/failed/etc).
            pass_ratio: Overall pass ratio for quality estimation.
        """
        from ..aa import AgentProfile

        for node_id, status in task_status.items():
            if node_id not in plan.rankings:
                continue

            ranking = plan.rankings[node_id]
            profile: AgentProfile = ranking.profile

            # Determine if task was successful
            accepted = status == "completed"

            # Use pass_ratio as a proxy for quality score
            # In real implementation, this could be more sophisticated
            quality_score = pass_ratio if accepted else 0.3

            # Update the agent's performance score
            profile.update_after_task(
                quality_score=quality_score,
                accepted=accepted,
            )

    def _invoke_agent(
        self,
        agent_id: str,
        prompt: str,
        *,
        context: ExecutionContext | None = None,
        node_id: str | None = None,
    ) -> AgentOutput | None:
        """Invoke an agent with context from previous executions.

        Args:
            agent_id: The agent identifier
            prompt: Base prompt (usually the user utterance)
            context: Accumulated context from previous agent executions
            node_id: Current task node identifier

        Returns:
            AgentOutput capturing the result, or None if agent not found
        """
        agent = self._get_runtime_agents().get(agent_id)
        if agent is None:
            return None

        # Build enriched prompt with context
        if context and node_id:
            requirement_desc = prompt  # Use original prompt as requirement description
            enriched_prompt = context.build_prompt(node_id, requirement_desc)
        else:
            enriched_prompt = prompt

        try:
            coro = agent.reply(Msg(name="system", role="user", content=enriched_prompt))
            loop = asyncio.new_event_loop()
            try:
                result = loop.run_until_complete(coro)
            finally:
                loop.close()
            content = result.get_text_content() or ""
            return AgentOutput(
                agent_id=agent_id,
                node_id=node_id or "unknown",
                content=content,
                success=True,
            )
        except Exception as e:
            return AgentOutput(
                agent_id=agent_id,
                node_id=node_id or "unknown",
                content=f"ÊâßË°åÂ§±Ë¥•: {str(e)}",
                success=False,
            )

    async def _invoke_agent_async(
        self,
        agent_id: str,
        prompt: str,
        *,
        context: ExecutionContext | None = None,
        node_id: str | None = None,
    ) -> AgentOutput | None:
        """Async version of agent invocation."""
        agent = self._get_runtime_agents().get(agent_id)
        if agent is None:
            return None

        if context and node_id:
            enriched_prompt = context.build_prompt(node_id, prompt)
        else:
            enriched_prompt = prompt

        try:
            result = await agent.reply(
                Msg(name="system", role="user", content=enriched_prompt),
                task_id=node_id,  # Pass task_id for observability
            )
            content = result.get_text_content() or ""
            return AgentOutput(
                agent_id=agent_id,
                node_id=node_id or "unknown",
                content=content,
                success=True,
            )
        except Exception as e:
            return AgentOutput(
                agent_id=agent_id,
                node_id=node_id or "unknown",
                content=f"ÊâßË°åÂ§±Ë¥•: {str(e)}",
                success=False,
            )

    async def _execute_with_msghub(
        self,
        graph: TaskGraph,
        context: ExecutionContext,
    ) -> list[AgentOutput]:
        """Execute tasks with MsgHub for agent-to-agent broadcasting.

        All agents are added to a MsgHub so they can observe each other's outputs.
        """
        outputs: list[AgentOutput] = []
        runtime_agents = self._get_runtime_agents()

        # Collect all agents involved in this execution
        agent_instances = []
        node_agent_map: dict[str, object] = {}
        for node_id in graph.topological_order():
            node = graph.get(node_id)
            if node.assigned_agent_id and node.assigned_agent_id in runtime_agents:
                agent = runtime_agents[node.assigned_agent_id]
                if agent not in agent_instances:
                    agent_instances.append(agent)
                node_agent_map[node_id] = agent

        if not agent_instances:
            # No runtime agents available - this is a critical issue!
            _log_execution(
                f"Êó†ÂèØÁî® Agent ÂÆû‰æã! runtime_agents={len(runtime_agents)}, graph_nodes={len(list(graph.nodes()))}",
                level="error",
                prefix="[MsgHub]",
            )
            # Log details about why agents weren't matched
            for node_id in graph.topological_order():
                node = graph.get(node_id)
                if node.assigned_agent_id:
                    if node.assigned_agent_id not in runtime_agents:
                        _log_execution(
                            f"‰ªªÂä° {node_id} ÂàÜÈÖçÁöÑ Agent '{node.assigned_agent_id}' ‰∏çÂú® runtime_agents ‰∏≠",
                            level="error",
                            prefix="[MsgHub]",
                        )
                else:
                    _log_execution(
                        f"‰ªªÂä° {node_id} Êú™ÂàÜÈÖç Agent (assigned_agent_id=None)",
                        level="warning",
                        prefix="[MsgHub]",
                    )
            return outputs

        _log_execution(
            f"[MsgHub] ÂºÄÂßãÊâßË°å: {len(agent_instances)} ‰∏™ Agent ÂÆû‰æã, {len(list(graph.nodes()))} ‰∏™‰ªªÂä°ËäÇÁÇπ",
        )

        # Create announcement message with initial context
        announcement = Msg(
            name="ExecutionLoop",
            role="system",
            content=f"## Âçè‰Ωú‰ªªÂä°ÂêØÂä®\n\nÁî®Êà∑ÈúÄÊ±Ç: {context.intent_utterance}\n\n"
            f"ÂèÇ‰∏é‰ª£ÁêÜ: {len(agent_instances)} ‰∏™\n"
            f"‰ªªÂä°Êï∞Èáè: {len(list(graph.nodes()))} ‰∏™",
        )

        # Execute within MsgHub context for automatic broadcasting
        import time
        total_tasks = len(list(graph.nodes()))
        completed_tasks = 0

        async with MsgHub(
            participants=agent_instances,
            announcement=announcement,
            enable_auto_broadcast=True,
        ):
            for node_id in graph.topological_order():
                graph.mark_running(node_id)
                node = graph.get(node_id)

                # Check if this is a collaborative task (multiple agents)
                if node.is_collaborative:
                    task_start = time.time()
                    _log_execution(
                        f"[MsgHub] ‰ªªÂä° {completed_tasks + 1}/{total_tasks}: {node_id} -> Âçè‰ΩúÊâßË°å ({len(node.assigned_agent_ids)} Agents)...",
                    )

                    # Get workspace for collaborative execution
                    workspace = self._get_workspace_for_collaborative()

                    if workspace:
                        collab_outputs = await self._execute_collaborative_task(
                            node=node,
                            context=context,
                            workspace=workspace,
                        )
                        outputs.extend(collab_outputs)

                        task_duration = time.time() - task_start
                        _log_execution(
                            f"[MsgHub] Âçè‰Ωú‰ªªÂä° {node_id} ÂÆåÊàê (ËÄóÊó∂: {task_duration:.1f}s, {len(collab_outputs)} ‰∏™ËæìÂá∫)",
                        )
                    else:
                        _log_execution(
                            f"[MsgHub] ‚ö†Ô∏è Âçè‰Ωú‰ªªÂä° {node_id} ÈúÄË¶Å RuntimeWorkspaceWithPRÔºåÈôçÁ∫ß‰∏∫Âçï Agent ÊâßË°å",
                            level="warning",
                        )
                        # Fallback to single agent (first one)
                        if node.assigned_agent_id:
                            output = await self._invoke_agent_async(
                                node.assigned_agent_id,
                                context.intent_utterance,
                                context=context,
                                node_id=node_id,
                            )
                            if output:
                                outputs.append(output)
                                context.add_output(output)

                    graph.mark_completed(node_id)
                    completed_tasks += 1
                    continue

                elif node.assigned_agent_id:
                    task_start = time.time()
                    _log_execution(
                        f"[MsgHub] ‰ªªÂä° {completed_tasks + 1}/{total_tasks}: {node_id} -> Agent '{node.assigned_agent_id}' ÂºÄÂßãÊâßË°å...",
                    )

                    output = await self._invoke_agent_async(
                        node.assigned_agent_id,
                        context.intent_utterance,
                        context=context,
                        node_id=node_id,
                    )

                    task_duration = time.time() - task_start

                    if output:
                        outputs.append(output)
                        context.add_output(output)
                        _log_execution(
                            f"[MsgHub] ‰ªªÂä° {node_id} ÂÆåÊàê (ËÄóÊó∂: {task_duration:.1f}s, ËæìÂá∫ÈïøÂ∫¶: {len(output.content) if output.content else 0} Â≠óÁ¨¶)",
                        )

                        # Store in shared memory if available
                        if self.shared_memory:
                            await self.shared_memory.add(
                                Msg(
                                    name=output.agent_id,
                                    role="assistant",
                                    content=output.content,
                                    metadata={"node_id": node_id},
                                )
                            )

                        # === TASK-LEVEL IMMEDIATE VALIDATION (NEW) ===
                        if self.enable_task_validation:
                            _log_execution(f"[{node_id}] üîç ÂºÄÂßã‰ªªÂä°Á∫ßÈ™åÊî∂...")
                            validation = await self._validate_and_fix_task(
                                node_id=node_id,
                                node=node,
                                output=output,
                                context=context,
                            )
                            if validation.passed:
                                _log_execution(f"[{node_id}] ‚úì ‰ªªÂä°È™åÊî∂ÈÄöËøá")
                            if not validation.passed:
                                # Task failed validation after retries
                                graph.mark_failed(
                                    node_id,
                                    reason=validation.error_summary,
                                )
                                _log_execution(
                                    f"[MsgHub] ‰ªªÂä° {node_id} È™åÊî∂Â§±Ë¥• (Â∑≤ÈáçËØï): {validation.error_summary}",
                                    level="error",
                                )
                                # Continue with other tasks instead of stopping
                                completed_tasks += 1
                                continue
                        # === END TASK-LEVEL VALIDATION ===
                    else:
                        _log_execution(
                            f"[MsgHub] ‰ªªÂä° {node_id} Êú™‰∫ßÁîüËæìÂá∫ (ËÄóÊó∂: {task_duration:.1f}s)",
                            level="warning",
                        )
                else:
                    _log_execution(
                        f"[MsgHub] ‰ªªÂä° {node_id} Êó†ÂàÜÈÖç Agent, Ë∑≥Ëøá",
                        level="warning",
                    )

                graph.mark_completed(node_id)
                completed_tasks += 1

        _log_execution(
            f"[MsgHub] ÊâßË°åÂÆåÊàê: {completed_tasks}/{total_tasks} ‰ªªÂä°, {len(outputs)} ‰∏™ËæìÂá∫",
        )
        return outputs

    async def _validate_and_fix_task(
        self,
        node_id: str,
        node: Any,
        output: "AgentOutput",
        context: "ExecutionContext",
    ) -> TaskValidationResult:
        """Validate a task immediately after completion and fix if needed.

        This method implements the task-level immediate validation pattern:
        1. Validate the task output
        2. If validation fails, generate a fix prompt
        3. Let the SAME agent fix the issue
        4. Re-validate after fix
        5. Repeat up to max_task_retries times

        Args:
            node_id: Task identifier (e.g., 'REQ-001')
            node: The task node from the graph
            output: The agent's output
            context: Execution context

        Returns:
            Final TaskValidationResult after validation/fix attempts
        """
        validator = self.get_task_validator()

        # Initial validation (check for [ERROR] markers)
        validation = await validator.validate_task(
            node_id=node_id,
            requirement=node.requirement,
            output_content=output.content if output else None,
        )

        if not validation.passed:
            # Output has error markers, go to fix loop
            pass
        elif self.enable_active_validation:
            # Output looks OK, but do active validation to verify
            _log_execution(
                f"[{node_id}] üîç ÊâßË°å‰∏ªÂä®È™åËØÅ...",
            )
            active_validation = await validator.validate_with_agent(
                node_id=node_id,
                requirement=node.requirement,
                output_content=output.content if output else None,
                validation_timeout=self.active_validation_timeout,
            )
            if not active_validation.passed:
                # Active validation failed - use its errors
                validation = active_validation
            else:
                # Both validations passed
                return validation
        else:
            # No active validation, output check passed
            return validation

        # Validation failed - attempt fixes
        for retry in range(self.max_task_retries):
            _log_execution(
                f"[{node_id}] ‚ö†Ô∏è ‰ªªÂä°È™åÊî∂Â§±Ë¥• (Â∞ùËØï {retry + 1}/{self.max_task_retries}): {validation.error_summary}",
                level="warning",
            )

            # Generate fix prompt
            fix_prompt = validator.generate_fix_prompt(
                node_id=node_id,
                requirement=node.requirement,
                validation_result=validation,
                original_output=output.content if output else None,
            )

            # Let the SAME agent fix the issue
            _log_execution(
                f"[{node_id}] üîß Agent '{node.assigned_agent_id}' ÂºÄÂßã‰øÆÂ§ç...",
            )

            fix_output = await self._invoke_agent_async(
                node.assigned_agent_id,
                fix_prompt,
                context=context,
                node_id=node_id,
            )

            # Update output
            if fix_output:
                output = fix_output
                context.add_output(fix_output)

            # Re-validate
            validation = await validator.validate_task(
                node_id=node_id,
                requirement=node.requirement,
                output_content=fix_output.content if fix_output else None,
            )

            if validation.passed:
                _log_execution(
                    f"[{node_id}] ‚úì ‰øÆÂ§çÊàêÂäü (Á¨¨ {retry + 1} Ê¨°Â∞ùËØï)",
                )
                return validation

        # All retries exhausted
        _log_execution(
            f"[{node_id}] ‚úó ‰øÆÂ§çÂ§±Ë¥• (Â∑≤ËææÊúÄÂ§ßÈáçËØïÊ¨°Êï∞ {self.max_task_retries})",
            level="error",
        )
        return validation

    # -------------------------------------------------------------------------
    # Multi-Agent Collaborative Execution
    # -------------------------------------------------------------------------

    async def _execute_collaborative_task(
        self,
        node: Any,
        context: "ExecutionContext",
        workspace: Any,
    ) -> list["AgentOutput"]:
        """Execute a task with multiple agents collaborating in parallel.

        This method implements the multi-agent collaboration pattern:
        1. Create git worktree for each agent
        2. Execute all agents in parallel with MsgHub communication
        3. Collect PRs and sort by size (largest first)
        4. Merge PRs sequentially, handling conflicts

        Args:
            node: The task node with multiple assigned agents.
            context: Execution context.
            workspace: RuntimeWorkspaceWithPR instance.

        Returns:
            List of AgentOutput from all agents.
        """
        from agentscope.scripts._runtime_workspace import AgentPRStats

        node_id = node.node_id
        agent_ids = node.assigned_agent_ids
        outputs: list[AgentOutput] = []

        _log_execution(
            f"[{node_id}] ü§ù ÂºÄÂßãÂçè‰ΩúÊâßË°å: {len(agent_ids)} ‰∏™ Agent",
            prefix="[Collaborative]",
        )

        # Step 1: Initialize git repo if needed
        await workspace.init_git_repo()

        # Step 2: Create worktree for each agent
        for agent_id in agent_ids:
            success = await workspace.create_agent_worktree(agent_id)
            if not success:
                _log_execution(
                    f"[{node_id}] ‚ö†Ô∏è Êó†Ê≥ï‰∏∫ {agent_id} ÂàõÂª∫ worktree",
                    level="warning",
                    prefix="[Collaborative]",
                )

        # Step 3: Get agent instances
        runtime_agents = self._get_runtime_agents()
        agent_instances = []
        for agent_id in agent_ids:
            if agent_id in runtime_agents:
                agent_instances.append(runtime_agents[agent_id])

        if not agent_instances:
            _log_execution(
                f"[{node_id}] ‚ùå Êó†ÂèØÁî® Agent ÂÆû‰æã",
                level="error",
                prefix="[Collaborative]",
            )
            return outputs

        # Step 4: Execute agents in parallel with MsgHub
        announcement = Msg(
            name="Coordinator",
            role="system",
            content=f"## Âçè‰Ωú‰ªªÂä°: {node_id}\n\n"
            f"ÈúÄÊ±Ç: {context.intent_utterance}\n\n"
            f"ÂèÇ‰∏é Agent: {', '.join(agent_ids)}\n\n"
            f"ËØ∑ÂêÑËá™ÂÆåÊàêËá™Â∑±ÁöÑÈÉ®ÂàÜÔºåÂπ∂ÈÄöËøáÊ∂àÊÅØÊ≤üÈÄöÂçèË∞É„ÄÇ\n"
            f"Â¶ÇÈúÄÂÖ∂‰ªñ Agent ÁöÑ‰ø°ÊÅØÔºåËØ∑ÂèëÈÄÅ [ÈúÄË¶ÅÂçèÂä©: @ËßíËâ≤Âêç] Ê∂àÊÅØ„ÄÇ",
        )

        async with MsgHub(
            participants=agent_instances,
            announcement=announcement,
            enable_auto_broadcast=True,
        ):
            # Create tasks for parallel execution
            tasks = []
            for agent_id in agent_ids:
                agent_dir = await workspace.get_agent_working_dir(agent_id)
                task_prompt = (
                    f"## Âçè‰Ωú‰ªªÂä°\n\n"
                    f"**‰ªªÂä°ID**: {node_id}\n"
                    f"**ÈúÄÊ±Ç**: {context.intent_utterance}\n"
                    f"**‰Ω†ÁöÑËßíËâ≤**: {agent_id}\n"
                    f"**Â∑•‰ΩúÁõÆÂΩï**: {agent_dir}\n\n"
                    f"ËØ∑Âú®‰Ω†ÁöÑÂ∑•‰ΩúÁõÆÂΩï‰∏≠ÂÆåÊàê‰ªªÂä°„ÄÇ‰Ω†ÂèØ‰ª•Êü•ÁúãÂÆåÊï¥‰ª£Á†ÅÂ∫ì‰ª•‰∫ÜËß£‰∏ä‰∏ãÊñá„ÄÇ\n"
                    f"Â¶ÇÊûúÈúÄË¶ÅÂÖ∂‰ªñ Agent ÁöÑ‰ø°ÊÅØÔºàÂ¶ÇÊé•Âè£ÂÆö‰πâÔºâÔºåËØ∑ÈÄöËøáÊ∂àÊÅØËØ¢ÈóÆ„ÄÇ"
                )
                tasks.append(
                    self._invoke_agent_async(
                        agent_id,
                        task_prompt,
                        context=context,
                        node_id=node_id,
                    )
                )

            # Execute in parallel
            _log_execution(
                f"[{node_id}] ‚ö° Âπ∂Ë°åÊâßË°å {len(tasks)} ‰∏™ Agent...",
                prefix="[Collaborative]",
            )
            results = await asyncio.gather(*tasks, return_exceptions=True)

            # Collect outputs
            for i, result in enumerate(results):
                agent_id = agent_ids[i]
                if isinstance(result, Exception):
                    _log_execution(
                        f"[{node_id}] ‚ùå {agent_id} ÊâßË°åÂºÇÂ∏∏: {result}",
                        level="error",
                        prefix="[Collaborative]",
                    )
                elif result:
                    outputs.append(result)
                    context.add_output(result)
                    _log_execution(
                        f"[{node_id}] ‚úì {agent_id} ÊâßË°åÂÆåÊàê",
                        prefix="[Collaborative]",
                    )

        # Step 5: Commit changes for each agent
        for agent_id in agent_ids:
            await workspace.commit_agent_changes(
                agent_id,
                f"{node_id}: {agent_id} changes",
            )

        # Step 6: Collect PR stats and sort by size
        pr_stats: list[AgentPRStats] = []
        for agent_id in agent_ids:
            stats = await workspace.get_agent_pr_stats(agent_id)
            pr_stats.append(stats)
            _log_execution(
                f"[{node_id}] üìä {agent_id} PR: +{stats.additions}/-{stats.deletions} ({stats.files_changed} files)",
                prefix="[Collaborative]",
            )

        # Sort by total changes (largest first)
        pr_stats.sort(key=lambda s: s.total_changes, reverse=True)

        _log_execution(
            f"[{node_id}] üìã ÂêàÂπ∂È°∫Â∫è: {[s.agent_id for s in pr_stats]}",
            prefix="[Collaborative]",
        )

        # Step 7: Merge PRs in order (largest first)
        for stats in pr_stats:
            agent_id = stats.agent_id

            # First, update agent's branch from main (in case others merged before)
            if pr_stats.index(stats) > 0:  # Not the first one
                _log_execution(
                    f"[{node_id}] üîÑ {agent_id} ÂêåÊ≠• main ÂàÜÊîØ...",
                    prefix="[Collaborative]",
                )
                sync_result = await workspace.update_agent_from_main(agent_id)

                if not sync_result.success and sync_result.conflicts:
                    # Has conflicts - let agent resolve
                    _log_execution(
                        f"[{node_id}] ‚ö†Ô∏è {agent_id} ÊúâÂÜ≤Á™Å: {sync_result.conflicts}",
                        level="warning",
                        prefix="[Collaborative]",
                    )

                    # Get conflict details
                    conflict_details = await workspace.get_conflict_details(agent_id)

                    # Let agent resolve conflicts
                    resolved = await self._resolve_agent_conflicts(
                        agent_id=agent_id,
                        node_id=node_id,
                        conflicts=sync_result.conflicts,
                        conflict_details=conflict_details,
                        context=context,
                        workspace=workspace,
                    )

                    if not resolved:
                        _log_execution(
                            f"[{node_id}] ‚ùå {agent_id} ÂÜ≤Á™ÅËß£ÂÜ≥Â§±Ë¥•",
                            level="error",
                            prefix="[Collaborative]",
                        )
                        continue

            # Merge to main
            _log_execution(
                f"[{node_id}] üîÄ ÂêàÂπ∂ {agent_id} Âà∞ main...",
                prefix="[Collaborative]",
            )
            merge_result = await workspace.merge_agent_to_main(agent_id)

            if merge_result.success:
                _log_execution(
                    f"[{node_id}] ‚úì {agent_id} ÂêàÂπ∂ÊàêÂäü",
                    prefix="[Collaborative]",
                )
            else:
                _log_execution(
                    f"[{node_id}] ‚ùå {agent_id} ÂêàÂπ∂Â§±Ë¥•: {merge_result.message}",
                    level="error",
                    prefix="[Collaborative]",
                )

        # Step 8: Cleanup worktrees
        for agent_id in agent_ids:
            await workspace.remove_agent_worktree(agent_id)

        _log_execution(
            f"[{node_id}] ‚úÖ Âçè‰ΩúÊâßË°åÂÆåÊàê",
            prefix="[Collaborative]",
        )

        return outputs

    async def _resolve_agent_conflicts(
        self,
        agent_id: str,
        node_id: str,
        conflicts: list[str],
        conflict_details: dict[str, str],
        context: "ExecutionContext",
        workspace: Any,
    ) -> bool:
        """Let an agent resolve merge conflicts.

        Args:
            agent_id: The agent to resolve conflicts.
            node_id: Task identifier.
            conflicts: List of conflicting file paths.
            conflict_details: Map of filename to content with conflict markers.
            context: Execution context.
            workspace: RuntimeWorkspaceWithPR instance.

        Returns:
            True if conflicts were resolved successfully.
        """
        _log_execution(
            f"[{node_id}] üîß ËÆ© {agent_id} Ëß£ÂÜ≥ÂÜ≤Á™Å...",
            prefix="[Collaborative]",
        )

        # Build conflict resolution prompt
        conflict_info = []
        for filename, content in conflict_details.items():
            # Extract just the conflicting section for context
            conflict_info.append(f"### {filename}\n```\n{content[:2000]}\n```")

        prompt = f"""## ÂêàÂπ∂ÂÜ≤Á™ÅÈúÄË¶ÅÂ§ÑÁêÜ

‰Ω†ÁöÑÂàÜÊîØÂú®ÂêàÂπ∂ main Êó∂ÂèëÁé∞‰ª•‰∏ãÊñá‰ª∂ÊúâÂÜ≤Á™ÅÔºö

{chr(10).join(f'- {f}' for f in conflicts)}

### ÂÜ≤Á™ÅÊ†áËÆ∞ËØ¥Êòé

Êñá‰ª∂‰∏≠ÂåÖÂê´Â¶Ç‰∏ãÊ†áËÆ∞Ôºö
```
<<<<<<< HEAD
‰Ω†ÁöÑ‰øÆÊîπ
=======
ÂÖ∂‰ªñ Agent ÁöÑ‰øÆÊîπÔºàÂ∑≤ÂêàÂπ∂Âà∞ mainÔºâ
>>>>>>> main
```

### ÂÜ≤Á™ÅÊñá‰ª∂ÂÜÖÂÆπ

{chr(10).join(conflict_info)}

### ‰Ω†ÈúÄË¶ÅÂÅöÁöÑ

1. ÁºñËæëÂÜ≤Á™ÅÊñá‰ª∂ÔºåÂêàÂπ∂ÂèåÊñπÁöÑ‰øÆÊîπ
2. Âà†Èô§ÊâÄÊúâ `<<<<<<<`„ÄÅ`=======`„ÄÅ`>>>>>>>` Ê†áËÆ∞
3. Á°Æ‰øù‰ª£Á†ÅÈÄªËæëÊ≠£Á°ÆÔºåÂäüËÉΩÂÆåÊï¥
4. ‰øùÂ≠òÊñá‰ª∂

**Ê≥®ÊÑè**Ôºö‰Ω†Â∫îËØ•‰øùÁïôÂèåÊñπÁöÑÊúâÊïà‰øÆÊîπÔºåËÄå‰∏çÊòØÁÆÄÂçïÂú∞ÈÄâÊã©‰∏ÄÊñπ„ÄÇ
"""

        # Let agent resolve
        output = await self._invoke_agent_async(
            agent_id,
            prompt,
            context=context,
            node_id=node_id,
        )

        if output:
            context.add_output(output)

        # Check if conflicts are resolved
        resolved = await workspace.mark_conflicts_resolved(agent_id)

        if resolved:
            _log_execution(
                f"[{node_id}] ‚úì {agent_id} ÂÜ≤Á™ÅËß£ÂÜ≥ÊàêÂäü",
                prefix="[Collaborative]",
            )
        else:
            _log_execution(
                f"[{node_id}] ‚ùå {agent_id} ‰ªçÊúâÊú™Ëß£ÂÜ≥ÁöÑÂÜ≤Á™ÅÊ†áËÆ∞",
                level="error",
                prefix="[Collaborative]",
            )

        return resolved

    def _execute_sequential(
        self,
        graph: TaskGraph,
        context: ExecutionContext,
        round_index: int = 1,
    ) -> list[AgentOutput]:
        """Execute tasks sequentially with context passing (sync version).

        Args:
            graph: Task graph to execute.
            context: Execution context with project memory.
            round_index: Current round index for decision tracking.

        Returns:
            List of agent outputs from this execution.
        """
        outputs: list[AgentOutput] = []

        for node_id in graph.topological_order():
            graph.mark_running(node_id)
            node = graph.get(node_id)

            if node.assigned_agent_id:
                output = self._invoke_agent(
                    node.assigned_agent_id,
                    context.intent_utterance,
                    context=context,
                    node_id=node_id,
                )
                if output:
                    outputs.append(output)
                    context.add_output(output)

                    # Parse and record decisions from this agent's output
                    if context.project_memory:
                        context.project_memory.parse_decisions_from_output(
                            output.content,
                            agent_id=output.agent_id,
                            round_index=round_index,
                        )

            graph.mark_completed(node_id)

        return outputs

    async def _execute_parallel(
        self,
        graph: TaskGraph,
        context: ExecutionContext,
        round_index: int = 1,
    ) -> list[AgentOutput]:
        """Execute tasks in parallel with agent collaboration.

        This method uses CollaborativeExecutor to run independent tasks
        concurrently while respecting dependency ordering in the task graph.

        Args:
            graph: Task graph to execute.
            context: Execution context with project memory.
            round_index: Current round index for decision tracking.

        Returns:
            List of agent outputs from this execution.
        """
        from .collaboration import CollaborativeExecutor
        from ..agent import AgentBase

        # Build task map and collect agents
        tasks: dict[str, str] = {}
        agents: dict[str, AgentBase] = {}
        agent_roles: dict[str, str] = {}
        dependencies: dict[str, set[str]] = {}

        for node_id in graph.topological_order():
            node = graph.get(node_id)
            if node.assigned_agent_id:
                agent = self.orchestrator.registry.get_agent_instance(
                    node.assigned_agent_id
                )
                if agent:
                    # Use requirement.notes as task description if available
                    task_desc = node.requirement.notes or "ÂÆåÊàêÂàÜÈÖçÁöÑ‰ªªÂä°"
                    tasks[node.assigned_agent_id] = (
                        f"[{node_id}] {context.intent_utterance}\n\n"
                        f"‰ªªÂä°ÊèèËø∞: {task_desc}"
                    )
                    agents[node.assigned_agent_id] = agent
                    # Get role from metadata or default to "Worker"
                    agent_roles[node.assigned_agent_id] = (
                        node.metadata.get("role") or "Worker"
                    )

                    # Build dependencies from node.dependencies
                    node_deps = set()
                    for dep_id in node.dependencies:
                        dep_node = graph.get(dep_id)
                        if dep_node and dep_node.assigned_agent_id:
                            node_deps.add(dep_node.assigned_agent_id)
                    if node_deps:
                        dependencies[node.assigned_agent_id] = node_deps

        if not tasks:
            _log_execution(
                "No tasks to execute in parallel mode",
                level="warning",
                prefix="[ParallelExec]",
            )
            return []

        # Create collaborative executor
        executor = CollaborativeExecutor(
            agents=agents,
            agent_roles=agent_roles,
            timeout_seconds=self.parallel_timeout_seconds,
        )

        _log_execution(
            f"Starting parallel execution with {len(agents)} agents, timeout={self.parallel_timeout_seconds:.1f}s",
            prefix="[ParallelExec]",
        )

        # Execute in parallel
        try:
            work_states = await executor.execute_parallel(
                tasks=tasks,
                dependencies=dependencies,
            )
        except asyncio.TimeoutError:
            _log_execution(
                f"Parallel execution timed out after {self.parallel_timeout_seconds:.1f}s",
                level="error",
                prefix="[ParallelExec]",
            )
            work_states = {}

        # Convert work states to AgentOutput format
        outputs: list[AgentOutput] = []
        for agent_id, state in work_states.items():
            graph_node_id = None
            for node_id in graph.topological_order():
                node = graph.get(node_id)
                if node.assigned_agent_id == agent_id:
                    graph_node_id = node_id
                    break

            if graph_node_id:
                if state.status == "completed":
                    graph.mark_completed(graph_node_id)
                elif state.status == "blocked":
                    graph.mark_failed(graph_node_id)

            output = AgentOutput(
                agent_id=agent_id,
                content=state.output or f"[{state.status}] {state.blocked_reason or ''}",
                metadata={
                    "status": state.status,
                    "node_id": graph_node_id,
                    "parallel_execution": True,
                },
            )
            outputs.append(output)
            context.add_output(output)

            # Parse and record decisions
            if context.project_memory and state.output:
                context.project_memory.parse_decisions_from_output(
                    state.output,
                    agent_id=agent_id,
                    round_index=round_index,
                )

        # Share artifacts from workspace
        for key, content in executor.workspace.artifacts.items():
            _log_execution(
                f"Shared artifact: {key}",
                prefix="[ParallelExec]",
            )

        _log_execution(
            f"Parallel execution completed: {sum(1 for s in work_states.values() if s.status == 'completed')}/{len(work_states)} agents succeeded",
            prefix="[ParallelExec]",
        )

        return outputs

    def _persist_intent(self, intent: IntentRequest) -> None:
        entry = MemoryEntry(
            key=f"intent:{intent.user_id}:{intent.project_id}",
            content=intent.utterance,
            tags={"intent", intent.user_id},
        )
        self.memory_pool.save(entry)

    def _ensure_project(self, intent: IntentRequest) -> str:
        if intent.project_id:
            if self.project_pool.get(intent.project_id) is None:
                descriptor = ProjectDescriptor(
                    project_id=intent.project_id,
                    name=f"Project {intent.project_id}",
                    metadata={"source": "aa"},
                )
                self.project_pool.register(descriptor)
            return intent.project_id
        project_id = f"proj-{shortuuid.uuid()}"
        descriptor = ProjectDescriptor(
            project_id=project_id,
            name=f"{intent.user_id}-{project_id}",
            metadata={"source": "aa"},
        )
        self.project_pool.register(descriptor)
        intent.project_id = project_id
        return project_id

    def _persist_round_summary(
        self,
        *,
        project_id: str,
        round_index: int,
        plan: StrategyPlan,
        task_status: dict[str, str],
        observed_metrics: dict[str, float],
    ) -> None:
        summary_lines = [
            f"Round {round_index}",
            f"Observed metrics: {observed_metrics}",
        ]
        for node_id, status in task_status.items():
            agent_name = (
                plan.rankings.get(node_id).profile.name
                if node_id in plan.rankings
                else "unassigned"
            )
            summary_lines.append(f"- {node_id}: {status} -> {agent_name}")
        entry = MemoryEntry(
            key=f"project:{project_id}:round:{round_index}",
            content="\n".join(summary_lines),
            tags={f"project:{project_id}", "round"},
        )
        self.memory_pool.save(entry)

    def _compute_metric_snapshot(
        self,
        *,
        baseline_cost: float,
        observed_cost: float,
        baseline_time: float,
        observed_time: float,
        acceptance: AcceptanceCriteria,
        round_index: int,
    ) -> tuple[float, dict[str, float], dict[str, bool]]:
        def ratio(baseline: float, observed: float) -> float:
            if observed <= 0:
                return 1.0
            if baseline <= 0:
                return 0.0
            return min(1.0, baseline / observed)

        def metric_value(name: str) -> float:
            lowered = name.lower()
            cost_component = ratio(baseline_cost, observed_cost)
            time_component = ratio(baseline_time, observed_time)
            base_score = (cost_component + time_component) / 2
            if "cost" in lowered:
                value = cost_component
            elif "time" in lowered or "speed" in lowered:
                value = time_component
            else:
                value = base_score
            if self.max_rounds > 1:
                progressive_bonus = ((round_index - 1) / (self.max_rounds - 1)) * 0.5
                value += progressive_bonus
            return max(0.0, min(1.0, value))

        metrics = acceptance.metrics or {"quality": 0.9}
        values: dict[str, float] = {name: metric_value(name) for name in metrics}
        passes: dict[str, bool] = {
            name: values[name] >= target for name, target in metrics.items()
        }
        total = max(len(metrics), 1)
        pass_ratio = sum(1 for ok in passes.values() if ok) / total
        return pass_ratio, values, passes

    def _broadcast_progress(
        self,
        *,
        project_id: str,
        round_index: int,
        summary: str,
        status: dict[str, str],
    ) -> None:
        if self.msg_hub_factory is None:
            return
        hub = self.msg_hub_factory(project_id)
        update = RoundUpdate(
            project_id=project_id,
            round_index=round_index,
            summary=summary,
            status=status,
        )
        hub.broadcast(update)

    @staticmethod
    def _plan_summary(plan: StrategyPlan) -> str:
        names = []
        for node_id, ranking in plan.rankings.items():
            names.append(f"{node_id}->{ranking.profile.name}")
        return "; ".join(names)

    def run_cycle(
        self,
        intent: IntentRequest,
        acceptance: AcceptanceCriteria,
        *,
        baseline_cost: float,
        observed_cost: float,
        baseline_time: float,
        observed_time: float,
    ) -> ExecutionReport:
        """Execute a full cycle with agent interaction and context passing.

        This method orchestrates multi-agent task execution with:
        - Context passing between agents (each agent sees previous outputs)
        - Optional MsgHub broadcasting for real-time agent communication
        - Shared memory support for cross-agent state

        Args:
            intent: The user intent request
            acceptance: Acceptance criteria for the task
            baseline_cost: Baseline cost for KPI calculation
            observed_cost: Observed cost for KPI calculation
            baseline_time: Baseline time for KPI calculation
            observed_time: Observed time for KPI calculation

        Returns:
            ExecutionReport with results and agent outputs
        """
        project_id = self._ensure_project(intent)
        self._persist_intent(intent)
        plan = self.orchestrator.plan_strategy(intent, acceptance)
        graph = self.task_graph_builder.build(
            requirements=plan.requirement_map,
            rankings=plan.rankings,
            edges=None,
        )

        # Initialize project task board from plan
        task_board = self._init_task_board_from_plan(project_id, plan, acceptance)

        accepted = False
        task_status: Dict[str, str] = {}
        observed_metrics: Dict[str, float] = {}
        deliverable: ArtifactDeliveryResult | None = None
        all_agent_outputs: list[AgentOutput] = []

        # Get project memory for cross-agent context sharing
        project_memory = self.get_project_memory(project_id)

        # Get enhanced project context for file tracking and validation
        project_context: "ProjectContext | None" = None
        if self.enable_project_context:
            project_context = self.get_project_context(project_id)

        # Track previous round feedback
        previous_feedback: "RoundFeedback | None" = None

        for round_index in range(1, self.max_rounds + 1):
            # Create execution context with project memory and previous feedback
            context = ExecutionContext(
                intent_utterance=intent.utterance,
                project_memory=project_memory,
                round_feedback=previous_feedback,
            )

            # Execute tasks with context passing and project memory
            if self.enable_parallel_execution:
                # Use parallel execution with CollaborativeExecutor
                _log_execution(
                    f"Using parallel execution mode for round {round_index}",
                    prefix="[ExecutionLoop]",
                )
                round_outputs = asyncio.get_event_loop().run_until_complete(
                    self._execute_parallel(graph, context, round_index=round_index)
                )
            else:
                # Use sequential execution (default)
                round_outputs = self._execute_sequential(
                    graph, context, round_index=round_index
                )
            all_agent_outputs.extend(round_outputs)

            # Parse decisions from agent outputs and save to project memory
            for output in round_outputs:
                project_memory.parse_decisions_from_output(
                    output.content,
                    agent_id=output.agent_id,
                    round_index=round_index,
                )

                # Register generated files with project context
                if project_context:
                    registered_files = self._parse_agent_output_files(
                        output, project_context
                    )
                    # Register dependency files for validation
                    for path in registered_files:
                        content = project_context.get_file_content(path)
                        if content:
                            self._register_dependency_file(
                                path, content, project_context
                            )

            task_status = {node.node_id: node.status.value for node in graph.nodes()}
            pass_ratio, metric_values, metric_passes = self._compute_metric_snapshot(
                baseline_cost=baseline_cost,
                observed_cost=observed_cost,
                baseline_time=baseline_time,
                observed_time=observed_time,
                acceptance=acceptance,
                round_index=round_index,
            )
            observed_metrics = {
                **metric_values,
                "pass_ratio": pass_ratio,
                "passed": sum(metric_passes.values()),
                "total": max(len(metric_passes), 1),
            }
            self._persist_round_summary(
                project_id=project_id,
                round_index=round_index,
                plan=plan,
                task_status=task_status,
                observed_metrics=observed_metrics,
            )
            self._broadcast_progress(
                project_id=project_id,
                round_index=round_index,
                summary=f"Round {round_index} status: {task_status}",
                status=task_status,
            )

            # Update agent scores based on task completion results
            self._update_agent_scores(plan, task_status, pass_ratio)

            # Update task board with execution results
            self._update_task_board_status(project_id, task_status)

            # Run LLM-driven acceptance validation
            acceptance_result = None
            if self.acceptance_agent is not None:
                _log_execution("Running LLM-driven acceptance validation...")
                # Use container workspace path if runtime_workspace is available
                acceptance_workspace = self.workspace_dir
                if (
                    self.acceptance_agent._runtime_workspace
                    and hasattr(self.acceptance_agent._runtime_workspace, "workspace_dir")
                ):
                    acceptance_workspace = self.acceptance_agent._runtime_workspace.workspace_dir
                acceptance_result = self.acceptance_agent.validate_sync(
                    workspace_dir=acceptance_workspace,
                    user_requirement=intent.utterance,
                    acceptance_criteria=[
                        c.description for c in acceptance.criteria
                    ] if hasattr(acceptance, "criteria") else None,
                    artifact_type=intent.artifact_type or "web",
                )
                accepted = acceptance_result.passed
                observed_metrics["acceptance_score"] = acceptance_result.score
                _log_execution(
                    f"Acceptance validation: {acceptance_result.status.value} (score={acceptance_result.score:.2f})",
                )
            else:
                # No acceptance agent configured - use pass_ratio as fallback
                # This is a simple heuristic, real validation requires AcceptanceAgent
                accepted = pass_ratio >= 0.8
                _log_execution(
                    f"No AcceptanceAgent configured, using pass_ratio fallback: {pass_ratio:.2f}",
                    level="warning",
                )

            if accepted:
                break

            # Generate round feedback for the next iteration
            if project_context:
                previous_feedback = project_context.generate_round_feedback(
                    requirement_id=intent.project_id or project_id,
                    round_index=round_index,
                )
                # Integrate acceptance validation results into feedback
                if acceptance_result is not None:
                    previous_feedback.add_acceptance_result(acceptance_result)
                _log_execution(
                    f"Round {round_index} feedback: {len(previous_feedback.critical_issues)} critical issues, {len(previous_feedback.warnings)} warnings",
                )
                # Log critical issues details for better observability
                if previous_feedback.critical_issues:
                    _log_execution("=== Critical Issues ===", level="error")
                    for i, issue in enumerate(previous_feedback.critical_issues[:5], 1):
                        _log_execution(f"  [{i}] {issue}", level="error")
                    if len(previous_feedback.critical_issues) > 5:
                        _log_execution(
                            f"  ... ËøòÊúâ {len(previous_feedback.critical_issues) - 5} ‰∏™ÈóÆÈ¢òÊú™ÊòæÁ§∫",
                            level="error",
                        )
                # Log warnings too
                if previous_feedback.warnings:
                    _log_execution("=== Warnings ===", level="warning")
                    for i, warning in enumerate(previous_feedback.warnings[:3], 1):
                        _log_execution(f"  [{i}] {warning}", level="warning")

            # Re-plan for next round to reflect potential agent changes.
            plan = self.orchestrator.plan_strategy(intent, acceptance)
            graph = self.task_graph_builder.build(
                requirements=plan.requirement_map,
                rankings=plan.rankings,
                edges=None,
            )
            observed_cost *= 0.8
            observed_time *= 0.8

        kpi_record = self.kpi_tracker.record_cycle(
            baseline_cost=baseline_cost,
            observed_cost=observed_cost,
            baseline_time=baseline_time,
            observed_time=observed_time,
        )

        if accepted and self.delivery_manager is not None:
            artifact_type = intent.artifact_type or "web"
            deliverable = self.delivery_manager.deliver(
                artifact_type=artifact_type,
                project_id=project_id,
                plan_summary=self._plan_summary(plan),
                task_status=task_status,
            )

        return ExecutionReport(
            project_id=project_id,
            accepted=accepted,
            kpi=kpi_record,
            task_status=task_status,
            plan=plan,
            deliverable=deliverable,
            agent_outputs=all_agent_outputs,
            acceptance_result=acceptance_result,
        )

    async def run_cycle_async(
        self,
        intent: IntentRequest,
        acceptance: AcceptanceCriteria,
        *,
        baseline_cost: float,
        observed_cost: float,
        baseline_time: float,
        observed_time: float,
    ) -> ExecutionReport:
        """Async version of run_cycle with MsgHub support.

        This method enables full agent interaction via MsgHub, where agents
        can broadcast their outputs and observe other agents' messages in
        real-time.

        Args:
            intent: The user intent request
            acceptance: Acceptance criteria for the task
            baseline_cost: Baseline cost for KPI calculation
            observed_cost: Observed cost for KPI calculation
            baseline_time: Baseline time for KPI calculation
            observed_time: Observed time for KPI calculation

        Returns:
            ExecutionReport with results and agent outputs
        """
        project_id = self._ensure_project(intent)
        self._persist_intent(intent)
        plan = self.orchestrator.plan_strategy(intent, acceptance)
        graph = self.task_graph_builder.build(
            requirements=plan.requirement_map,
            rankings=plan.rankings,
            edges=None,
        )

        # Initialize project task board from plan
        task_board = self._init_task_board_from_plan(project_id, plan, acceptance)

        accepted = False
        task_status: Dict[str, str] = {}
        observed_metrics: Dict[str, float] = {}
        deliverable: ArtifactDeliveryResult | None = None
        all_agent_outputs: list[AgentOutput] = []

        # Get project memory for cross-agent context sharing
        project_memory = self.get_project_memory(project_id)

        # Get enhanced project context for file tracking and validation
        project_context: "ProjectContext | None" = None
        if self.enable_project_context:
            project_context = self.get_project_context(project_id)

        # Track previous round feedback
        previous_feedback: "RoundFeedback | None" = None

        for round_index in range(1, self.max_rounds + 1):
            # Create execution context with project memory and previous feedback
            context = ExecutionContext(
                intent_utterance=intent.utterance,
                project_memory=project_memory,
                round_feedback=previous_feedback,
            )

            # Use MsgHub for agent interaction if enabled
            if self.enable_agent_msghub:
                round_outputs = await self._execute_with_msghub(graph, context)
            else:
                # Fall back to sequential execution without MsgHub
                round_outputs = self._execute_sequential(graph, context)

            all_agent_outputs.extend(round_outputs)

            # Parse decisions from agent outputs and save to project memory
            for output in round_outputs:
                project_memory.parse_decisions_from_output(
                    output.content,
                    agent_id=output.agent_id,
                    round_index=round_index,
                )

                # Register generated files with project context
                if project_context:
                    registered_files = self._parse_agent_output_files(
                        output, project_context
                    )
                    # Register dependency files for validation
                    for path in registered_files:
                        content = project_context.get_file_content(path)
                        if content:
                            self._register_dependency_file(
                                path, content, project_context
                            )

            task_status = {node.node_id: node.status.value for node in graph.nodes()}
            pass_ratio, metric_values, metric_passes = self._compute_metric_snapshot(
                baseline_cost=baseline_cost,
                observed_cost=observed_cost,
                baseline_time=baseline_time,
                observed_time=observed_time,
                acceptance=acceptance,
                round_index=round_index,
            )
            observed_metrics = {
                **metric_values,
                "pass_ratio": pass_ratio,
                "passed": sum(metric_passes.values()),
                "total": max(len(metric_passes), 1),
            }
            self._persist_round_summary(
                project_id=project_id,
                round_index=round_index,
                plan=plan,
                task_status=task_status,
                observed_metrics=observed_metrics,
            )
            self._broadcast_progress(
                project_id=project_id,
                round_index=round_index,
                summary=f"Round {round_index} status: {task_status}",
                status=task_status,
            )

            # Update agent scores based on task completion results
            self._update_agent_scores(plan, task_status, pass_ratio)

            # Update task board with execution results
            self._update_task_board_status(project_id, task_status)

            # Run LLM-driven acceptance validation
            acceptance_result = None
            if self.acceptance_agent is not None:
                _log_execution("Running LLM-driven acceptance validation...")
                # Use container workspace path if runtime_workspace is available
                acceptance_workspace = self.workspace_dir
                if (
                    self.acceptance_agent._runtime_workspace
                    and hasattr(self.acceptance_agent._runtime_workspace, "workspace_dir")
                ):
                    acceptance_workspace = self.acceptance_agent._runtime_workspace.workspace_dir
                acceptance_result = await self.acceptance_agent.validate(
                    workspace_dir=acceptance_workspace,
                    user_requirement=intent.utterance,
                    acceptance_criteria=[
                        c.description for c in acceptance.criteria
                    ] if hasattr(acceptance, "criteria") else None,
                    artifact_type=intent.artifact_type or "web",
                )
                accepted = acceptance_result.passed
                observed_metrics["acceptance_score"] = acceptance_result.score
                _log_execution(
                    f"Acceptance validation: {acceptance_result.status.value} (score={acceptance_result.score:.2f})",
                )
            else:
                # No acceptance agent configured - use pass_ratio as fallback
                # This is a simple heuristic, real validation requires AcceptanceAgent
                accepted = pass_ratio >= 0.8
                _log_execution(
                    f"No AcceptanceAgent configured, using pass_ratio fallback: {pass_ratio:.2f}",
                    level="warning",
                )

            if accepted:
                break

            # Generate round feedback for the next iteration
            if project_context:
                previous_feedback = project_context.generate_round_feedback(
                    requirement_id=intent.project_id or project_id,
                    round_index=round_index,
                )
                # Integrate acceptance validation results into feedback
                if acceptance_result is not None:
                    previous_feedback.add_acceptance_result(acceptance_result)
                _log_execution(
                    f"Round {round_index} feedback: {len(previous_feedback.critical_issues)} critical issues, {len(previous_feedback.warnings)} warnings",
                )
                # Log critical issues details for better observability
                if previous_feedback.critical_issues:
                    _log_execution("=== Critical Issues ===", level="error")
                    for i, issue in enumerate(previous_feedback.critical_issues[:5], 1):
                        _log_execution(f"  [{i}] {issue}", level="error")
                    if len(previous_feedback.critical_issues) > 5:
                        _log_execution(
                            f"  ... ËøòÊúâ {len(previous_feedback.critical_issues) - 5} ‰∏™ÈóÆÈ¢òÊú™ÊòæÁ§∫",
                            level="error",
                        )
                # Log warnings too
                if previous_feedback.warnings:
                    _log_execution("=== Warnings ===", level="warning")
                    for i, warning in enumerate(previous_feedback.warnings[:3], 1):
                        _log_execution(f"  [{i}] {warning}", level="warning")

            # Replan for next round, but try to reuse existing agents
            # Store current runtime agents before replanning
            current_agents = self._get_runtime_agents().copy()

            plan = self.orchestrator.plan_strategy(intent, acceptance)

            # Restore cached agents to avoid re-creation
            # This ensures agents are reused across rounds
            new_agents = self._get_runtime_agents()
            for agent_id, agent in current_agents.items():
                if agent_id not in new_agents:
                    new_agents[agent_id] = agent
                    logger.debug(
                        "Â§çÁî®ÁºìÂ≠òÁöÑ Agent: %s",
                        agent_id,
                    )

            graph = self.task_graph_builder.build(
                requirements=plan.requirement_map,
                rankings=plan.rankings,
                edges=None,
            )
            observed_cost *= 0.8
            observed_time *= 0.8

        kpi_record = self.kpi_tracker.record_cycle(
            baseline_cost=baseline_cost,
            observed_cost=observed_cost,
            baseline_time=baseline_time,
            observed_time=observed_time,
        )

        if accepted and self.delivery_manager is not None:
            artifact_type = intent.artifact_type or "web"
            deliverable = self.delivery_manager.deliver(
                artifact_type=artifact_type,
                project_id=project_id,
                plan_summary=self._plan_summary(plan),
                task_status=task_status,
            )

        return ExecutionReport(
            project_id=project_id,
            accepted=accepted,
            kpi=kpi_record,
            task_status=task_status,
            plan=plan,
            deliverable=deliverable,
            agent_outputs=all_agent_outputs,
            acceptance_result=acceptance_result,
        )
