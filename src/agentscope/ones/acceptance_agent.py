# -*- coding: utf-8 -*-
"""Acceptance Agent for dynamic project validation.

This module provides an LLM-driven acceptance agent that:
1. Analyzes deliverables to determine validation strategy
2. Selects appropriate sandbox environments (browser, CLI, API, etc.)
3. Executes validation dynamically based on LLM decisions
4. Verifies deliverables meet acceptance criteria

Key design principles:
- NO hardcoded validation methods - LLM decides everything
- Multi-sandbox support: browser for web, CLI for backend, etc.
- Deliverable-aware: validates against actual acceptance criteria
"""
from __future__ import annotations

import asyncio
import json
import time
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, TYPE_CHECKING

from .._logging import logger


def _log_progress(
    msg: str,
    *,
    level: str = "info",
    hub: "Any | None" = None,
    event_type: str | None = None,
    metadata: dict | None = None,
    project_id: str | None = None,
) -> None:
    """Log progress with immediate flush for observability.

    Args:
        msg: Message to log.
        level: Log level (info, warning, error).
        hub: Optional ObservabilityHub instance for timeline tracking.
        event_type: Optional timeline event type for ObservabilityHub.
        metadata: Optional metadata for timeline event.
        project_id: Optional project ID for timeline event.
    """
    from datetime import datetime

    timestamp = time.strftime("%H:%M:%S")
    prefix = "[AcceptanceAgent]"
    formatted = f"{timestamp} | {prefix} {msg}"

    # Print with flush for immediate visibility
    print(formatted, flush=True)

    # Also log via logger for file logging
    if level == "warning":
        logger.warning(msg)
    elif level == "error":
        logger.error(msg)
    else:
        logger.info(msg)

    # Record to ObservabilityHub if provided
    if hub is not None and event_type is not None:
        try:
            from ..observability._types import TimelineEvent

            event = TimelineEvent(
                timestamp=datetime.now(),
                event_type=event_type,  # type: ignore[arg-type]
                project_id=project_id,
                agent_id="acceptance_agent",
                metadata={"message": msg, **(metadata or {})},
            )
            hub.record_timeline_event(event)
        except Exception:
            pass  # Don't fail on observability errors

if TYPE_CHECKING:
    from ..model import ChatModelBase
    from .sandbox_orchestrator import SandboxOrchestrator, SandboxTypeEnum

    # Forward reference for RuntimeWorkspace (defined in full_user_flow_cli.py)
    class RuntimeWorkspace:
        """Type stub for RuntimeWorkspace."""

        sandbox_id: str | None
        is_initialized: bool

        def execute_command(
            self, command: str, *, working_dir: str | None = None, timeout: int = 300
        ) -> dict: ...

        def read_file(self, path: str) -> str: ...

        def list_directory(self, path: str = "") -> list[str]: ...


class AcceptanceStatus(str, Enum):
    """Status of acceptance validation."""

    PASSED = "passed"
    FAILED = "failed"
    PARTIAL = "partial"  # Some checks passed, some failed
    ERROR = "error"  # Validation itself failed to run


class ValidationEnvironment(str, Enum):
    """Environment type for validation execution."""

    CLI = "cli"  # Command line execution
    BROWSER = "browser"  # Browser automation (Playwright)
    API = "api"  # HTTP API testing
    GUI = "gui"  # Desktop GUI automation
    MOBILE = "mobile"  # Mobile app testing
    VISUAL = "visual"  # Screenshot comparison


@dataclass
class ValidationCheck:
    """A single validation check result."""

    name: str
    description: str
    environment: ValidationEnvironment
    action: dict[str, Any]  # LLM-defined action to execute
    passed: bool
    output: str = ""
    error: str | None = None
    screenshot: str | None = None  # Path to screenshot if applicable
    critical: bool = False  # If True, failure stops all subsequent checks
    phase: str = "function"  # dependency/compile/startup/function


@dataclass
class AcceptanceResult:
    """Result of acceptance validation."""

    status: AcceptanceStatus
    score: float  # 0.0 to 1.0
    checks: list[ValidationCheck] = field(default_factory=list)
    summary: str = ""
    recommendations: list[str] = field(default_factory=list)
    deliverable_assessment: dict[str, Any] = field(default_factory=dict)

    @property
    def passed(self) -> bool:
        """Whether acceptance passed."""
        return self.status == AcceptanceStatus.PASSED

    def to_dict(self) -> dict[str, Any]:
        """Convert to dictionary for serialization."""
        return {
            "status": self.status.value,
            "score": self.score,
            "passed": self.passed,
            "summary": self.summary,
            "checks": [
                {
                    "name": c.name,
                    "description": c.description,
                    "environment": c.environment.value,
                    "phase": c.phase,
                    "critical": c.critical,
                    "passed": c.passed,
                    "output": c.output[:500] if c.output else "",
                    "error": c.error,
                    "screenshot": c.screenshot,
                }
                for c in self.checks
            ],
            "recommendations": self.recommendations,
            "deliverable_assessment": self.deliverable_assessment,
        }


# Prompt for LLM to generate validation strategy
VALIDATION_STRATEGY_PROMPT = '''ä½ æ˜¯ä¸€ä¸ªæ™ºèƒ½éªŒæ”¶ä¸“å®¶ã€‚æ ¹æ®ä»¥ä¸‹ä¿¡æ¯ï¼Œç”ŸæˆéªŒæ”¶ç­–ç•¥ã€‚

## ç”¨æˆ·åŸå§‹éœ€æ±‚
{user_requirement}

## äº¤ä»˜æ ‡å‡†
{acceptance_criteria}

## äº§ç‰©ç±»å‹
{artifact_type}

## å·¥ä½œåŒºæ–‡ä»¶
{file_list}

## å¯ç”¨éªŒæ”¶ç¯å¢ƒ
{available_environments}

## ä»»åŠ¡
åˆ†æäº§ç‰©å’Œäº¤ä»˜æ ‡å‡†ï¼Œç”ŸæˆéªŒæ”¶ç­–ç•¥ã€‚

### ã€å¼ºåˆ¶ã€‘éªŒæ”¶å¿…é¡»æŒ‰é¡ºåºè¦†ç›–ä»¥ä¸‹é˜¶æ®µï¼š

**é˜¶æ®µ 1ï¼šä¾èµ–éªŒè¯**ï¼ˆrequired=true, critical=trueï¼‰
- æ ¹æ®é¡¹ç›®æ–‡ä»¶ï¼ˆå¦‚ requirements.txtã€package.jsonã€go.modã€Cargo.toml ç­‰ï¼‰éªŒè¯ä¾èµ–å®‰è£…
- å¿…é¡»æ‰§è¡Œå®é™…çš„å®‰è£…å‘½ä»¤å¹¶æ£€æŸ¥è¾“å‡ºæ˜¯å¦æœ‰é”™è¯¯
- æ£€æŸ¥å®‰è£…å‘½ä»¤çš„ exit code å’Œè¾“å‡ºä¸­æ˜¯å¦åŒ…å« ERROR/error/failed ç­‰å…³é”®è¯
- æ­¤é˜¶æ®µå¤±è´¥æ„å‘³ç€ä¾èµ–ä¸å®Œæ•´ï¼Œä»£ç æ— æ³•è¿è¡Œ

**é˜¶æ®µ 2ï¼šç¼–è¯‘/å¯¼å…¥éªŒè¯**ï¼ˆrequired=true, critical=trueï¼‰
- éªŒè¯ä»£ç å¯ä»¥æˆåŠŸç¼–è¯‘æˆ–å¯¼å…¥ï¼Œã€ç¦æ­¢ã€‘åªæ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
- æ ¹æ®é¡¹ç›®ç±»å‹æ‰§è¡Œç›¸åº”çš„éªŒè¯å‘½ä»¤ï¼Œä¾‹å¦‚ï¼š
  - Python é¡¹ç›®ï¼šæ‰§è¡Œ `python -c "from <main_module> import <entry>"` éªŒè¯å¯¼å…¥æ— é”™è¯¯
  - TypeScript/å‰ç«¯é¡¹ç›®ï¼šæ‰§è¡Œ `npm run build` æˆ– `npx tsc --noEmit` éªŒè¯ç¼–è¯‘
  - Go é¡¹ç›®ï¼šæ‰§è¡Œ `go build ./...` éªŒè¯ç¼–è¯‘
  - Rust é¡¹ç›®ï¼šæ‰§è¡Œ `cargo check` éªŒè¯ç¼–è¯‘
  - Java é¡¹ç›®ï¼šæ‰§è¡Œ `mvn compile` æˆ– `gradle build` éªŒè¯ç¼–è¯‘
- æ­¤é˜¶æ®µå¤±è´¥æ„å‘³ç€ä»£ç å­˜åœ¨è¯­æ³•é”™è¯¯æˆ–ç±»å‹é”™è¯¯ï¼Œå¿…é¡»æ ‡è®°ä¸º critical

**é˜¶æ®µ 3ï¼šå¯åŠ¨éªŒè¯**ï¼ˆrequired=true, critical=trueï¼‰
- éªŒè¯æœåŠ¡å¯ä»¥æˆåŠŸå¯åŠ¨å¹¶ä¿æŒè¿è¡Œ
- ã€ç¦æ­¢ã€‘åªæ£€æŸ¥å¯åŠ¨å‘½ä»¤æ˜¯å¦æ‰§è¡ŒæˆåŠŸï¼Œå¿…é¡»éªŒè¯æœåŠ¡ç¡®å®åœ¨è¿è¡Œ
- éªŒè¯æ–¹å¼ï¼šå¥åº·æ£€æŸ¥ç«¯ç‚¹ã€ç«¯å£ç›‘å¬æ£€æµ‹ã€è¿›ç¨‹çŠ¶æ€æ£€æŸ¥ã€æ—¥å¿—æ— è‡´å‘½é”™è¯¯
- å¯¹äºåå°æœåŠ¡ï¼Œå¯åŠ¨åç­‰å¾…æ•°ç§’å†éªŒè¯æœåŠ¡çŠ¶æ€

**é˜¶æ®µ 4ï¼šåŠŸèƒ½éªŒè¯**ï¼ˆæ ¹æ®äº¤ä»˜æ ‡å‡†è®¾è®¡ï¼‰
- éªŒè¯ä¸šåŠ¡åŠŸèƒ½æ˜¯å¦æ»¡è¶³ç”¨æˆ·éœ€æ±‚å’Œäº¤ä»˜æ ‡å‡†
- ä½¿ç”¨ API æµ‹è¯•ã€æµè§ˆå™¨æµ‹è¯•ç­‰æ–¹å¼éªŒè¯å…·ä½“åŠŸèƒ½

### ã€é‡è¦ã€‘éªŒæ”¶è§„åˆ™ï¼š
- é˜¶æ®µ 1-3 çš„æ£€æŸ¥å¿…é¡»è®¾ç½® "critical": trueï¼Œä»»ä¸€å¤±è´¥åˆ™æ•´ä½“éªŒæ”¶å¤±è´¥
- ä¸èƒ½è·³è¿‡é˜¶æ®µ 1-3 ç›´æ¥è¿›è¡ŒåŠŸèƒ½éªŒè¯
- æ¯ä¸ªé˜¶æ®µè‡³å°‘åŒ…å«ä¸€ä¸ªéªŒè¯æ£€æŸ¥
- æ ¹æ®å®é™…æ–‡ä»¶åˆ¤æ–­é¡¹ç›®ç±»å‹ï¼Œä¸è¦å‡è®¾ä»»ä½•ç‰¹å®šæ¡†æ¶

è¾“å‡ºJSONæ ¼å¼ï¼š
```json
{{
  "artifact_analysis": {{
    "detected_type": "æ£€æµ‹åˆ°çš„äº§ç‰©ç±»å‹",
    "main_components": ["ä¸»è¦ç»„ä»¶åˆ—è¡¨"],
    "entry_points": ["å…¥å£ç‚¹ï¼Œå¦‚URLã€å‘½ä»¤ç­‰"]
  }},
  "validation_strategy": {{
    "approach": "éªŒæ”¶æ–¹æ³•æ€»è¿°",
    "environments_needed": ["éœ€è¦çš„éªŒæ”¶ç¯å¢ƒ"]
  }},
  "validation_checks": [
    {{
      "name": "æ£€æŸ¥åç§°",
      "description": "æ£€æŸ¥ä»€ä¹ˆï¼Œä¸ºä»€ä¹ˆé‡è¦",
      "phase": "dependency/compile/startup/function",
      "acceptance_criterion": "å¯¹åº”å“ªæ¡äº¤ä»˜æ ‡å‡†",
      "environment": "cli/browser/api/gui/mobile/visual",
      "action": {{
        "type": "åŠ¨ä½œç±»å‹",
        ... åŠ¨ä½œå‚æ•°
      }},
      "expected_result": "æœŸæœ›ç»“æœæè¿°",
      "required": true,
      "critical": true/false
    }}
  ],
  "reasoning": "ä¸ºä»€ä¹ˆé€‰æ‹©è¿™ä¸ªéªŒæ”¶ç­–ç•¥"
}}
```

## åŠ¨ä½œç±»å‹è¯´æ˜

### CLIåŠ¨ä½œ (environment: "cli")
```json
{{"type": "shell", "command": "è¦æ‰§è¡Œçš„å‘½ä»¤", "timeout": 60}}
{{"type": "file_check", "paths": ["æ£€æŸ¥çš„æ–‡ä»¶è·¯å¾„"], "check": "exists/not_empty/contains"}}
```

### BrowseråŠ¨ä½œ (environment: "browser")

**ã€é‡è¦ã€‘browser_snapshot è¿”å›çš„æ˜¯æ— éšœç¢æ ‘ï¼ˆAccessibility Treeï¼‰æ ¼å¼ï¼Œä¸æ˜¯ HTML DOMï¼**

æ— éšœç¢æ ‘æ ¼å¼ç¤ºä¾‹ï¼š
```
- document [FOCUSED]
  - banner
    - heading "é¡µé¢æ ‡é¢˜" [level=1]
    - navigation
      - link "é¦–é¡µ"
      - link "å…³äº"
  - main
    - region "å†…å®¹åŒºåŸŸ"
      - paragraph
        - text "è¿™æ˜¯æ­£æ–‡å†…å®¹"
      - button "ç‚¹å‡»æˆ‘"
```

å› æ­¤ï¼š
- ã€ç¦æ­¢ã€‘ä½¿ç”¨ CSS é€‰æ‹©å™¨ï¼ˆå¦‚ "#id", ".class", "div > p"ï¼‰- ä¸ä¼šå·¥ä½œ
- ã€æ¨èã€‘ä½¿ç”¨ check_text æ£€æŸ¥é¡µé¢æ˜¯å¦åŒ…å«ç‰¹å®šæ–‡æœ¬
- ã€æ¨èã€‘ä½¿ç”¨ check_title æ£€æŸ¥é¡µé¢æ ‡é¢˜
- ã€å¯ç”¨ã€‘check_element çš„ selector åº”è¯¥å¡«å†™æ— éšœç¢æ ‘ä¸­çš„è§’è‰²åæˆ–æ–‡æœ¬ç‰‡æ®µ

```json
{{"type": "navigate", "url": "http://localhost:3000"}}
{{"type": "screenshot", "name": "æˆªå›¾åç§°"}}
{{"type": "check_text", "text": "æœŸæœ›åœ¨é¡µé¢ä¸­å‡ºç°çš„æ–‡æœ¬"}}
{{"type": "check_title", "title": "æœŸæœ›çš„é¡µé¢æ ‡é¢˜"}}
{{"type": "check_element", "selector": "button/link/headingç­‰è§’è‰²æˆ–æ–‡æœ¬ç‰‡æ®µ", "check": "exists/text_contains", "value": "æœŸæœ›å€¼"}}
{{"type": "click", "element": "æŒ‰é’®æˆ–é“¾æ¥çš„æ–‡æœ¬/è§’è‰²æè¿°"}}
{{"type": "input", "element": "è¾“å…¥æ¡†çš„æ–‡æœ¬/è§’è‰²æè¿°", "value": "è¾“å…¥å€¼"}}
{{"type": "wait", "timeout": 5000}}
```

éªŒè¯ç¤ºä¾‹ï¼ˆæ­£ç¡®æ–¹å¼ï¼‰ï¼š
- éªŒè¯é¡µé¢æœ‰æ ‡é¢˜ â†’ {{"type": "check_text", "text": "æ–°å“å‘å¸ƒ"}}
- éªŒè¯æœ‰å¯¼èˆªé“¾æ¥ â†’ {{"type": "check_text", "text": "é¦–é¡µ"}}
- éªŒè¯æœ‰æŒ‰é’® â†’ {{"type": "check_element", "selector": "button", "check": "exists"}}
- éªŒè¯ç‰¹å®šæ–‡æœ¬å­˜åœ¨ â†’ {{"type": "check_text", "text": "æ¬¢è¿ä½¿ç”¨"}}

### APIåŠ¨ä½œ (environment: "api")
```json
{{"type": "request", "method": "GET/POST/PUT/DELETE", "url": "APIåœ°å€", "headers": {{}}, "body": {{}}}}
{{"type": "check_response", "status": 200, "body_contains": "æœŸæœ›å†…å®¹"}}
```

æ³¨æ„ï¼š
- æ ¹æ®äº§ç‰©ç±»å‹é€‰æ‹©åˆé€‚çš„éªŒæ”¶ç¯å¢ƒ
- Webåº”ç”¨å¿…é¡»ç”¨æµè§ˆå™¨ç¯å¢ƒéªŒæ”¶UI
- APIæœåŠ¡ç”¨APIç¯å¢ƒéªŒæ”¶
- åç«¯ä»£ç ç”¨CLIç¯å¢ƒéªŒæ”¶
- éªŒæ”¶æ­¥éª¤è¦èƒ½éªŒè¯äº¤ä»˜æ ‡å‡†æ˜¯å¦æ»¡è¶³
- ã€é‡è¦ã€‘æµè§ˆå™¨ç¯å¢ƒä½¿ç”¨æ— éšœç¢æ ‘ï¼Œä¸æ”¯æŒ CSS é€‰æ‹©å™¨ï¼è¯·ç”¨ check_text å’Œ check_title éªŒè¯
- ã€é‡è¦ã€‘éªŒè¯é¡µé¢å†…å®¹æ—¶ï¼Œä½¿ç”¨ check_text æ£€æŸ¥å…·ä½“æ–‡æœ¬æ˜¯å¦å­˜åœ¨

### ã€critical æ£€æŸ¥è§„åˆ™ã€‘
- phase ä¸º dependency/compile/startup çš„æ£€æŸ¥å¿…é¡»è®¾ç½® "critical": true
- critical=true çš„æ£€æŸ¥å¤±è´¥ä¼šå¯¼è‡´æ•´ä½“éªŒæ”¶ç«‹å³å¤±è´¥ï¼Œä¸ä¼šç»§ç»­åç»­æ£€æŸ¥
- å¿…é¡»å…ˆé€šè¿‡ä¾èµ–ã€ç¼–è¯‘ã€å¯åŠ¨éªŒè¯ï¼Œå†è¿›è¡ŒåŠŸèƒ½éªŒè¯
- éªŒæ”¶ç­–ç•¥å¿…é¡»åŒ…å«è‡³å°‘ï¼š1ä¸ªä¾èµ–æ£€æŸ¥ + 1ä¸ªç¼–è¯‘/å¯¼å…¥æ£€æŸ¥ + 1ä¸ªå¯åŠ¨æ£€æŸ¥
'''

# Prompt for analyzing validation results
RESULT_ANALYSIS_PROMPT = '''ä½ æ˜¯ä¸€ä¸ªéªŒæ”¶ä¸“å®¶ã€‚æ ¹æ®ä»¥ä¸‹éªŒæ”¶ç»“æœï¼Œåˆ¤æ–­äº§ç‰©æ˜¯å¦æ»¡è¶³äº¤ä»˜æ ‡å‡†ã€‚

## ç”¨æˆ·åŸå§‹éœ€æ±‚
{user_requirement}

## äº¤ä»˜æ ‡å‡†
{acceptance_criteria}

## éªŒæ”¶ç»“æœ
{results}

## ä»»åŠ¡
åˆ†æéªŒæ”¶ç»“æœï¼Œå¯¹ç…§äº¤ä»˜æ ‡å‡†ç»™å‡ºåˆ¤æ–­ï¼š
1. æ¯æ¡äº¤ä»˜æ ‡å‡†æ˜¯å¦æ»¡è¶³
2. æ€»ä½“æ˜¯å¦é€šè¿‡éªŒæ”¶
3. å…·ä½“é—®é¢˜å’Œæ”¹è¿›å»ºè®®

### ã€é‡è¦ã€‘éªŒæ”¶è§„åˆ™
- éªŒæ”¶ç»“æœåŒ…å« phase å­—æ®µï¼ˆdependency/compile/startup/functionï¼‰å’Œ critical å­—æ®µ
- ã€å¼ºåˆ¶ã€‘å¦‚æœä»»ä½• critical=true çš„æ£€æŸ¥å¤±è´¥ï¼Œæ•´ä½“éªŒæ”¶å¿…é¡»ä¸º "failed"ï¼Œscore=0
- ã€å¼ºåˆ¶ã€‘dependency/compile/startup é˜¶æ®µçš„å¤±è´¥ä¸èƒ½è¢«å¿½ç•¥ï¼Œè¿™äº›æ˜¯ä»£ç å¯è¿è¡Œçš„å‰æ
- åªæœ‰å½“æ‰€æœ‰ critical æ£€æŸ¥éƒ½é€šè¿‡åï¼Œæ‰æ ¹æ®åŠŸèƒ½éªŒè¯ç»“æœåˆ¤æ–­æœ€ç»ˆçŠ¶æ€

è¾“å‡ºJSONæ ¼å¼ï¼š
```json
{{
  "deliverable_assessment": {{
    "criterion_1": {{"met": true/false, "evidence": "ä¾æ®", "issues": []}},
    "criterion_2": {{"met": true/false, "evidence": "ä¾æ®", "issues": []}}
  }},
  "status": "passed/failed/partial",
  "score": 0-100,
  "summary": "éªŒæ”¶æ€»ç»“ï¼Œè¯´æ˜äº§ç‰©æ˜¯å¦æ»¡è¶³éœ€æ±‚",
  "critical_issues": ["ä¸¥é‡é—®é¢˜åˆ—è¡¨"],
  "recommendations": ["æ”¹è¿›å»ºè®®"]
}}
```
'''


class AcceptanceAgent:
    """LLM-driven acceptance agent for multi-environment validation.

    This agent:
    1. Analyzes deliverables to understand what needs validation
    2. Uses LLM to decide which sandbox environments to use
    3. Executes validation in appropriate environments
    4. Verifies deliverables meet acceptance criteria

    Supported environments:
    - CLI: Shell commands, file checks, test execution
    - Browser: Web UI testing with Playwright
    - API: HTTP endpoint testing
    - GUI: Desktop application testing
    - Mobile: Mobile app testing

    Example:
        >>> agent = AcceptanceAgent(model=model, orchestrator=orchestrator)
        >>> result = await agent.validate(
        ...     workspace_dir="/workspace",
        ...     user_requirement="åˆ›å»ºä¸€ä¸ªå¥èº«ç³»ç»Ÿ",
        ...     acceptance_criteria=["åç«¯APIå¯è®¿é—®", "å‰ç«¯é¡µé¢å¯æ­£å¸¸æ˜¾ç¤º"],
        ...     artifact_type="web",
        ... )
        >>> if result.passed:
        ...     print("éªŒæ”¶é€šè¿‡")
        ... else:
        ...     for issue in result.recommendations:
        ...         print(f"é—®é¢˜: {issue}")
    """

    def __init__(
        self,
        *,
        model: "ChatModelBase",
        sandbox_orchestrator: "SandboxOrchestrator | None" = None,
        runtime_workspace: "RuntimeWorkspace | None" = None,
        playwright_mcp: "Any | None" = None,
        http_port: int | None = None,
        default_timeout: int = 120,
        project_id: str | None = None,
    ) -> None:
        """Initialize acceptance agent.

        Args:
            model: LLM model for generating validation strategies.
            sandbox_orchestrator: Orchestrator for multi-sandbox execution.
            runtime_workspace: RuntimeWorkspace instance for executing in same Docker sandbox.
                If provided, will be used instead of creating new sandboxes.
            playwright_mcp: Playwright MCP client for browser testing.
                Can be StatefulClientBase or BrowserSandboxManager.
            http_port: HTTP server port for serving web content to browser.
            default_timeout: Default timeout for validation actions.
            project_id: Optional project ID for observability tracking.
        """
        self._model = model
        self._orchestrator = sandbox_orchestrator
        self._runtime_workspace = runtime_workspace
        self._playwright_mcp = playwright_mcp
        self._http_port = http_port
        self._default_timeout = default_timeout
        self._project_id = project_id

        # Initialize ObservabilityHub for timeline tracking
        try:
            from ..observability import ObservabilityHub
            self._hub = ObservabilityHub()
        except Exception:
            self._hub = None

    def _emit_event(
        self,
        event_type: str,
        message: str,
        *,
        level: str = "info",
        metadata: dict | None = None,
    ) -> None:
        """Emit a progress event to console and ObservabilityHub.

        Args:
            event_type: Type of event (acceptance_start/end/step/check).
            message: Human-readable message.
            level: Log level (info, warning, error).
            metadata: Additional event metadata.
        """
        _log_progress(
            message,
            level=level,
            hub=self._hub,
            event_type=event_type,
            metadata=metadata,
            project_id=self._project_id,
        )

    async def validate(
        self,
        *,
        workspace_dir: str = "/workspace",
        user_requirement: str = "",
        acceptance_criteria: list[str] | None = None,
        artifact_type: str = "web",
        file_list: list[str] | None = None,
        serve_url: str | None = None,
    ) -> AcceptanceResult:
        """Run acceptance validation on deliverables.

        Args:
            workspace_dir: Directory containing project files.
            user_requirement: Original user requirement.
            acceptance_criteria: List of acceptance criteria to verify.
            artifact_type: Type of artifact (web, api, cli, mobile, etc.).
            file_list: List of files in workspace (auto-detected if None).
            serve_url: URL where the app is served (for browser validation).

        Returns:
            AcceptanceResult with validation status and details.
        """
        validation_start = time.time()
        self._emit_event("acceptance_start", "=" * 60)
        self._emit_event(
            "acceptance_start",
            "ğŸ” å¼€å§‹éªŒæ”¶æµç¨‹",
            metadata={"artifact_type": artifact_type, "workspace": workspace_dir},
        )
        self._emit_event("acceptance_step", f"   äº§ç‰©ç±»å‹: {artifact_type}")
        self._emit_event("acceptance_step", f"   å·¥ä½œåŒº: {workspace_dir}")
        if acceptance_criteria:
            self._emit_event(
                "acceptance_step",
                f"   éªŒæ”¶æ ‡å‡†: {len(acceptance_criteria)} æ¡",
                metadata={"criteria_count": len(acceptance_criteria)},
            )

        # Step 1: Get file list if not provided
        if file_list is None:
            self._emit_event("acceptance_step", "ğŸ“ æ­¥éª¤1: æ‰«æå·¥ä½œåŒºæ–‡ä»¶...")
            file_list = await self._list_workspace_files(workspace_dir)
            self._emit_event(
                "acceptance_step",
                f"   æ‰¾åˆ° {len(file_list)} ä¸ªæ–‡ä»¶",
                metadata={"file_count": len(file_list)},
            )

        if not file_list:
            return AcceptanceResult(
                status=AcceptanceStatus.ERROR,
                score=0.0,
                summary="å·¥ä½œåŒºä¸ºç©ºï¼Œæ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡ä»¶",
                recommendations=["è¯·å…ˆç”Ÿæˆé¡¹ç›®æ–‡ä»¶"],
            )

        # Step 2: Generate validation strategy using LLM
        self._emit_event("acceptance_step", "ğŸ§  æ­¥éª¤2: ç”ŸæˆéªŒæ”¶ç­–ç•¥ï¼ˆè°ƒç”¨LLMï¼‰...")
        strategy_start = time.time()

        criteria_str = "\n".join(
            f"- {i+1}. {c}" for i, c in enumerate(acceptance_criteria or [])
        )
        if not criteria_str:
            criteria_str = "- é¡¹ç›®æ–‡ä»¶å®Œæ•´\n- ä»£ç å¯è¿è¡Œ\n- åŸºæœ¬åŠŸèƒ½æ­£å¸¸"

        available_envs = self._get_available_environments()

        strategy = await self._generate_validation_strategy(
            user_requirement=user_requirement,
            acceptance_criteria=criteria_str,
            artifact_type=artifact_type,
            file_list=file_list,
            available_environments=available_envs,
        )

        strategy_elapsed = time.time() - strategy_start
        if strategy is None:
            self._emit_event(
                "acceptance_step",
                f"   âŒ ç­–ç•¥ç”Ÿæˆå¤±è´¥ (è€—æ—¶: {strategy_elapsed:.1f}s)",
                level="error",
                metadata={"elapsed_s": strategy_elapsed, "success": False},
            )
            return AcceptanceResult(
                status=AcceptanceStatus.ERROR,
                score=0.0,
                summary="æ— æ³•ç”ŸæˆéªŒæ”¶ç­–ç•¥",
                recommendations=["è¯·æ£€æŸ¥LLMé…ç½®"],
            )

        check_count = len(strategy.get("validation_checks", []))
        self._emit_event(
            "acceptance_step",
            f"   âœ“ ç­–ç•¥ç”Ÿæˆå®Œæˆ (è€—æ—¶: {strategy_elapsed:.1f}s)",
            metadata={"elapsed_s": strategy_elapsed, "check_count": check_count},
        )
        self._emit_event("acceptance_step", f"   ç”Ÿæˆ {check_count} ä¸ªéªŒæ”¶æ£€æŸ¥é¡¹")

        # Step 2.5: Prepare services (install deps & start services) if needed
        needs_api = any(
            c.get("environment") == "api"
            for c in strategy.get("validation_checks", [])
        )
        service_info = None
        service_prep_failed = False
        service_prep_errors: list[str] = []
        if needs_api:
            self._emit_event("acceptance_step", "ğŸš€ æ­¥éª¤2.5: å‡†å¤‡æœåŠ¡ï¼ˆå®‰è£…ä¾èµ–ã€å¯åŠ¨æœåŠ¡ï¼‰...")
            prep_start = time.time()
            prep_result = await self._prepare_services(
                workspace_dir=workspace_dir,
                file_list=file_list,
                artifact_type=artifact_type,
            )
            prep_elapsed = time.time() - prep_start
            if prep_result.get("services"):
                service_info = prep_result
                self._emit_event(
                    "acceptance_step",
                    f"   âœ“ æœåŠ¡å‡†å¤‡å®Œæˆ (è€—æ—¶: {prep_elapsed:.1f}s)",
                    metadata={"elapsed_s": prep_elapsed, "success": True},
                )
            if not prep_result.get("success"):
                service_prep_failed = True
                service_prep_errors = prep_result.get("errors", [])
                self._emit_event(
                    "acceptance_step",
                    f"   âš  æœåŠ¡å‡†å¤‡å¤±è´¥: {'; '.join(service_prep_errors[:2])}",
                    level="warning",
                    metadata={"errors": service_prep_errors},
                )

        # Step 3: Execute validation checks in appropriate environments
        self._emit_event("acceptance_step", f"âœ… æ­¥éª¤3: æ‰§è¡Œ {check_count} ä¸ªéªŒæ”¶æ£€æŸ¥...")
        checks_start = time.time()
        checks = await self._execute_validation_checks(
            strategy=strategy,
            workspace_dir=workspace_dir,
            serve_url=serve_url,
            skip_api_if_service_failed=service_prep_failed,
            service_prep_errors=service_prep_errors,
        )
        checks_elapsed = time.time() - checks_start

        # Summary of check results
        passed_checks = sum(1 for c in checks if c.passed)
        failed_checks = len(checks) - passed_checks
        self._emit_event(
            "acceptance_step",
            f"   æ£€æŸ¥å®Œæˆ (è€—æ—¶: {checks_elapsed:.1f}s)",
            metadata={"elapsed_s": checks_elapsed},
        )
        self._emit_event(
            "acceptance_step",
            f"   ç»“æœ: âœ“ {passed_checks} é€šè¿‡, âœ— {failed_checks} å¤±è´¥",
            metadata={"passed": passed_checks, "failed": failed_checks},
        )

        # Step 4: Analyze results using LLM
        self._emit_event("acceptance_step", "ğŸ“Š æ­¥éª¤4: åˆ†æéªŒæ”¶ç»“æœï¼ˆè°ƒç”¨LLMï¼‰...")
        analyze_start = time.time()
        result = await self._analyze_results(
            checks=checks,
            user_requirement=user_requirement,
            acceptance_criteria=criteria_str,
        )
        analyze_elapsed = time.time() - analyze_start
        self._emit_event(
            "acceptance_step",
            f"   åˆ†æå®Œæˆ (è€—æ—¶: {analyze_elapsed:.1f}s)",
            metadata={"elapsed_s": analyze_elapsed},
        )

        # Final summary
        total_elapsed = time.time() - validation_start
        self._emit_event("acceptance_end", "=" * 60)
        status_emoji = "âœ…" if result.passed else "âŒ"
        self._emit_event(
            "acceptance_end",
            f"{status_emoji} éªŒæ”¶ç»“æœ: {result.status.value.upper()}",
            metadata={
                "status": result.status.value,
                "score": result.score,
                "total_elapsed_s": total_elapsed,
            },
        )
        self._emit_event("acceptance_end", f"   å¾—åˆ†: {result.score * 100:.0f}%")
        self._emit_event("acceptance_end", f"   æ€»è€—æ—¶: {total_elapsed:.1f}s")
        if result.summary:
            self._emit_event("acceptance_end", f"   æ‘˜è¦: {result.summary[:100]}...")
        self._emit_event("acceptance_end", "=" * 60)

        return result

    async def _call_model(self, prompt: str) -> Any:
        """Call the LLM model with proper format.

        Handles different model types (OpenAIChatModel, etc.) with their
        expected input formats.
        """
        from ..message import Msg

        msg = Msg(name="acceptance_agent", role="user", content=prompt)

        # OpenAIChatModel ä½¿ç”¨ __call__ æ–¹æ³•ï¼ŒæœŸæœ› list[dict]
        if hasattr(self._model, "__call__") and not hasattr(self._model, "reply"):
            messages = [{"role": "user", "content": prompt}]
            return await self._model(messages)

        # å…¶ä»–æ¨¡å‹ä½¿ç”¨ reply æ–¹æ³•
        if hasattr(self._model, "reply"):
            return await self._model.reply(msg)

        raise AttributeError("Model has no reply or __call__ method")

    def _extract_text_content(self, response: Any) -> str:
        """Extract text content from model response.

        Handles different response formats:
        - ChatResponse with content list of TextBlock/TypedDict
        - Dict with content field
        - Msg with content attribute
        - Plain string
        """
        # Handle None
        if response is None:
            return ""

        # Handle string directly
        if isinstance(response, str):
            return response

        # Handle ChatResponse or dict-like with 'content' field
        content = None
        if hasattr(response, "content"):
            content = response.content
        elif isinstance(response, dict) and "content" in response:
            content = response["content"]

        if content is None:
            return str(response)

        # Handle list of blocks (TextBlock, etc.)
        if isinstance(content, (list, tuple)):
            text_parts = []
            for block in content:
                if isinstance(block, dict):
                    # TextBlock is TypedDict, access via ["text"]
                    if block.get("type") == "text" and "text" in block:
                        text_parts.append(block["text"])
                    elif "text" in block:
                        text_parts.append(block["text"])
                elif hasattr(block, "text"):
                    # Object with text attribute
                    text_parts.append(block.text)
            return "\n".join(text_parts) if text_parts else str(content)

        # Handle string content
        if isinstance(content, str):
            return content

        return str(content)

    def _get_available_environments(self) -> str:
        """Get description of available validation environments."""
        envs = []

        # Always available
        envs.append("- cli: å‘½ä»¤è¡Œæ‰§è¡Œï¼ˆshellå‘½ä»¤ã€æ–‡ä»¶æ£€æŸ¥ã€æµ‹è¯•è¿è¡Œï¼‰")

        # Check sandbox capabilities
        from .sandbox_orchestrator import SandboxTypeEnum

        envs.append("- browser: æµè§ˆå™¨è‡ªåŠ¨åŒ–ï¼ˆPlaywrightï¼Œç”¨äºWeb UIæµ‹è¯•ï¼‰")
        envs.append("- api: HTTP APIæµ‹è¯•ï¼ˆè¯·æ±‚å‘é€ã€å“åº”éªŒè¯ï¼‰")

        # Optional based on orchestrator config
        envs.append("- gui: æ¡Œé¢åº”ç”¨æµ‹è¯•ï¼ˆComputer Useï¼‰")
        envs.append("- mobile: ç§»åŠ¨åº”ç”¨æµ‹è¯•")
        envs.append("- visual: è§†è§‰å¯¹æ¯”ï¼ˆæˆªå›¾å¯¹æ¯”ï¼‰")

        return "\n".join(envs)

    async def _list_workspace_files(self, workspace_dir: str) -> list[str]:
        """List files in workspace directory."""
        # Prefer RuntimeWorkspace if available (uses same Docker sandbox)
        if self._runtime_workspace and self._runtime_workspace.is_initialized:
            try:
                # Use relative path from workspace_dir to RuntimeWorkspace base
                # This handles cases where workspace_dir might be delivery_dir
                # or working_dir different from RuntimeWorkspace.workspace_dir
                rt_base = getattr(self._runtime_workspace, "base_workspace_dir", "/workspace")
                if workspace_dir.startswith(rt_base):
                    # Extract relative path from base
                    rel_path = workspace_dir[len(rt_base):].lstrip("/")
                else:
                    rel_path = ""
                files = self._runtime_workspace.list_directory(rel_path)
                if files:
                    # Recursively get all files
                    all_files = []
                    self._collect_files_recursive(all_files, "", files, base_path=rel_path)
                    return all_files[:100]
            except Exception as exc:
                logger.warning("Failed to list files via RuntimeWorkspace: %s", exc)

        # Fallback to SandboxOrchestrator
        if self._orchestrator:
            try:
                result = self._orchestrator.execute_command(
                    f"find {workspace_dir} -type f -name '*' | head -100",
                    timeout=30,
                )
                if result.success:
                    files = [
                        f.replace(workspace_dir + "/", "")
                        for f in result.output.strip().split("\n")
                        if f
                    ]
                    return files
            except Exception as exc:
                logger.warning("Failed to list workspace files: %s", exc)

        return []

    def _collect_files_recursive(
        self,
        all_files: list[str],
        prefix: str,
        entries: list[str],
        *,
        base_path: str = "",
    ) -> None:
        """Recursively collect files from directory listing.

        Args:
            all_files: List to append file paths to.
            prefix: Current path prefix for file names.
            entries: Directory entries to process.
            base_path: Base path in container (e.g., "delivery" for /workspace/delivery).
        """
        for entry in entries:
            if entry.startswith(".") or entry.startswith("_"):
                continue
            # File path relative to workspace_dir (for returned results)
            file_path = f"{prefix}/{entry}" if prefix else entry
            # Container path for list_directory calls
            container_path = f"{base_path}/{file_path}".lstrip("/") if base_path else file_path
            try:
                # Try to list as directory
                sub_entries = self._runtime_workspace.list_directory(container_path)
                if sub_entries:
                    self._collect_files_recursive(
                        all_files, file_path, sub_entries, base_path=base_path
                    )
                else:
                    all_files.append(file_path)
            except Exception:
                # Probably a file, not a directory
                all_files.append(file_path)

    async def _generate_validation_strategy(
        self,
        *,
        user_requirement: str,
        acceptance_criteria: str,
        artifact_type: str,
        file_list: list[str],
        available_environments: str,
    ) -> dict[str, Any] | None:
        """Use LLM to generate validation strategy."""
        prompt = VALIDATION_STRATEGY_PROMPT.format(
            user_requirement=user_requirement or "æœªæŒ‡å®š",
            acceptance_criteria=acceptance_criteria,
            artifact_type=artifact_type,
            file_list="\n".join(f"- {f}" for f in file_list[:50]),
            available_environments=available_environments,
        )

        try:
            response = await self._call_model(prompt)
            content = self._extract_text_content(response)

            strategy = self._extract_json(content)
            if strategy and "validation_checks" in strategy:
                logger.info(
                    "Generated validation strategy with %d checks",
                    len(strategy["validation_checks"]),
                )
                return strategy
        except Exception as exc:
            logger.error("Failed to generate validation strategy: %s", exc)

        return None

    async def _prepare_services(
        self,
        *,
        workspace_dir: str,
        file_list: list[str],
        artifact_type: str,
    ) -> dict[str, Any]:
        """Prepare services using framework-specific agents.

        This method:
        1. Uses FrameworkDetector to identify frameworks in the project
        2. Gets startup strategy from framework-specific agents
        3. Falls back to LLM if no framework is detected
        4. Executes commands to install deps and start services

        Returns:
            dict with keys: success, services, errors
        """
        result = {
            "success": False,
            "services": [],
            "errors": [],
        }

        # Use LLM to generate startup strategy
        startup_strategy = await self._generate_startup_strategy(
            workspace_dir=workspace_dir,
            file_list=file_list,
            artifact_type=artifact_type,
        )

        if not startup_strategy:
            result["errors"].append("Failed to generate startup strategy")
            return result

        commands = startup_strategy.get("commands", [])

        # Execute startup commands
        total_cmds = len(commands)
        for cmd_idx, cmd_spec in enumerate(commands, 1):
            cmd = cmd_spec.get("command", "")
            desc = cmd_spec.get("description", cmd[:50])
            cmd_working_dir = cmd_spec.get("working_dir", "")
            timeout = cmd_spec.get("timeout", 120)
            background = cmd_spec.get("background", False)

            # Handle working_dir: RuntimeWorkspace.execute_command expects
            # a path RELATIVE to workspace_dir (it will prepend workspace_dir)
            # So we need to extract just the relative part

            # Strip /workspace prefix if LLM returned absolute path
            if cmd_working_dir.startswith("/workspace/"):
                cmd_working_dir = cmd_working_dir[len("/workspace/"):]
            elif cmd_working_dir == "/workspace":
                cmd_working_dir = ""

            # Also strip workspace_dir prefix if it was included
            if cmd_working_dir.startswith(workspace_dir + "/"):
                cmd_working_dir = cmd_working_dir[len(workspace_dir) + 1:]
            elif cmd_working_dir == workspace_dir:
                cmd_working_dir = ""

            # Now cmd_working_dir should be relative (e.g., "backend", "frontend")
            # or empty string for workspace root
            # Pass it directly - RuntimeWorkspace will prepend workspace_dir
            effective_working_dir = cmd_working_dir if cmd_working_dir else None

            if not cmd:
                continue

            _log_progress(f"      [{cmd_idx}/{total_cmds}] æ‰§è¡Œ: {desc}")
            _log_progress(f"               å‘½ä»¤: {cmd[:80]}{'...' if len(cmd) > 80 else ''}")

            # For background commands, wrap with nohup
            if background:
                cmd = f"nohup {cmd} > /tmp/service_{len(result['services'])}.log 2>&1 & echo $!"

            cmd_start = time.time()
            exec_result = await self._execute_command_in_workspace(
                cmd,
                working_dir=effective_working_dir,
                timeout=timeout,
            )
            cmd_elapsed = time.time() - cmd_start

            if exec_result["success"]:
                _log_progress(f"               âœ“ æˆåŠŸ ({cmd_elapsed:.1f}s)")
                if background and exec_result["output"].strip():
                    result["services"].append({
                        "description": desc,
                        "pid": exec_result["output"].strip(),
                    })
            else:
                error_msg = f"{desc}: {exec_result['error'][:200]}"
                result["errors"].append(error_msg)
                _log_progress(f"               âœ— å¤±è´¥ ({cmd_elapsed:.1f}s)", level="warning")
                _log_progress(f"               é”™è¯¯: {exec_result['error'][:100]}", level="warning")

                # Check if this is a critical command
                if cmd_spec.get("critical", False):
                    _log_progress("      âš  å…³é”®å‘½ä»¤å¤±è´¥ï¼Œç»ˆæ­¢æœåŠ¡å‡†å¤‡", level="warning")
                    return result

        # Wait for services to initialize if any were started
        if result["services"]:
            wait_time = startup_strategy.get("wait_after_start", 3)
            _log_progress(f"      â³ ç­‰å¾… {wait_time}s è®©æœåŠ¡åˆå§‹åŒ–...")
            await asyncio.sleep(wait_time)

        result["success"] = len(result["errors"]) == 0
        return result

    async def _generate_startup_strategy(
        self,
        *,
        workspace_dir: str,
        file_list: list[str],
        artifact_type: str,
    ) -> dict[str, Any] | None:
        """Use LLM to generate service startup strategy."""
        # Show first 50 files to avoid token overflow
        files_str = "\n".join(file_list[:50])
        if len(file_list) > 50:
            files_str += f"\n... and {len(file_list) - 50} more files"

        prompt = f'''ä½ æ˜¯ä¸€ä¸ª DevOps ä¸“å®¶ã€‚åˆ†æä»¥ä¸‹é¡¹ç›®æ–‡ä»¶ï¼Œç”Ÿæˆå¯åŠ¨æœåŠ¡æ‰€éœ€çš„å‘½ä»¤ã€‚

## å·¥ä½œåŒºç›®å½•
{workspace_dir}

## é¡¹ç›®æ–‡ä»¶
{files_str}

## äº§ç‰©ç±»å‹
{artifact_type}

## ä»»åŠ¡
åˆ†æé¡¹ç›®ç»“æ„ï¼Œç”Ÿæˆå¯åŠ¨æœåŠ¡æ‰€éœ€çš„å‘½ä»¤åºåˆ—ã€‚ä½ éœ€è¦ï¼š
1. è¯†åˆ«é¡¹ç›®ç±»å‹ï¼ˆPython/Node.js/Go/Java ç­‰ï¼‰
2. ç¡®å®šä¾èµ–å®‰è£…å‘½ä»¤
3. ç¡®å®šæœåŠ¡å¯åŠ¨å‘½ä»¤
4. ç¡®å®šæœåŠ¡è¿è¡Œçš„ç«¯å£

è¾“å‡ºJSONæ ¼å¼ï¼š
```json
{{
  "project_analysis": {{
    "type": "é¡¹ç›®ç±»å‹",
    "framework": "ä½¿ç”¨çš„æ¡†æ¶",
    "entry_point": "å…¥å£æ–‡ä»¶"
  }},
  "commands": [
    {{
      "description": "å‘½ä»¤æè¿°",
      "command": "è¦æ‰§è¡Œçš„å‘½ä»¤",
      "working_dir": "ç›¸å¯¹è·¯å¾„ï¼Œå¦‚ backend æˆ– frontendï¼Œæˆ–ç•™ç©ºè¡¨ç¤ºæ ¹ç›®å½•",
      "timeout": 120,
      "background": false,
      "critical": true
    }}
  ],
  "services": [
    {{
      "name": "æœåŠ¡åç§°",
      "port": 8000,
      "health_check": "http://localhost:8000/health"
    }}
  ],
  "wait_after_start": 3
}}
```

ã€é‡è¦ã€‘working_dir ä½¿ç”¨è§„åˆ™ï¼š
- ä½¿ç”¨**ç›¸å¯¹è·¯å¾„**ï¼Œå¦‚ "backend" æˆ– "frontend"ï¼Œä¸è¦ä½¿ç”¨ç»å¯¹è·¯å¾„
- å¦‚æœå‘½ä»¤éœ€è¦åœ¨æ ¹ç›®å½•æ‰§è¡Œï¼Œworking_dir è®¾ä¸ºç©ºå­—ç¬¦ä¸² ""
- é”™è¯¯ç¤ºä¾‹ï¼š"/workspace/backend"ï¼ˆä¸è¦è¿™æ ·å†™ï¼‰
- æ­£ç¡®ç¤ºä¾‹ï¼š"backend"ï¼ˆè¿™æ ·å†™ï¼‰

æ³¨æ„ï¼š
- ã€é‡è¦ã€‘åªæ ¹æ®ä¸Šé¢åˆ—å‡ºçš„å®é™…æ–‡ä»¶æ¥ç”Ÿæˆå‘½ä»¤ï¼Œä¸è¦å‡è®¾æ–‡ä»¶å­˜åœ¨
- å¦‚æœæ²¡æœ‰ requirements.txtï¼Œä¸è¦æ‰§è¡Œ pip install -r requirements.txt
- å¦‚æœæ²¡æœ‰ package.jsonï¼Œä¸è¦æ‰§è¡Œ npm install
- ä¾èµ–å®‰è£…å‘½ä»¤åº”è¯¥åœ¨æœåŠ¡å¯åŠ¨å‘½ä»¤ä¹‹å‰
- åå°æœåŠ¡éœ€è¦è®¾ç½® "background": true
- critical è¡¨ç¤ºå¦‚æœå‘½ä»¤å¤±è´¥æ˜¯å¦åº”è¯¥ä¸­æ­¢åç»­å‘½ä»¤
- æ ¹æ®å®é™…é¡¹ç›®é…ç½®ç¡®å®šç«¯å£å·ï¼Œä¸è¦å‡è®¾å›ºå®šç«¯å£
- å¦‚æœæ˜¯ Python FastAPI é¡¹ç›®ï¼Œä½¿ç”¨ uvicorn å¯åŠ¨
- å¦‚æœæ˜¯ Node.js é¡¹ç›®ï¼Œä½¿ç”¨ npm run dev æˆ– npm start
- å¯¹äºç®€å•çš„é™æ€ HTML é¡µé¢ï¼Œä¸éœ€è¦å¯åŠ¨ä»»ä½•æœåŠ¡ï¼Œè¿”å›ç©ºçš„ commands åˆ—è¡¨
'''

        try:
            response = await self._call_model(prompt)
            content = self._extract_text_content(response)
            strategy = self._extract_json(content)

            if strategy and "commands" in strategy:
                logger.info(
                    "Generated startup strategy with %d commands",
                    len(strategy["commands"]),
                )
                return strategy
        except Exception as exc:
            logger.error("Failed to generate startup strategy: %s", exc)

        return None

    async def _execute_validation_checks(
        self,
        *,
        strategy: dict[str, Any],
        workspace_dir: str,
        serve_url: str | None,
        skip_api_if_service_failed: bool = False,
        service_prep_errors: list[str] | None = None,
    ) -> list[ValidationCheck]:
        """Execute validation checks in appropriate environments.

        Args:
            strategy: Validation strategy with checks to execute.
            workspace_dir: Workspace directory path.
            serve_url: URL for browser tests.
            skip_api_if_service_failed: If True, skip API checks and mark as failed.
            service_prep_errors: Error messages from service preparation.

        Returns:
            List of ValidationCheck results. If a critical check fails,
            subsequent checks are skipped.
        """
        checks: list[ValidationCheck] = []
        critical_failed = False  # Track if any critical check failed
        total_checks = len(strategy.get("validation_checks", []))

        for idx, check_spec in enumerate(strategy.get("validation_checks", []), 1):
            name = check_spec.get("name", "Unknown")
            description = check_spec.get("description", "")
            env_str = check_spec.get("environment", "cli")
            action = check_spec.get("action", {})
            is_critical = check_spec.get("critical", False)
            phase = check_spec.get("phase", "function")

            # Skip remaining checks if a critical check has already failed
            if critical_failed:
                check = ValidationCheck(
                    name=name,
                    description=description,
                    environment=ValidationEnvironment.CLI,
                    action=action,
                    passed=False,
                    error="è·³è¿‡ï¼šå‰ç½®å…³é”®æ£€æŸ¥å¤±è´¥",
                    critical=is_critical,
                    phase=phase,
                )
                checks.append(check)
                self._emit_event(
                    "acceptance_check",
                    f"   [{idx}/{total_checks}] â­ {name} - è·³è¿‡(å‰ç½®å¤±è´¥)",
                    metadata={"name": name, "skipped": True},
                )
                continue

            try:
                env = ValidationEnvironment(env_str)
            except ValueError:
                env = ValidationEnvironment.CLI

            # Log check start with phase info
            phase_emoji = {"dependency": "ğŸ“¦", "compile": "ğŸ”¨", "startup": "ğŸš€", "function": "âš™ï¸"}.get(phase, "ğŸ“‹")
            critical_marker = "[å…³é”®]" if is_critical else ""
            self._emit_event(
                "acceptance_check",
                f"   [{idx}/{total_checks}] {phase_emoji} {name} {critical_marker}",
                metadata={"name": name, "phase": phase, "critical": is_critical, "index": idx},
            )
            self._emit_event("acceptance_check", f"            ç¯å¢ƒ: {env.value}, é˜¶æ®µ: {phase}")
            check_start = time.time()

            try:
                if env == ValidationEnvironment.CLI:
                    check = await self._execute_cli_check(
                        name=name,
                        description=description,
                        action=action,
                        workspace_dir=workspace_dir,
                    )
                elif env == ValidationEnvironment.BROWSER:
                    check = await self._execute_browser_check(
                        name=name,
                        description=description,
                        action=action,
                        serve_url=serve_url,
                    )
                elif env == ValidationEnvironment.API:
                    # Skip API checks if service preparation failed
                    if skip_api_if_service_failed:
                        error_msg = "æœåŠ¡å¯åŠ¨å¤±è´¥ï¼Œè·³è¿‡ API éªŒè¯"
                        if service_prep_errors:
                            error_msg += f": {'; '.join(service_prep_errors[:2])}"
                        check = ValidationCheck(
                            name=name,
                            description=description,
                            environment=env,
                            action=action,
                            passed=False,
                            error=error_msg,
                            critical=is_critical,
                            phase=phase,
                        )
                    else:
                        check = await self._execute_api_check(
                            name=name,
                            description=description,
                            action=action,
                        )
                else:
                    # Unsupported environment - try CLI fallback
                    check = ValidationCheck(
                        name=name,
                        description=description,
                        environment=env,
                        action=action,
                        passed=False,
                        error=f"Environment {env.value} not yet implemented",
                        critical=is_critical,
                        phase=phase,
                    )
            except Exception as exc:
                check = ValidationCheck(
                    name=name,
                    description=description,
                    environment=env,
                    action=action,
                    passed=False,
                    error=str(exc),
                    critical=is_critical,
                    phase=phase,
                )

            # Set critical and phase on check (in case it was created by sub-methods)
            check.critical = is_critical
            check.phase = phase

            checks.append(check)
            check_elapsed = time.time() - check_start

            # Log check result with details
            if check.passed:
                self._emit_event(
                    "acceptance_check",
                    f"            âœ“ é€šè¿‡ ({check_elapsed:.1f}s)",
                    metadata={"name": name, "passed": True, "elapsed_s": check_elapsed},
                )
                if check.output and len(check.output) < 100:
                    self._emit_event("acceptance_check", f"            è¾“å‡º: {check.output}")
            else:
                self._emit_event(
                    "acceptance_check",
                    f"            âœ— å¤±è´¥ ({check_elapsed:.1f}s)",
                    level="warning",
                    metadata={"name": name, "passed": False, "elapsed_s": check_elapsed, "error": check.error},
                )
                if check.error:
                    self._emit_event(
                        "acceptance_check",
                        f"            é”™è¯¯: {check.error[:150]}",
                        level="warning",
                    )

            # If critical check failed, mark for skipping subsequent checks
            if is_critical and not check.passed:
                critical_failed = True
                self._emit_event(
                    "acceptance_check",
                    f"   âš  å…³é”®æ£€æŸ¥ '{name}' å¤±è´¥ï¼Œåç»­æ£€æŸ¥å°†è¢«è·³è¿‡",
                    level="warning",
                    metadata={"critical_failure": True, "name": name},
                )

        return checks

    async def _execute_command_in_workspace(
        self, command: str, *, working_dir: str | None = None, timeout: int = 300
    ) -> dict[str, Any]:
        """Execute command using RuntimeWorkspace or SandboxOrchestrator.

        Returns dict with keys: success, output, error
        """
        # Prefer RuntimeWorkspace if available (same Docker sandbox)
        if self._runtime_workspace and self._runtime_workspace.is_initialized:
            # execute_command is synchronous, run in executor to avoid blocking
            import asyncio
            loop = asyncio.get_event_loop()
            result = await loop.run_in_executor(
                None,
                lambda: self._runtime_workspace.execute_command(
                    command, working_dir=working_dir, timeout=timeout
                )
            )
            return {
                "success": result.get("success", False),
                "output": result.get("output", ""),
                "error": result.get("error", ""),
            }

        # Fallback to SandboxOrchestrator
        if self._orchestrator:
            result = self._orchestrator.execute_command(
                command, working_dir=working_dir, timeout=timeout
            )
            return {
                "success": result.success,
                "output": result.output or "",
                "error": result.error or "",
            }

        return {"success": False, "output": "", "error": "No execution backend available"}

    async def _execute_cli_check(
        self,
        *,
        name: str,
        description: str,
        action: dict[str, Any],
        workspace_dir: str,
    ) -> ValidationCheck:
        """Execute a CLI validation check."""
        action_type = action.get("type", "shell")

        if action_type == "shell":
            command = action.get("command", "echo 'no command'")
            timeout = action.get("timeout", self._default_timeout)

            # Use unified execution method (prefers RuntimeWorkspace)
            result = await self._execute_command_in_workspace(
                command, working_dir=workspace_dir, timeout=timeout
            )

            return ValidationCheck(
                name=name,
                description=description,
                environment=ValidationEnvironment.CLI,
                action=action,
                passed=result["success"],
                output=result["output"][:2000] if result["output"] else "",
                error=result["error"] if not result["success"] else None,
            )

        elif action_type == "file_check":
            paths = action.get("paths", [])
            check_type = action.get("check", "exists")

            all_passed = True
            outputs = []

            for path in paths:
                full_path = f"{workspace_dir}/{path}"
                if check_type == "exists":
                    result = await self._execute_command_in_workspace(
                        f"test -e {full_path} && echo 'EXISTS' || echo 'NOT_FOUND'",
                        timeout=10,
                    )
                    passed = "EXISTS" in result["output"]
                elif check_type == "not_empty":
                    result = await self._execute_command_in_workspace(
                        f"test -s {full_path} && echo 'NOT_EMPTY' || echo 'EMPTY'",
                        timeout=10,
                    )
                    passed = "NOT_EMPTY" in result["output"]
                else:
                    passed = False

                outputs.append(f"{path}: {'OK' if passed else 'FAILED'}")
                if not passed:
                    all_passed = False

            return ValidationCheck(
                name=name,
                description=description,
                environment=ValidationEnvironment.CLI,
                action=action,
                passed=all_passed,
                output="\n".join(outputs),
            )

        return ValidationCheck(
            name=name,
            description=description,
            environment=ValidationEnvironment.CLI,
            action=action,
            passed=False,
            error=f"Unknown CLI action type: {action_type}",
        )

    async def _execute_browser_check(
        self,
        *,
        name: str,
        description: str,
        action: dict[str, Any],
        serve_url: str | None,
    ) -> ValidationCheck:
        """Execute a browser validation check using Playwright MCP or BrowserSandbox.

        Supports two backends:
        1. BrowserSandboxManager (sync): Uses agentscope-runtime's BrowserSandbox
        2. StatefulClientBase (async): Uses Playwright MCP via stdio

        The backend is determined by self._playwright_mcp type.
        """
        action_type = action.get("type", "navigate")

        # Check if playwright_mcp is available
        if self._playwright_mcp is None:
            return ValidationCheck(
                name=name,
                description=description,
                environment=ValidationEnvironment.BROWSER,
                action=action,
                passed=False,
                error="Browser testing not available: playwright_mcp not configured",
            )

        # Determine URL - prefer serve_url (for Docker), then http_port, then default
        if serve_url:
            default_url = serve_url
        elif self._http_port:
            default_url = f"http://localhost:{self._http_port}"
        else:
            default_url = "http://localhost:3000"

        try:
            # Check if using BrowserSandboxManager (sync) or StatefulClientBase (async)
            is_browser_sandbox = hasattr(self._playwright_mcp, "browser_navigate")

            if action_type == "navigate":
                url = action.get("url", default_url)
                # For BrowserSandbox (Docker), replace localhost with host.docker.internal
                if is_browser_sandbox and serve_url and "host.docker.internal" in serve_url:
                    # Extract port from serve_url and replace localhost URLs
                    import re
                    port_match = re.search(r":(\d+)", serve_url)
                    if port_match:
                        port = port_match.group(1)
                        # Replace localhost:any_port or localhost (without port) with serve_url base
                        url = re.sub(
                            r"http://localhost(:\d+)?",
                            f"http://host.docker.internal:{port}",
                            url,
                        )
                if is_browser_sandbox:
                    result = self._playwright_mcp.browser_navigate(url)
                    # Wait for page load
                    self._playwright_mcp.browser_wait_for(time=1.0)
                else:
                    result = await self._playwright_mcp.session.call_tool(
                        "browser_navigate",
                        arguments={"url": url}
                    )
                    await asyncio.sleep(1)

                return ValidationCheck(
                    name=name,
                    description=description,
                    environment=ValidationEnvironment.BROWSER,
                    action=action,
                    passed=True,  # Navigation success if no exception
                    output=str(result)[:1000],
                )

            elif action_type == "screenshot":
                screenshot_name = action.get("name", "screenshot")
                if is_browser_sandbox:
                    result = self._playwright_mcp.browser_take_screenshot(filename=screenshot_name)
                else:
                    result = await self._playwright_mcp.session.call_tool(
                        "browser_take_screenshot",
                        arguments={}
                    )
                return ValidationCheck(
                    name=name,
                    description=description,
                    environment=ValidationEnvironment.BROWSER,
                    action=action,
                    passed=True,
                    output="Screenshot taken",
                    screenshot=str(result)[:500] if result else None,
                )

            elif action_type == "check_element":
                selector = action.get("selector", "body")
                check = action.get("check", "exists")
                value = action.get("value", "")

                # Get page snapshot to check elements
                if is_browser_sandbox:
                    snapshot = self._playwright_mcp.browser_snapshot()
                else:
                    snapshot = await self._playwright_mcp.session.call_tool(
                        "browser_snapshot",
                        arguments={}
                    )

                snapshot_str = str(snapshot)
                passed = False
                output = ""

                if check == "exists":
                    # Check if selector appears in snapshot
                    passed = selector in snapshot_str
                    output = f"Element '{selector}' {'found' if passed else 'not found'} in page"
                elif check == "visible":
                    # For visibility, we check if it's in snapshot (simplification)
                    passed = selector in snapshot_str
                    output = f"Element '{selector}' visibility: {passed}"
                elif check == "text_contains" and value:
                    passed = value in snapshot_str
                    output = f"Text '{value}' {'found' if passed else 'not found'} in page"

                return ValidationCheck(
                    name=name,
                    description=description,
                    environment=ValidationEnvironment.BROWSER,
                    action=action,
                    passed=passed,
                    output=output,
                )

            elif action_type == "click":
                # æ”¯æŒ element (æ–°) å’Œ selector (å…¼å®¹æ—§)
                element = action.get("element") or action.get("selector", "")
                if is_browser_sandbox:
                    result = self._playwright_mcp.browser_click(element=element)
                else:
                    result = await self._playwright_mcp.session.call_tool(
                        "browser_click",
                        arguments={"element": element}
                    )
                return ValidationCheck(
                    name=name,
                    description=description,
                    environment=ValidationEnvironment.BROWSER,
                    action=action,
                    passed=True,  # Success if no exception
                    output=str(result)[:500],
                )

            elif action_type == "input":
                # æ”¯æŒ element (æ–°) å’Œ selector (å…¼å®¹æ—§)
                element = action.get("element") or action.get("selector", "")
                value = action.get("value", "")
                if is_browser_sandbox:
                    result = self._playwright_mcp.browser_type(element=element, text=value)
                else:
                    result = await self._playwright_mcp.session.call_tool(
                        "browser_type",
                        arguments={"element": element, "text": value}
                    )
                return ValidationCheck(
                    name=name,
                    description=description,
                    environment=ValidationEnvironment.BROWSER,
                    action=action,
                    passed=True,  # Success if no exception
                    output=str(result)[:500],
                )

            elif action_type == "wait":
                # timeout å•ä½ä¸ºæ¯«ç§’ï¼Œè½¬æ¢ä¸ºç§’
                wait_time = action.get("timeout", 1000) / 1000.0
                if is_browser_sandbox:
                    try:
                        self._playwright_mcp.browser_wait_for(time=wait_time)
                    except Exception:
                        # å¦‚æœ wait_for ä¸æ”¯æŒï¼Œä½¿ç”¨ sleep
                        await asyncio.sleep(wait_time)
                else:
                    await asyncio.sleep(wait_time)
                return ValidationCheck(
                    name=name,
                    description=description,
                    environment=ValidationEnvironment.BROWSER,
                    action=action,
                    passed=True,
                    output=f"Waited {wait_time}s",
                )

            elif action_type == "check_title":
                expected_title = action.get("title", "")
                if is_browser_sandbox:
                    snapshot = self._playwright_mcp.browser_snapshot()
                else:
                    snapshot = await self._playwright_mcp.session.call_tool(
                        "browser_snapshot",
                        arguments={}
                    )
                snapshot_str = str(snapshot)
                passed = expected_title in snapshot_str if expected_title else True
                return ValidationCheck(
                    name=name,
                    description=description,
                    environment=ValidationEnvironment.BROWSER,
                    action=action,
                    passed=passed,
                    output=f"Title check: expected '{expected_title}', found: {passed}",
                )

            elif action_type == "check_text":
                expected_text = action.get("text", "")
                if is_browser_sandbox:
                    snapshot = self._playwright_mcp.browser_snapshot()
                else:
                    snapshot = await self._playwright_mcp.session.call_tool(
                        "browser_snapshot",
                        arguments={}
                    )
                snapshot_str = str(snapshot)
                passed = expected_text in snapshot_str if expected_text else True
                return ValidationCheck(
                    name=name,
                    description=description,
                    environment=ValidationEnvironment.BROWSER,
                    action=action,
                    passed=passed,
                    output=f"Text '{expected_text}' {'found' if passed else 'not found'} in page",
                )

        except Exception as exc:
            logger.warning("Browser check '%s' failed: %s", name, exc)
            return ValidationCheck(
                name=name,
                description=description,
                environment=ValidationEnvironment.BROWSER,
                action=action,
                passed=False,
                error=str(exc),
            )

        return ValidationCheck(
            name=name,
            description=description,
            environment=ValidationEnvironment.BROWSER,
            action=action,
            passed=False,
            error=f"Unknown browser action type: {action_type}",
        )

    async def _execute_api_check(
        self,
        *,
        name: str,
        description: str,
        action: dict[str, Any],
    ) -> ValidationCheck:
        """Execute an API validation check."""
        action_type = action.get("type", "request")

        if action_type == "request":
            method = action.get("method", "GET")
            url = action.get("url", "")
            headers = action.get("headers", {})
            body = action.get("body", {})
            expected_status = action.get("expected_status", 200)

            # Use curl in CLI for API testing
            # --connect-timeout: è¿æ¥è¶…æ—¶ï¼ˆç§’ï¼‰
            # -m/--max-time: æ€»è¶…æ—¶ï¼ˆç§’ï¼‰
            header_args = " ".join(f'-H "{k}: {v}"' for k, v in headers.items())
            body_arg = f"-d '{json.dumps(body)}'" if body else ""

            cmd = f"curl --connect-timeout 5 -m 15 -s -o /dev/null -w '%{{http_code}}' -X {method} {header_args} {body_arg} '{url}'"

            # Use unified execution method (prefers RuntimeWorkspace)
            result = await self._execute_command_in_workspace(cmd, timeout=30)

            status_code = result["output"].strip() if result["output"] else "0"
            passed = status_code == str(expected_status)

            return ValidationCheck(
                name=name,
                description=description,
                environment=ValidationEnvironment.API,
                action=action,
                passed=passed,
                output=f"Status: {status_code} (expected: {expected_status})",
                error=result["error"] if not passed else None,
            )

        return ValidationCheck(
            name=name,
            description=description,
            environment=ValidationEnvironment.API,
            action=action,
            passed=False,
            error=f"Unknown API action type: {action_type}",
        )

    async def _analyze_results(
        self,
        *,
        checks: list[ValidationCheck],
        user_requirement: str,
        acceptance_criteria: str,
    ) -> AcceptanceResult:
        """Use LLM to analyze validation results against acceptance criteria.

        IMPORTANT: If any critical check failed, immediately return FAILED
        status without consulting the LLM. This ensures that dependency,
        compile, and startup failures cannot be overridden.
        """
        if not checks:
            return AcceptanceResult(
                status=AcceptanceStatus.ERROR,
                score=0.0,
                summary="æ²¡æœ‰æ‰§è¡Œä»»ä½•éªŒæ”¶æ£€æŸ¥",
            )

        # Check for critical failures first - these cannot be overridden
        critical_failures = [c for c in checks if c.critical and not c.passed]
        if critical_failures:
            failed_phases = set(c.phase for c in critical_failures)
            failed_names = [c.name for c in critical_failures]
            self._emit_event(
                "acceptance_step",
                "âš ï¸ å‘ç°å…³é”®æ£€æŸ¥å¤±è´¥ï¼Œè·³è¿‡LLMåˆ†æ",
                level="warning",
                metadata={"critical_failures": len(critical_failures), "phases": list(failed_phases)},
            )
            self._emit_event(
                "acceptance_step",
                f"   å¤±è´¥é˜¶æ®µ: {', '.join(failed_phases)}",
                level="warning",
            )
            for cf in critical_failures[:3]:
                self._emit_event(
                    "acceptance_step",
                    f"   - {cf.name}: {cf.error or 'æ£€æŸ¥å¤±è´¥'}",
                    level="warning",
                )
            return AcceptanceResult(
                status=AcceptanceStatus.FAILED,
                score=0.0,
                checks=checks,
                summary=f"å…³é”®éªŒæ”¶æ£€æŸ¥å¤±è´¥ï¼ˆ{', '.join(failed_phases)}é˜¶æ®µï¼‰ï¼š{'; '.join(failed_names[:3])}",
                recommendations=[
                    f"ä¿®å¤ {c.phase} é˜¶æ®µçš„é—®é¢˜ï¼š{c.name} - {c.error or 'æ£€æŸ¥å¤±è´¥'}"
                    for c in critical_failures[:5]
                ],
            )

        # Format results for LLM
        results_str = json.dumps(
            [
                {
                    "name": c.name,
                    "description": c.description,
                    "environment": c.environment.value,
                    "phase": c.phase,
                    "critical": c.critical,
                    "passed": c.passed,
                    "output": c.output[:500] if c.output else "",
                    "error": c.error,
                }
                for c in checks
            ],
            ensure_ascii=False,
            indent=2,
        )

        prompt = RESULT_ANALYSIS_PROMPT.format(
            user_requirement=user_requirement or "æœªæŒ‡å®š",
            acceptance_criteria=acceptance_criteria,
            results=results_str,
        )

        try:
            response = await self._call_model(prompt)
            content = self._extract_text_content(response)

            analysis = self._extract_json(content)
            if analysis:
                status_str = analysis.get("status", "failed")
                status = AcceptanceStatus(status_str) if status_str in [
                    s.value for s in AcceptanceStatus
                ] else AcceptanceStatus.FAILED

                return AcceptanceResult(
                    status=status,
                    score=float(analysis.get("score", 0)) / 100.0,
                    checks=checks,
                    summary=analysis.get("summary", ""),
                    recommendations=analysis.get("recommendations", []),
                    deliverable_assessment=analysis.get("deliverable_assessment", {}),
                )
        except Exception as exc:
            logger.error("Failed to analyze results: %s", exc)

        # Fallback: simple pass ratio calculation
        # Note: Critical failures are already handled above, so if we reach here,
        # all critical checks have passed.
        passed_count = sum(1 for c in checks if c.passed)
        total_count = len(checks)
        score = passed_count / total_count if total_count > 0 else 0.0

        # Require higher threshold: 90% for PASSED (was 80%)
        if score >= 0.9:
            status = AcceptanceStatus.PASSED
        elif score >= 0.6:
            status = AcceptanceStatus.PARTIAL
        else:
            status = AcceptanceStatus.FAILED

        return AcceptanceResult(
            status=status,
            score=score,
            checks=checks,
            summary=f"é€šè¿‡ {passed_count}/{total_count} é¡¹æ£€æŸ¥ï¼ˆéœ€è¦ 90% ä»¥ä¸Šé€šè¿‡ï¼‰",
        )

    def _extract_json(self, text: str) -> dict[str, Any] | None:
        """Extract JSON from LLM response text."""
        import re

        # Try to find JSON in code blocks
        json_match = re.search(r"```(?:json)?\s*(\{.*?\})\s*```", text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(1))
            except json.JSONDecodeError:
                pass

        # Try to find bare JSON object
        json_match = re.search(r"\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}", text, re.DOTALL)
        if json_match:
            try:
                return json.loads(json_match.group(0))
            except json.JSONDecodeError:
                pass

        # Try parsing entire text as JSON
        try:
            return json.loads(text)
        except json.JSONDecodeError:
            pass

        return None

    def validate_sync(
        self,
        *,
        workspace_dir: str = "/workspace",
        user_requirement: str = "",
        acceptance_criteria: list[str] | None = None,
        artifact_type: str = "web",
        file_list: list[str] | None = None,
        serve_url: str | None = None,
    ) -> AcceptanceResult:
        """Synchronous wrapper for validate().

        For use in non-async contexts.
        """
        import asyncio

        try:
            loop = asyncio.get_event_loop()
        except RuntimeError:
            loop = asyncio.new_event_loop()
            asyncio.set_event_loop(loop)

        return loop.run_until_complete(
            self.validate(
                workspace_dir=workspace_dir,
                user_requirement=user_requirement,
                acceptance_criteria=acceptance_criteria,
                artifact_type=artifact_type,
                file_list=file_list,
                serve_url=serve_url,
            )
        )
