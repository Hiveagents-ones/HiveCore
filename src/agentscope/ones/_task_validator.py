# -*- coding: utf-8 -*-
"""Task-level immediate validation for execution loop.

This module provides language-agnostic validation by checking:
1. Whether expected files were created
2. Whether Claude Code reported any errors
3. Active validation by calling Claude Code to verify the task

Language-specific validation (syntax, imports, etc.) is handled
by Claude Code itself during the active validation step.
"""
from __future__ import annotations

import asyncio
import logging
import subprocess
import time as _time_module
from dataclasses import dataclass, field
from pathlib import Path
from typing import TYPE_CHECKING, Any

if TYPE_CHECKING:
    from ..aa import Requirement

logger = logging.getLogger(__name__)


def _log_validation(
    msg: str,
    *,
    level: str = "info",
    prefix: str = "[TaskValidator]",
) -> None:
    """Log validation message with real-time output.

    Args:
        msg: Message to log.
        level: Log level (info, warning, error).
        prefix: Prefix for the message.
    """
    timestamp = _time_module.strftime("%H:%M:%S")
    formatted = f"{timestamp} | {prefix} {msg}"
    print(formatted, flush=True)
    if level == "warning":
        logger.warning(msg)
    elif level == "error":
        logger.error(msg)
    else:
        logger.info(msg)


@dataclass
class TaskValidationResult:
    """Result of task-level validation."""

    passed: bool
    errors: list[str] = field(default_factory=list)
    warnings: list[str] = field(default_factory=list)
    error_summary: str = ""
    files_checked: list[str] = field(default_factory=list)
    validation_type: str = "basic"

    def __post_init__(self) -> None:
        if not self.error_summary and self.errors:
            self.error_summary = "; ".join(self.errors[:3])
            if len(self.errors) > 3:
                self.error_summary += f" (è¿˜æœ‰ {len(self.errors) - 3} ä¸ªé”™è¯¯)"


class TaskLevelValidator:
    """Validates each task immediately after completion.

    This is a language-agnostic validator that:
    1. Checks if Claude Code reported errors in the output
    2. Verifies that files were created/modified

    Language-specific validation (syntax, imports, tests) should be
    performed by Claude Code itself during task execution.
    """

    def __init__(
        self,
        workspace_dir: str | Path,
        *,
        container_id: str | None = None,
        container_workspace: str = "/workspace/working",
        validation_timeout: float = 30.0,
    ) -> None:
        """Initialize the validator.

        Args:
            workspace_dir: Local workspace directory (used when not in container mode).
            container_id: Docker container ID for container mode.
            container_workspace: Workspace path inside the container.
            validation_timeout: Timeout for validation commands.
        """
        self.workspace_dir = Path(workspace_dir)
        self.container_id = container_id
        self.container_workspace = container_workspace
        self.validation_timeout = validation_timeout

        # Track which validations have passed for incremental checks
        self._passed_validations: dict[str, TaskValidationResult] = {}

    @property
    def is_container_mode(self) -> bool:
        """Check if running in container mode."""
        return self.container_id is not None

    async def validate_task(
        self,
        node_id: str,
        requirement: Requirement | None,
        output_content: str | None,
    ) -> TaskValidationResult:
        """Validate a single task immediately after completion.

        This performs language-agnostic validation:
        1. Check if the output contains error indicators
        2. Verify files were created (if applicable)

        Args:
            node_id: Task identifier (e.g., 'REQ-001')
            requirement: The requirement being fulfilled
            output_content: The agent's output content

        Returns:
            TaskValidationResult with pass/fail status and errors
        """
        if requirement is None:
            return TaskValidationResult(passed=True, validation_type="skipped")

        errors: list[str] = []
        warnings: list[str] = []

        logger.info(
            "[%s] å¼€å§‹ä»»åŠ¡çº§éªŒæ”¶ (å®¹å™¨æ¨¡å¼: %s)",
            node_id,
            self.is_container_mode,
        )

        # Check for different error/timeout types from Claude Code output
        # 1. [ERROR] - definitive error, trigger repair
        # 2. [TIMEOUT:STALLED] - timeout with no progress, trigger repair
        # 3. [TIMEOUT:PROGRESS] - timeout but has progress, don't trigger repair
        if output_content:
            if "[ERROR]" in output_content:
                idx = output_content.find("[ERROR]")
                snippet = output_content[idx : idx + 300].split("\n")[0]
                errors.append(f"Claude Code æŠ¥å‘Šé”™è¯¯: {snippet}")
            elif "[TIMEOUT:STALLED]" in output_content:
                idx = output_content.find("[TIMEOUT:STALLED]")
                snippet = output_content[idx : idx + 300].split("\n")[0]
                errors.append(f"ä»»åŠ¡å¡ä½: {snippet}")
            elif "[TIMEOUT:PROGRESS]" in output_content:
                # Task made progress but timed out - not an error, just a warning
                idx = output_content.find("[TIMEOUT:PROGRESS]")
                snippet = output_content[idx : idx + 300].split("\n")[0]
                warnings.append(f"ä»»åŠ¡è¶…æ—¶ä½†æœ‰è¿›å±•: {snippet}")
                logger.info(
                    "[%s] ä»»åŠ¡è¶…æ—¶ä½†æœ‰è¿›å±•ï¼Œä¸è§¦å‘ä¿®å¤æµç¨‹",
                    node_id,
                )

        result = TaskValidationResult(
            passed=len(errors) == 0,
            errors=errors,
            warnings=warnings,
            validation_type="output_check",
        )

        # Log result
        if result.passed:
            logger.info("[%s] âœ“ ä»»åŠ¡éªŒæ”¶é€šè¿‡", node_id)
            self._passed_validations[node_id] = result
        else:
            logger.warning(
                "[%s] âœ— ä»»åŠ¡éªŒæ”¶å¤±è´¥: %s",
                node_id,
                result.error_summary,
            )

        return result

    async def _check_files_created(self, output_content: str) -> bool | None:
        """Check if files mentioned in output were actually created.

        This is a best-effort check that looks for file paths in the output
        and verifies they exist.

        Args:
            output_content: The output to parse for file paths.

        Returns:
            True if files exist, False if not, None if cannot determine.
        """
        # Simple heuristic: look for "Files Changed" or similar patterns
        if "Files Changed" not in output_content and "åˆ›å»ºçš„æ–‡ä»¶" not in output_content:
            return None

        # For now, just return None (cannot determine)
        # A more sophisticated implementation could parse file paths
        # and verify their existence
        return None

    async def _run_command(
        self,
        cmd: list[str],
        cwd: str | None = None,
    ) -> tuple[int, str, str]:
        """Run a command either locally or in container.

        Args:
            cmd: Command to run.
            cwd: Working directory.

        Returns:
            Tuple of (return_code, stdout, stderr).
        """
        if self.is_container_mode:
            docker_cmd = ["docker", "exec"]
            if cwd:
                docker_cmd.extend(["-w", cwd])
            docker_cmd.append(self.container_id)
            docker_cmd.extend(cmd)
            final_cmd = docker_cmd
            final_cwd = None
        else:
            final_cmd = cmd
            final_cwd = cwd

        try:
            result = await asyncio.wait_for(
                asyncio.get_event_loop().run_in_executor(
                    None,
                    lambda: subprocess.run(
                        final_cmd,
                        cwd=final_cwd,
                        capture_output=True,
                        timeout=int(self.validation_timeout),
                    ),
                ),
                timeout=self.validation_timeout + 5,
            )
            return (
                result.returncode,
                result.stdout.decode("utf-8", errors="replace"),
                result.stderr.decode("utf-8", errors="replace"),
            )
        except asyncio.TimeoutError:
            return -1, "", "å‘½ä»¤è¶…æ—¶"
        except Exception as e:
            return -1, "", str(e)[:200]

    def generate_fix_prompt(
        self,
        node_id: str,
        requirement: Requirement | None,
        validation_result: TaskValidationResult,
        original_output: str | None,
    ) -> str:
        """Generate a targeted fix prompt for the agent.

        Args:
            node_id: Task identifier
            requirement: Original requirement
            validation_result: The failed validation result
            original_output: What the agent produced

        Returns:
            A prompt focused on fixing the specific errors
        """
        title = getattr(requirement, "title", node_id) if requirement else node_id

        prompt = f"""## ä»»åŠ¡ä¿®å¤è¯·æ±‚

**ä»»åŠ¡**: {node_id} - {title}

### éªŒæ”¶å¤±è´¥åŸå› 

{chr(10).join(f'- âŒ {e}' for e in validation_result.errors)}

### ä¿®å¤è¦æ±‚

è¯·é’ˆå¯¹ä¸Šè¿°é”™è¯¯è¿›è¡Œä¿®å¤ï¼š

1. åˆ†æé”™è¯¯åŸå› 
2. ä¿®å¤å…·ä½“é—®é¢˜
3. **å¿…é¡»**è¿è¡ŒéªŒè¯å‘½ä»¤ç¡®è®¤ä¿®å¤æˆåŠŸ

### éªŒè¯è¦æ±‚ï¼ˆé‡è¦ï¼‰

ä¿®å¤å®Œæˆåï¼Œä½ **å¿…é¡»**ä¸»åŠ¨éªŒè¯ä»£ç çš„æ­£ç¡®æ€§ï¼š
1. è¿è¡Œé€‚å½“çš„å‘½ä»¤éªŒè¯ä»£ç å¯ä»¥è¢«æ­£ç¡®åŠ è½½/ç¼–è¯‘
2. å¦‚æœé¡¹ç›®æœ‰æµ‹è¯•æ¡†æ¶ï¼Œè¿è¡Œç›¸å…³æµ‹è¯•
3. å¦‚æœéªŒè¯å¤±è´¥ï¼Œç»§ç»­ä¿®å¤ç›´åˆ°éªŒè¯é€šè¿‡
4. åªæœ‰éªŒè¯é€šè¿‡åæ‰ç®—ä¿®å¤å®Œæˆ

éªŒè¯æ–¹å¼ç”±ä½ æ ¹æ®é¡¹ç›®ç±»å‹è‡ªè¡Œå†³å®šã€‚

### é‡è¦æç¤º

- åªä¿®å¤ä¸Šè¿°é”™è¯¯ï¼Œä¸è¦ä¿®æ”¹å…¶ä»–åŠŸèƒ½
- å¦‚æœéœ€è¦åˆ›å»ºç¼ºå¤±çš„æ–‡ä»¶æˆ–ç›®å½•ï¼Œè¯·ä¸€å¹¶åˆ›å»º
"""

        return prompt

    def get_validation_summary(self) -> dict[str, bool]:
        """Get summary of all passed validations."""
        return {
            node_id: result.passed
            for node_id, result in self._passed_validations.items()
        }

    async def validate_with_agent(
        self,
        node_id: str,
        requirement: "Requirement | None",
        output_content: str | None,
        *,
        validation_timeout: float = 120.0,
    ) -> TaskValidationResult:
        """Actively validate task by calling Claude Code to verify.

        This method calls Claude Code to let it verify the task result
        by running appropriate validation commands (build, test, import, etc.)
        based on the project type.

        Args:
            node_id: Task identifier (e.g., 'REQ-001')
            requirement: The requirement being fulfilled
            output_content: The agent's output content (for context)
            validation_timeout: Timeout for validation (default 120s)

        Returns:
            TaskValidationResult with pass/fail status and errors
        """
        if requirement is None:
            return TaskValidationResult(passed=True, validation_type="skipped")

        _log_validation(f"[{node_id}] ğŸ” å¼€å§‹ä¸»åŠ¨éªŒè¯...")

        try:
            from agentscope.scripts._claude_code import claude_code_edit
        except ImportError:
            _log_validation(
                f"[{node_id}] âš  claude_code_edit ä¸å¯ç”¨ï¼Œè·³è¿‡ä¸»åŠ¨éªŒè¯",
                level="warning",
            )
            return TaskValidationResult(passed=True, validation_type="skipped")

        # Build validation prompt
        title = getattr(requirement, "title", node_id) if requirement else node_id
        validation_prompt = f"""## ä»»åŠ¡éªŒè¯è¯·æ±‚

**ä»»åŠ¡**: {node_id} - {title}

è¯·éªŒè¯æ­¤ä»»åŠ¡çš„ä»£ç æ˜¯å¦å¯ä»¥æ­£å¸¸è¿è¡Œã€‚ä½ éœ€è¦ï¼š

1. **æ£€æŸ¥å…³é”®æ–‡ä»¶æ˜¯å¦å­˜åœ¨**ï¼š
   - å¦‚æœæ˜¯ Python é¡¹ç›®ï¼šæ£€æŸ¥ requirements.txt æˆ– pyproject.toml æ˜¯å¦å­˜åœ¨
   - å¦‚æœæ˜¯ Node.js é¡¹ç›®ï¼šæ£€æŸ¥ package.json æ˜¯å¦å­˜åœ¨
   - æ£€æŸ¥ä»»åŠ¡ç›¸å…³çš„ä»£ç æ–‡ä»¶æ˜¯å¦å­˜åœ¨

2. **éªŒè¯ä»£ç å¯ä»¥æ­£å¸¸è¿è¡Œ**ï¼š
   - Python: å°è¯•å¯¼å…¥ä¸»è¦æ¨¡å— (`python -c "import ..."`)
   - Node.js:
     * é¦–å…ˆæ£€æŸ¥ node_modules æ˜¯å¦å­˜åœ¨ï¼Œå¦‚ä¸å­˜åœ¨åˆ™è¿è¡Œ `npm install`
     * ç„¶åè¿è¡Œ `npm run build` æˆ– `npm run lint`ï¼ˆå‡è®¾ä¾èµ–å·²å®‰è£…ï¼‰
   - å…¶ä»–è¯­è¨€: è¿è¡Œé€‚å½“çš„ç¼–è¯‘/æ„å»ºå‘½ä»¤

   **æ³¨æ„**: npm install å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚å¦‚æœæ„å»ºå¤±è´¥ï¼Œ
   è¯·ä»”ç»†åˆ†æé”™è¯¯ä¿¡æ¯ï¼ˆå¦‚ TypeScript ç±»å‹é”™è¯¯ã€å¯¼å…¥é”™è¯¯ç­‰ï¼‰ã€‚

3. **è¾“å‡ºéªŒè¯ç»“æœ**ï¼š
   - å¦‚æœæ‰€æœ‰éªŒè¯é€šè¿‡ï¼Œè¾“å‡º: `[VALIDATION_PASSED]`
   - å¦‚æœä»»ä½•éªŒè¯å¤±è´¥ï¼Œè¾“å‡º: `[VALIDATION_FAILED] å…·ä½“å¤±è´¥åŸå› `

**é‡è¦**ï¼š
- åªåšéªŒè¯ï¼Œä¸è¦ä¿®æ”¹ä»»ä½•ä»£ç 
- éªŒè¯è¶…æ—¶é™åˆ¶: {validation_timeout}ç§’
- å¦‚æœå‘ç°ç¼ºå¤±çš„å…³é”®æ–‡ä»¶ï¼ˆå¦‚ requirements.txtï¼‰ï¼Œå¿…é¡»æŠ¥å‘Šä¸ºå¤±è´¥
"""

        try:
            _log_validation(f"[{node_id}] è°ƒç”¨ Claude Code æ‰§è¡ŒéªŒè¯...")
            result = await asyncio.wait_for(
                claude_code_edit(prompt=validation_prompt),
                timeout=validation_timeout,
            )

            # Extract result text
            result_text = ""
            if result and result.content:
                for block in result.content:
                    if hasattr(block, "text"):
                        result_text += block.text
                    elif isinstance(block, dict) and "text" in block:
                        result_text += block["text"]

            # Parse validation result
            errors: list[str] = []
            warnings: list[str] = []

            if "[VALIDATION_FAILED]" in result_text:
                # Extract failure reason
                idx = result_text.find("[VALIDATION_FAILED]")
                failure_msg = result_text[idx + len("[VALIDATION_FAILED]"):].strip()
                # Take first line or first 200 chars
                failure_msg = failure_msg.split("\n")[0][:200]
                errors.append(f"éªŒè¯å¤±è´¥: {failure_msg}")
                _log_validation(
                    f"[{node_id}] âœ— ä¸»åŠ¨éªŒè¯å¤±è´¥: {failure_msg}",
                    level="error",
                )
            elif "[VALIDATION_PASSED]" in result_text:
                _log_validation(f"[{node_id}] âœ“ ä¸»åŠ¨éªŒè¯é€šè¿‡")
            elif "[ERROR]" in result_text:
                idx = result_text.find("[ERROR]")
                snippet = result_text[idx : idx + 300].split("\n")[0]
                errors.append(f"éªŒè¯è¿‡ç¨‹å‡ºé”™: {snippet}")
                _log_validation(
                    f"[{node_id}] âœ— éªŒè¯è¿‡ç¨‹å‡ºé”™: {snippet}",
                    level="error",
                )
            else:
                # No explicit marker - assume passed with warning
                warnings.append("éªŒè¯å®Œæˆä½†æœªè¿”å›æ˜ç¡®ç»“æœ")
                _log_validation(
                    f"[{node_id}] âš  éªŒè¯å®Œæˆä½†æœªè¿”å›æ˜ç¡®ç»“æœ",
                    level="warning",
                )

            validation_result = TaskValidationResult(
                passed=len(errors) == 0,
                errors=errors,
                warnings=warnings,
                validation_type="active_validation",
            )

            if validation_result.passed:
                self._passed_validations[node_id] = validation_result

            return validation_result

        except asyncio.TimeoutError:
            _log_validation(
                f"[{node_id}] âœ— ä¸»åŠ¨éªŒè¯è¶…æ—¶ ({validation_timeout}s)",
                level="error",
            )
            return TaskValidationResult(
                passed=False,
                errors=[f"éªŒè¯è¶…æ—¶ ({validation_timeout}s)"],
                validation_type="active_validation",
            )
        except Exception as e:
            _log_validation(
                f"[{node_id}] âœ— ä¸»åŠ¨éªŒè¯å¼‚å¸¸: {e}",
                level="error",
            )
            return TaskValidationResult(
                passed=False,
                errors=[f"éªŒè¯å¼‚å¸¸: {str(e)[:200]}"],
                validation_type="active_validation",
            )
