# -*- coding: utf-8 -*-
"""Agent role implementations for requirement design and implementation.

This module provides:
- Blueprint design (design_requirement)
- Requirement implementation (implement_requirement)
- File generation utilities
- Edit operations
- Scaffold execution
- Claude Code integration for code generation
"""
from __future__ import annotations

import ast
import json
import re
import textwrap
from typing import Any, TYPE_CHECKING

if TYPE_CHECKING:
    from ._sandbox import RuntimeWorkspace

# Claude Code integration configuration
USE_CLAUDE_CODE = False  # Set to False to use traditional LLM generation (Claude CLI not available in Worker)

# Claude Code execution mode:
# - "stepwise": Agent decides file list, Claude Code generates each file (legacy)
# - "autonomous": Claude Code has full control over implementation (direct call)
# - "agent_with_limbs": Agent (brain) calls claude_code_edit (limbs) via toolkit
#
# Recommended: "agent_with_limbs" - proper brain/limbs separation
CLAUDE_CODE_MODE = "agent_with_limbs"


# ---------------------------------------------------------------------------
# Text Utilities
# ---------------------------------------------------------------------------
def extract_file_structure(content: str, file_path: str) -> str:
    """Extract structure information from a file.

    Args:
        content: File content
        file_path: File path

    Returns:
        str: Structure description
    """
    lines = content.split("\n")
    structure_lines: list[str] = []

    ext = file_path.split(".")[-1].lower() if "." in file_path else ""

    for i, line in enumerate(lines):
        stripped = line.strip()
        # Python
        if ext == "py":
            if stripped.startswith(("class ", "def ", "async def ")):
                structure_lines.append(f"L{i + 1}: {stripped[:80]}")
        # JavaScript/TypeScript/Vue
        elif ext in ("js", "ts", "jsx", "tsx", "vue"):
            if any(stripped.startswith(kw) for kw in ["function ", "class ", "const ", "export ", "async function "]):
                if "=" in stripped and "function" not in stripped and "class" not in stripped:
                    if any(k in stripped for k in ["Component", "Router", "Store"]):
                        structure_lines.append(f"L{i + 1}: {stripped[:80]}")
                else:
                    structure_lines.append(f"L{i + 1}: {stripped[:80]}")
            elif any(tag in stripped for tag in ["<script", "<template", "<style"]):
                structure_lines.append(f"L{i + 1}: {stripped[:80]}")
        # Other
        else:
            if stripped.startswith(("class ", "def ", "function ")):
                structure_lines.append(f"L{i + 1}: {stripped[:80]}")

    if structure_lines:
        return "æ–‡ä»¶ç»“æ„:\n" + "\n".join(structure_lines[:30])
    return ""


def find_similar_text(content: str, target: str, context_lines: int = 3) -> str:
    """Find similar text in content and return context.

    Args:
        content: File content
        target: Target text to find
        context_lines: Number of context lines

    Returns:
        str: Context information
    """
    if not target or len(target) < 10:
        return ""

    lines = content.split("\n")

    # Extract keywords
    keywords = re.findall(r"\b([a-zA-Z_][a-zA-Z0-9_]{2,})\b", target)
    keyword_counts: dict[str, int] = {}
    for kw in keywords:
        keyword_counts[kw] = keyword_counts.get(kw, 0) + 1

    # Find unique keywords
    unique_keywords = sorted(keyword_counts.keys(), key=lambda k: keyword_counts[k])[:5]

    # Search for lines with keywords
    matches: list[tuple[int, int, str]] = []
    for i, line in enumerate(lines):
        score = sum(1 for kw in unique_keywords if kw in line)
        if score >= 2:
            start = max(0, i - context_lines)
            end = min(len(lines), i + context_lines + 1)
            context = "\n".join(f"L{j + 1}: {lines[j]}" for j in range(start, end))
            matches.append((score, i + 1, context))

    if matches:
        matches.sort(key=lambda x: -x[0])
        best = matches[0]
        return f"å¯èƒ½åŒ¹é…ä½ç½® (L{best[1]}, ç›¸ä¼¼åº¦={best[0]}):\n{best[2]}"

    return ""


def validate_python_syntax(content: str) -> tuple[bool, str]:
    """Validate Python code syntax.

    Args:
        content: Python code

    Returns:
        tuple: (is_valid, error_message)
    """
    try:
        ast.parse(content)
        return True, ""
    except SyntaxError as e:
        msg = f"è¯­æ³•é”™è¯¯ (è¡Œ {e.lineno}): {e.msg}"
        if e.text:
            msg += f" | ä»£ç : {e.text.strip()[:50]}"
        return False, msg
    except Exception as e:
        return False, f"è§£æé”™è¯¯: {str(e)}"


# ---------------------------------------------------------------------------
# Edit Operations
# ---------------------------------------------------------------------------
def apply_edits(
    existing_content: str,
    edits: list[dict[str, Any]],
    verbose: bool = False,
) -> tuple[str, bool, list[dict[str, Any]]]:
    """Apply edit operations to file content.

    Args:
        existing_content: Original content
        edits: List of edit operations
        verbose: Whether to print debug info

    Returns:
        tuple: (modified_content, all_success, failed_edits)
    """
    content = existing_content
    all_success = True
    failed_edits: list[dict[str, Any]] = []

    for i, edit in enumerate(edits):
        edit_type = edit.get("type", "")
        success = False
        failure_reason = ""

        try:
            if edit_type == "replace":
                old_text = edit.get("old", "")
                new_text = edit.get("new", "")
                if old_text and old_text in content:
                    content = content.replace(old_text, new_text, 1)
                    success = True
                else:
                    failure_reason = f"æœªæ‰¾åˆ°ç›®æ ‡æ–‡æœ¬: '{old_text[:100]}...'" if len(old_text) > 100 else f"æœªæ‰¾åˆ°ç›®æ ‡æ–‡æœ¬: '{old_text}'"

            elif edit_type == "insert_after":
                anchor = edit.get("anchor", "")
                new_content = edit.get("content", "")
                if anchor and anchor in content:
                    lines = content.split("\n")
                    for j, line in enumerate(lines):
                        if anchor in line:
                            lines.insert(j + 1, new_content)
                            content = "\n".join(lines)
                            success = True
                            break
                else:
                    failure_reason = f"æœªæ‰¾åˆ°é”šç‚¹: '{anchor[:100]}...'" if len(anchor) > 100 else f"æœªæ‰¾åˆ°é”šç‚¹: '{anchor}'"

            elif edit_type == "insert_before":
                anchor = edit.get("anchor", "")
                new_content = edit.get("content", "")
                if anchor and anchor in content:
                    lines = content.split("\n")
                    for j, line in enumerate(lines):
                        if anchor in line:
                            lines.insert(j, new_content)
                            content = "\n".join(lines)
                            success = True
                            break
                else:
                    failure_reason = f"æœªæ‰¾åˆ°é”šç‚¹: '{anchor[:100]}...'" if len(anchor) > 100 else f"æœªæ‰¾åˆ°é”šç‚¹: '{anchor}'"

            elif edit_type == "delete":
                target = edit.get("target", "")
                if target and target in content:
                    content = content.replace(target, "", 1)
                    success = True
                else:
                    failure_reason = f"æœªæ‰¾åˆ°ç›®æ ‡æ–‡æœ¬"

            elif edit_type == "append":
                new_content = edit.get("content", "")
                if new_content:
                    if not content.endswith("\n"):
                        content += "\n"
                    content += new_content
                    success = True

            elif edit_type == "prepend":
                new_content = edit.get("content", "")
                if new_content:
                    content = new_content + ("\n" if not new_content.endswith("\n") else "") + content
                    success = True

            else:
                failure_reason = f"æœªçŸ¥ç¼–è¾‘ç±»å‹: {edit_type}"

        except Exception as e:
            failure_reason = f"æ‰§è¡Œå‡ºé”™: {e}"

        if success:
            if verbose:
                from ._observability import get_logger
                get_logger().debug(f"[EDIT] #{i + 1} {edit_type}: æˆåŠŸ")
        else:
            all_success = False
            failed_edits.append({
                "index": i + 1,
                "edit": edit,
                "reason": failure_reason,
            })
            if verbose:
                from ._observability import get_logger
                get_logger().debug(f"[EDIT] #{i + 1} {edit_type}: {failure_reason}")

    return content, all_success, failed_edits


# ---------------------------------------------------------------------------
# Blueprint Design
# ---------------------------------------------------------------------------
async def design_requirement(
    llm: Any,
    requirement: dict[str, Any],
    feedback: str,
    passed_ids: set[str],
    failed_criteria: list[dict[str, Any]],
    prev_blueprint: dict[str, Any] | None,
    contextual_notes: str | None = None,
    existing_workspace_files: list[str] | None = None,
    skeleton_context: dict[str, Any] | None = None,
    *,
    verbose: bool = False,
) -> dict[str, Any]:
    """Design blueprint for a requirement.

    Args:
        llm: LLM model instance
        requirement: Requirement dict
        feedback: Previous QA feedback
        passed_ids: Set of passed criteria IDs
        failed_criteria: List of failed criteria
        prev_blueprint: Previous blueprint (if any)
        contextual_notes: Context notes
        existing_workspace_files: List of existing files in workspace
        skeleton_context: Skeleton context with file_ownership and shared_files
        verbose: Whether to print debug info

    Returns:
        dict: Blueprint JSON
    """
    from ._llm_utils import call_llm_json

    # Build criteria guidance for code generation
    criteria_guidance = ""
    if failed_criteria:
        criteria_guidance = "\nã€å¿…é¡»å®ç°çš„éªŒæ”¶æ ‡å‡†ã€‘\n"
        for i, criterion in enumerate(failed_criteria, 1):
            crit_id = criterion.get("id", f"AC-{i}")
            crit_name = criterion.get("name", criterion.get("title", ""))
            crit_desc = criterion.get("description", "")
            criteria_guidance += f"{i}. [{crit_id}] {crit_name}\n"
            if crit_desc:
                criteria_guidance += f"   æè¿°: {crit_desc}\n"
            # Add QA failure reason if available (explains WHY previous attempt failed)
            if criterion.get("qa_failure_reason"):
                criteria_guidance += f"   âš ï¸ ä¸Šæ¬¡å¤±è´¥åŸå› : {criterion.get('qa_failure_reason')}\n"
            # Add recommendation if available (explains HOW to fix)
            if criterion.get("recommendation"):
                criteria_guidance += f"   ğŸ’¡ ä¿®å¤å»ºè®®: {criterion.get('recommendation')}\n"
        criteria_guidance += "\nã€é‡è¦ã€‘files_plan ä¸­çš„æ¯ä¸ªæ–‡ä»¶å¿…é¡»æ ‡æ³¨å®ƒå®ç°å“ªäº›éªŒæ”¶æ ‡å‡†ï¼\n"

    prompt = textwrap.dedent(f"""
        éœ€æ±‚å¯¹è±¡:
        {json.dumps(requirement, ensure_ascii=False, indent=2)}

        å·²é€šè¿‡çš„æ ‡å‡†:
        {sorted(passed_ids) if passed_ids else "æ— "}
        {criteria_guidance}

        ä¸Šä¸€ç‰ˆ Blueprint (å¦‚æœ‰):
        {json.dumps(prev_blueprint, ensure_ascii=False, indent=2) if prev_blueprint else "æ— "}

        ä¹‹å‰ QA çš„åé¦ˆ (å¦‚æœ‰):
        {feedback or "æ— "}

        è¯·è¾“å‡º Blueprintï¼ˆJSONï¼‰ï¼Œå­—æ®µåŒ…æ‹¬:
        {{
          "requirement_id": "{requirement.get('id', '')}",
          "artifact_type": "web|api|script|document",
          "deliverable_pitch": "...",
          "recommended_stack": "...",
          "generation_mode": "single|stepwise|scaffold",
          "scaffold": {{
            "frontend": {{
              "init_command": "npm create vite@latest frontend --template vue",
              "install_template": "cd frontend && npm install {{packages}}",
              "packages": ["element-plus", "axios", "vue-router"]
            }},
            "backend": {{
              "init_command": null,
              "install_template": "pip install {{packages}}",
              "packages": ["fastapi", "sqlalchemy", "passlib[bcrypt]"]
            }}
          }},
          "files_plan": [
            {{"path": "frontend/src/views/HomeView.vue", "description": "é¦–é¡µè§†å›¾", "action": "create", "priority": 1, "criteria_ids": ["AC-1"]}},
            {{"path": "frontend/src/components/TaskItem.vue", "description": "ä»»åŠ¡é¡¹ç»„ä»¶", "action": "create", "priority": 2, "criteria_ids": ["AC-2"]}},
            {{"path": "backend/app/main.py", "description": "åç«¯å…¥å£", "action": "modify", "priority": 1, "criteria_ids": ["AC-3"]}}
          ]
        }}

        ã€å…³é”® - æ–‡ä»¶è·¯å¾„è§„èŒƒã€‘
        1. files_plan.path å¿…é¡»ä½¿ç”¨å®Œæ•´çš„ç›¸å¯¹è·¯å¾„ï¼ŒåŒ…å«ç›®å½•ç»“æ„ï¼
        2. å‰ç«¯æ–‡ä»¶å¿…é¡»ä»¥ frontend/ å¼€å¤´ï¼Œå¦‚: frontend/src/views/XXX.vue, frontend/src/components/XXX.vue
        3. åç«¯æ–‡ä»¶å¿…é¡»ä»¥ backend/ å¼€å¤´ï¼Œå¦‚: backend/app/main.py, backend/app/routers/xxx.py
        4. ç¦æ­¢ä½¿ç”¨ç®€çŸ­è·¯å¾„å¦‚ src/XXX.vue æˆ– main.pyï¼Œå¿…é¡»ä½¿ç”¨å®Œæ•´è·¯å¾„ï¼
        5. files_plan.criteria_ids å¿…é¡»æ ‡æ³¨è¯¥æ–‡ä»¶éœ€è¦æ»¡è¶³å“ªäº›éªŒæ”¶æ ‡å‡†IDï¼

        ã€ç”Ÿæˆæ¨¡å¼è¯´æ˜ã€‘
        - single: ç®€å•é¡¹ç›®ï¼ˆå•æ–‡ä»¶ï¼‰
        - stepwise: å¤æ‚é¡¹ç›®ï¼ˆå¤šæ–‡ä»¶åˆ†æ­¥ç”Ÿæˆï¼‰
        - scaffold: ä½¿ç”¨è„šæ‰‹æ¶åˆå§‹åŒ–åç”Ÿæˆä¸šåŠ¡ä»£ç 

        ã€scaffold é…ç½®è¯´æ˜ã€‘
        - init_command: é¡¹ç›®åˆå§‹åŒ–å‘½ä»¤ï¼ˆåªæ‰§è¡Œä¸€æ¬¡ï¼Œå¦‚ npm createã€django-admin startprojectï¼‰
        - install_template: å¢é‡å®‰è£…ä¾èµ–çš„å‘½ä»¤æ¨¡æ¿ï¼Œ{{packages}} ä¼šè¢«æ›¿æ¢ä¸ºåŒ…åˆ—è¡¨
        - packages: æœ¬éœ€æ±‚æ‰€éœ€çš„ä¾èµ–åŒ…åˆ—è¡¨ï¼ˆä¼šä¸å·²å®‰è£…çš„åŒ…åˆå¹¶ï¼Œåªå®‰è£…æ–°å¢çš„ï¼‰

        ã€ä¾èµ–ç®¡ç† - é‡è¦ã€‘
        å¦‚æœä»£ç éœ€è¦ä½¿ç”¨ç¬¬ä¸‰æ–¹åº“ï¼ˆå¦‚ PyJWT, bcrypt, axios ç­‰ï¼‰ï¼Œå¿…é¡»ï¼š
        1. åœ¨ scaffold.packages ä¸­åˆ—å‡ºæ‰€éœ€çš„ç¬¬ä¸‰æ–¹ä¾èµ–
        2. æˆ–åœ¨ files_plan ä¸­åŒ…å« requirements.txtï¼ˆPythonï¼‰æˆ– package.jsonï¼ˆNode.jsï¼‰
        å¦åˆ™ä»£ç å°†å› ç¼ºå°‘ä¾èµ–è€Œæ— æ³•è¿è¡Œï¼

        è¾“å‡ºåˆæ³• JSONã€‚
    """)

    if existing_workspace_files:
        prompt += f"\n\nã€å½“å‰å·¥ä½œåŒºå·²æœ‰æ–‡ä»¶ã€‘\n{existing_workspace_files}\n"
        prompt += "å¦‚æœæœ‰æ¡†æ¶æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ stepwise è€Œé scaffold æ¨¡å¼ã€‚\n"

    # Add skeleton context information
    if skeleton_context and skeleton_context.get("skeleton_generated"):
        req_id = requirement.get("id", "")
        file_ownership = skeleton_context.get("file_ownership", {})
        shared_files = skeleton_context.get("shared_files", [])

        # Find files owned by this requirement
        owned_files = [f for f, owner in file_ownership.items() if owner == req_id]
        # Find shared files this requirement can modify
        modifiable_shared = [f for f in shared_files if req_id in file_ownership.get(f, "")]

        prompt += "\n\nã€ç»Ÿä¸€ä»£ç éª¨æ¶ä¿¡æ¯ã€‘\n"
        prompt += "é¡¹ç›®å·²ç”Ÿæˆç»Ÿä¸€ä»£ç éª¨æ¶ï¼Œè¯·åŸºäºéª¨æ¶è¿›è¡Œå¢é‡å¡«å……ï¼Œä¸è¦è¦†ç›–æ•´ä¸ªæ–‡ä»¶ã€‚\n"

        if owned_files:
            prompt += f"\næ­¤éœ€æ±‚è´Ÿè´£å¡«å……çš„æ–‡ä»¶:\n{json.dumps(owned_files, ensure_ascii=False, indent=2)}\n"
            prompt += "å¯¹äºè¿™äº›æ–‡ä»¶ï¼Œè¯·ä½¿ç”¨ action: 'modify' å¡«å…… TODO æ ‡è®°çš„éƒ¨åˆ†ã€‚\n"

        if modifiable_shared:
            prompt += f"\næ­¤éœ€æ±‚å¯ä¿®æ”¹çš„å…±äº«æ–‡ä»¶:\n{json.dumps(modifiable_shared, ensure_ascii=False, indent=2)}\n"
            prompt += "å¯¹äºå…±äº«æ–‡ä»¶ï¼Œåªå¡«å……æ ‡è®°ä¸ºæœ¬éœ€æ±‚çš„ TODO éƒ¨åˆ†ï¼Œä¿ç•™å…¶ä»–éœ€æ±‚çš„ TODO æ ‡è®°ã€‚\n"

        if shared_files:
            prompt += f"\næ‰€æœ‰å…±äº«æ–‡ä»¶åˆ—è¡¨:\n{json.dumps(shared_files, ensure_ascii=False, indent=2)}\n"
            prompt += "ã€é‡è¦ã€‘ä¸è¦åˆ é™¤æˆ–å®Œå…¨é‡å†™å…±äº«æ–‡ä»¶ï¼Œåªèƒ½å¢é‡ä¿®æ”¹ã€‚\n"

    if contextual_notes:
        prompt += "\nå…±äº«ä¸Šä¸‹æ–‡:\n" + contextual_notes

    blueprint, _ = await call_llm_json(
        llm,
        [
            {"role": "system", "content": "ä½ æ˜¯èµ„æ·±æ¶æ„/ä½“éªŒè®¾è®¡å¸ˆï¼Œè¾“å‡º Blueprint JSONã€‚"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.35,
        label=f"blueprint:{requirement.get('id', '')}",
        verbose=verbose,
    )

    # Validate and fix files_plan paths
    files_plan = blueprint.get("files_plan", [])
    from ._observability import get_logger
    logger = get_logger()

    if files_plan:
        logger.debug(f"[Blueprint] files_plan åŸå§‹è·¯å¾„: {[f.get('path', '') for f in files_plan]}")
        fixed_files_plan = []
        for f in files_plan:
            path = f.get("path", "")
            # Fix paths that don't start with frontend/ or backend/
            if path and not path.startswith(("frontend/", "backend/")):
                # Try to infer the correct prefix
                if any(path.endswith(ext) for ext in [".vue", ".jsx", ".tsx", ".css", ".scss"]):
                    # Frontend file
                    if path.startswith("src/"):
                        path = "frontend/" + path
                    elif not "/" in path or path.startswith("components/") or path.startswith("views/"):
                        path = "frontend/src/" + path
                    else:
                        path = "frontend/src/" + path
                elif any(path.endswith(ext) for ext in [".py"]):
                    # Backend file
                    if path.startswith("app/"):
                        path = "backend/" + path
                    elif not "/" in path:
                        path = "backend/app/" + path
                    else:
                        path = "backend/" + path
                logger.debug(f"[Blueprint] ä¿®æ­£è·¯å¾„: {f.get('path')} -> {path}")
                f = {**f, "path": path}
            fixed_files_plan.append(f)
        blueprint["files_plan"] = fixed_files_plan
        logger.debug(f"[Blueprint] files_plan ä¿®æ­£åè·¯å¾„: {[f.get('path', '') for f in fixed_files_plan]}")
    else:
        logger.warn(f"[Blueprint] files_plan ä¸ºç©ºï¼")

    # NOTE: Scaffold files are now pre-written BEFORE skeleton generation
    # in _execution.py, no need to add them to files_plan here

    return blueprint


def validate_blueprint_criteria_coverage(
    blueprint: dict[str, Any],
    failed_criteria: list[dict[str, Any]],
) -> dict[str, Any]:
    """Validate that Blueprint's files_plan covers all failed acceptance criteria.

    This function checks if every failed criterion is assigned to at least one
    file in the files_plan. If not, it returns the uncovered criteria IDs.

    Args:
        blueprint: Blueprint dict with files_plan
        failed_criteria: List of failed criteria that must be implemented

    Returns:
        dict: Validation result with 'is_valid', 'covered_ids', 'uncovered_ids'
    """
    files_plan = blueprint.get("files_plan", [])
    failed_ids = {c.get("id", "") for c in failed_criteria if c.get("id")}

    # Collect all criteria IDs covered by files_plan
    covered_ids: set[str] = set()
    for fp in files_plan:
        criteria_ids = fp.get("criteria_ids", [])
        if isinstance(criteria_ids, list):
            covered_ids.update(criteria_ids)

    # Find uncovered criteria
    uncovered_ids = failed_ids - covered_ids

    return {
        "is_valid": len(uncovered_ids) == 0,
        "covered_ids": list(covered_ids),
        "uncovered_ids": list(uncovered_ids),
        "coverage_ratio": len(covered_ids) / len(failed_ids) if failed_ids else 1.0,
    }


async def fix_blueprint_coverage(
    llm: Any,
    blueprint: dict[str, Any],
    uncovered_criteria: list[dict[str, Any]],
    *,
    verbose: bool = False,
) -> dict[str, Any]:
    """Fix Blueprint to cover uncovered acceptance criteria.

    Args:
        llm: LLM model instance
        blueprint: Original Blueprint
        uncovered_criteria: List of criteria not covered in files_plan
        verbose: Whether to print debug info

    Returns:
        dict: Fixed Blueprint with updated files_plan
    """
    from ._llm_utils import call_llm_json

    criteria_text = json.dumps(uncovered_criteria, ensure_ascii=False, indent=2)

    prompt = textwrap.dedent(f"""
        å½“å‰ Blueprint çš„ files_plan æœªè¦†ç›–ä»¥ä¸‹éªŒæ”¶æ ‡å‡†:
        {criteria_text}

        å½“å‰ Blueprint:
        {json.dumps(blueprint, ensure_ascii=False, indent=2)}

        è¯·æ›´æ–° Blueprintï¼Œç¡®ä¿æ‰€æœ‰éªŒæ”¶æ ‡å‡†éƒ½åˆ†é…ç»™è‡³å°‘ä¸€ä¸ªæ–‡ä»¶ã€‚

        é€‰é¡¹:
        1. åœ¨ç°æœ‰æ–‡ä»¶çš„ criteria_ids ä¸­æ·»åŠ é—æ¼çš„æ ‡å‡† ID
        2. æ·»åŠ æ–°æ–‡ä»¶æ¥å®ç°é—æ¼çš„æ ‡å‡†

        è¾“å‡ºå®Œæ•´çš„æ›´æ–°å Blueprint JSONã€‚
    """)

    fixed_blueprint, _ = await call_llm_json(
        llm,
        [
            {"role": "system", "content": "ä½ æ˜¯æ¶æ„å¸ˆï¼Œè´Ÿè´£ç¡®ä¿ Blueprint è¦†ç›–æ‰€æœ‰éªŒæ”¶æ ‡å‡†ã€‚"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
        label="fix_blueprint_coverage",
        verbose=verbose,
    )

    return fixed_blueprint


# ---------------------------------------------------------------------------
# Requirement Implementation
# ---------------------------------------------------------------------------
async def implement_requirement(
    llm: Any,
    requirement: dict[str, Any],
    blueprint: dict[str, Any],
    feedback: str,
    passed_ids: set[str],
    failed_criteria: list[dict[str, Any]],
    previous_artifact: str,
    contextual_notes: str | None = None,
    workspace_files: dict[str, str] | None = None,
    skeleton_context: dict[str, Any] | None = None,
    *,
    verbose: bool = False,
) -> dict[str, Any]:
    """Implement a requirement based on blueprint.

    Args:
        llm: LLM model instance
        requirement: Requirement dict
        blueprint: Blueprint dict
        feedback: QA feedback
        passed_ids: Set of passed criteria IDs
        failed_criteria: List of failed criteria
        previous_artifact: Previous artifact content
        contextual_notes: Context notes
        workspace_files: Existing workspace files
        skeleton_context: Skeleton context with file_ownership and shared_files
        verbose: Whether to print debug info

    Returns:
        dict: Implementation result with files
    """
    from ._llm_utils import call_llm_json

    workspace_context = ""
    if workspace_files:
        workspace_context = "\nå½“å‰å·¥ä½œåŒºæ–‡ä»¶:\n"
        for fname, content in workspace_files.items():
            workspace_context += f"\n--- {fname} (å‰ 2000 å­—ç¬¦) ---\n{content[:2000]}\n"

    # Build detailed criteria implementation guide
    criteria_impl_guide = ""
    if failed_criteria:
        criteria_impl_guide = "\n\nã€å¿…é¡»å®ç°çš„éªŒæ”¶æ ‡å‡† - è¯·é€æ¡ç¡®ä¿æ»¡è¶³ã€‘\n"
        for i, criterion in enumerate(failed_criteria, 1):
            crit_id = criterion.get("id", f"AC-{i}")
            crit_name = criterion.get("name", criterion.get("title", ""))
            crit_desc = criterion.get("description", "")
            criteria_impl_guide += f"\n{i}. [{crit_id}] {crit_name}\n"
            if crit_desc:
                criteria_impl_guide += f"   è¦æ±‚: {crit_desc}\n"
            if criterion.get("recommendation"):
                criteria_impl_guide += f"   å®ç°å»ºè®®: {criterion.get('recommendation')}\n"
            if criterion.get("reason"):
                criteria_impl_guide += f"   ä¸Šæ¬¡å¤±è´¥åŸå› : {criterion.get('reason')}\n"
        criteria_impl_guide += "\nã€è­¦å‘Šã€‘å¦‚æœéªŒæ”¶æ ‡å‡†æ¶‰åŠæ€§èƒ½æµ‹è¯•ï¼Œå¿…é¡»åœ¨ä»£ç ä¸­æ·»åŠ æ€§èƒ½éªŒè¯é€»è¾‘ï¼\n"
        criteria_impl_guide += "ã€è­¦å‘Šã€‘å¦‚æœéªŒæ”¶æ ‡å‡†æ¶‰åŠç‰¹å®šåŠŸèƒ½ï¼Œå¿…é¡»å®ç°å®Œæ•´çš„æ¥å£å’Œé€»è¾‘ï¼\n"

    prompt = textwrap.dedent(f"""
        éœ€æ±‚:
        {json.dumps(requirement, ensure_ascii=False, indent=2)}

        Blueprint:
        {json.dumps(blueprint, ensure_ascii=False, indent=2)}
        {workspace_context}
        {criteria_impl_guide}

        å·²é€šè¿‡çš„æ ‡å‡†: {sorted(passed_ids) if passed_ids else "æ— "}
        ä¸Šä¸€ç‰ˆäº¤ä»˜ç‰‡æ®µ: {previous_artifact[:1200] if previous_artifact else "æ— "}
        QA åé¦ˆ: {feedback or "æ— "}

        è¯·è¾“å‡º JSON:
        {{
          "summary": "...",
          "project_type": "fullstack|frontend|backend",
          "files": [
            {{"path": "backend/app.py", "content": "..."}}
          ],
          "setup_commands": ["pip install -r requirements.txt"],
          "run_command": "...",
          "entry_point": "http://localhost:3000"
        }}
    """)

    if contextual_notes:
        prompt += "\nå¯å¼•ç”¨çš„ä¸Šä¸‹æ–‡:\n" + contextual_notes

    # Add skeleton context for incremental filling
    if skeleton_context and skeleton_context.get("skeleton_generated"):
        req_id = requirement.get("id", "")
        file_ownership = skeleton_context.get("file_ownership", {})
        shared_files = skeleton_context.get("shared_files", [])

        prompt += "\n\nã€å¢é‡å¡«å……æ¨¡å¼ã€‘\n"
        prompt += "é¡¹ç›®ä½¿ç”¨ç»Ÿä¸€ä»£ç éª¨æ¶ï¼Œä½ éœ€è¦å¢é‡å¡«å……è€Œéè¦†ç›–æ•´ä¸ªæ–‡ä»¶ã€‚\n"

        # Find files owned by this requirement
        owned_files = [f for f, owner in file_ownership.items() if owner == req_id]
        if owned_files:
            prompt += f"\nä½ è´Ÿè´£å¡«å……çš„æ–‡ä»¶: {json.dumps(owned_files, ensure_ascii=False)}\n"
            prompt += "å¯¹äºè¿™äº›æ–‡ä»¶ä¸­çš„ TODO æ ‡è®°ï¼Œè¯·ç”Ÿæˆå®Œæ•´å®ç°ä»£ç ã€‚\n"

        if shared_files:
            prompt += f"\nå…±äº«æ–‡ä»¶ï¼ˆåªèƒ½å¢é‡ä¿®æ”¹ï¼Œä¸èƒ½è¦†ç›–ï¼‰: {json.dumps(shared_files, ensure_ascii=False)}\n"
            prompt += f"åœ¨å…±äº«æ–‡ä»¶ä¸­ï¼Œåªå¡«å……æ ‡è®°ä¸º `# TODO: [{req_id}]` çš„éƒ¨åˆ†ã€‚\n"
            prompt += "ä¿ç•™å…¶ä»–éœ€æ±‚çš„ TODO æ ‡è®°å’Œç°æœ‰ä»£ç ã€‚\n"

        prompt += "\nã€é‡è¦ã€‘å¯¹äºå·²å­˜åœ¨çš„æ–‡ä»¶ï¼Œä½ çš„ content åº”è¯¥æ˜¯ä¿®æ”¹åçš„å®Œæ•´å†…å®¹ï¼Œ"
        prompt += "ä½†è¦ä¿ç•™å…¶ä»–éœ€æ±‚çš„å ä½ç¬¦å’Œå®ç°ã€‚\n"

    impl, _ = await call_llm_json(
        llm,
        [
            {"role": "system", "content": "ä½ æ˜¯äº¤ä»˜å·¥ç¨‹å¸ˆï¼Œéœ€è¦ç”Ÿæˆæœ€ç»ˆäº§ç‰©ã€‚"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.25,
        label=f"implement:{requirement.get('id', '')}",
        verbose=verbose,
    )

    return impl


# ---------------------------------------------------------------------------
# Single File Generation
# ---------------------------------------------------------------------------
async def generate_single_file(
    llm: Any,
    requirement: dict[str, Any],
    blueprint: dict[str, Any],
    file_plan: dict[str, Any],
    generated_files: dict[str, str],
    contextual_notes: str | None = None,
    runtime_workspace: "RuntimeWorkspace | None" = None,
    skeleton_context: dict[str, Any] | None = None,
    file_criteria: list[dict[str, Any]] | None = None,
    module_registry: dict[str, list[dict[str, str]]] | None = None,
    *,
    verbose: bool = False,
) -> dict[str, Any]:
    """Generate a single file based on file plan.

    Args:
        llm: LLM model instance
        requirement: Requirement dict
        blueprint: Blueprint dict
        file_plan: File plan dict
        generated_files: Already generated files
        contextual_notes: Context notes
        runtime_workspace: RuntimeWorkspace instance
        skeleton_context: Skeleton context with file_ownership and shared_files
        file_criteria: Acceptance criteria this file should implement
        module_registry: Registry of existing modules and their exports
        verbose: Whether to print debug info

    Returns:
        dict: File result with path and content
    """
    from ._llm_utils import call_llm_json

    file_path = file_plan.get("path", "")
    file_desc = file_plan.get("description", "")
    action = file_plan.get("action", "create")
    dependencies = file_plan.get("dependencies", [])

    # Read existing content for modify action
    existing_content = ""
    if action == "modify" and runtime_workspace:
        try:
            existing_content = runtime_workspace.read_file(file_path)
        except Exception:
            pass

    # Build dependencies context
    deps_context = ""
    if dependencies and generated_files:
        deps_context = "\nã€ä¾èµ–æ–‡ä»¶å†…å®¹ã€‘\n"
        for dep in dependencies:
            if dep in generated_files:
                content = generated_files[dep]
                deps_context += f"\n--- {dep} ---\n{content[:3000]}\n"

    # Build prompt
    if action == "modify" and existing_content:
        prompt = textwrap.dedent(f"""
            ã€ä»»åŠ¡ã€‘ä¿®æ”¹æ–‡ä»¶: {file_path}

            ã€å½“å‰å†…å®¹ã€‘
            ```
            {existing_content[:6000]}
            ```

            ã€ä¿®æ”¹è¯´æ˜ã€‘
            {file_desc}

            ã€é¡¹ç›®éœ€æ±‚ã€‘
            {json.dumps(requirement, ensure_ascii=False, indent=2)}
            {deps_context}

            è¯·è¾“å‡ºä¿®æ”¹åçš„å®Œæ•´æ–‡ä»¶:
            {{"path": "{file_path}", "content": "...", "summary": "..."}}
        """)
    else:
        prompt = textwrap.dedent(f"""
            ã€ä»»åŠ¡ã€‘ç”Ÿæˆæ–‡ä»¶: {file_path}

            ã€æ–‡ä»¶æè¿°ã€‘
            {file_desc}

            ã€é¡¹ç›®éœ€æ±‚ã€‘
            {json.dumps(requirement, ensure_ascii=False, indent=2)}

            ã€Blueprintã€‘
            æ¨èæŠ€æœ¯æ ˆ: {blueprint.get('recommended_stack', '')}
            {deps_context}

            è¯·è¾“å‡ºå®Œæ•´æ–‡ä»¶:
            {{"path": "{file_path}", "content": "...", "summary": "..."}}
        """)

    if contextual_notes:
        prompt += "\nä¸Šä¸‹æ–‡:\n" + contextual_notes

    # Claude Code style: Read Before Write
    # 1. Read related modules before generating code
    workspace_files: dict[str, str] | None = None
    if runtime_workspace and module_registry:
        from ._code_context import (
            format_module_registry_for_prompt,
            get_interface_constraint_prompt,
            read_related_modules,
            scan_workspace_files,
        )
        from pathlib import Path

        # Scan workspace files for Read Before Write
        workspace_path = Path(runtime_workspace.workspace_dir)
        workspace_files = scan_workspace_files(workspace_path)

        # Add related module contents (Read Before Write)
        related_content = read_related_modules(
            file_path, workspace_files, module_registry, max_modules=3
        )
        if related_content:
            prompt += f"\n\n{related_content}"

        # Add module registry
        registry_str = format_module_registry_for_prompt(module_registry, max_modules=20)
        if registry_str:
            prompt += f"\n\n{registry_str}"
            # Add language-agnostic interface constraint
            prompt += f"\n{get_interface_constraint_prompt()}"
    elif module_registry:
        from ._code_context import format_module_registry_for_prompt, get_interface_constraint_prompt
        registry_str = format_module_registry_for_prompt(module_registry, max_modules=20)
        if registry_str:
            prompt += f"\n\n{registry_str}"
            prompt += f"\n{get_interface_constraint_prompt()}"

    # Add skeleton context for shared files
    if skeleton_context and skeleton_context.get("skeleton_generated"):
        req_id = requirement.get("id", "")
        shared_files = skeleton_context.get("shared_files", [])

        if file_path in shared_files:
            prompt += f"\n\nã€å…±äº«æ–‡ä»¶æ³¨æ„äº‹é¡¹ã€‘\n"
            prompt += f"æ­¤æ–‡ä»¶ ({file_path}) æ˜¯å…±äº«æ–‡ä»¶ï¼Œè¢«å¤šä¸ªéœ€æ±‚ä½¿ç”¨ã€‚\n"
            prompt += f"åªå¡«å……æ ‡è®°ä¸º `# TODO: [{req_id}]` çš„éƒ¨åˆ†ã€‚\n"
            prompt += "ä¿ç•™å…¶ä»–éœ€æ±‚çš„ TODO æ ‡è®°å’Œç°æœ‰ä»£ç ï¼Œä¸è¦åˆ é™¤æˆ–è¦†ç›–ã€‚\n"

    # Add acceptance criteria that this file must implement
    if file_criteria:
        prompt += "\n\nã€æ­¤æ–‡ä»¶å¿…é¡»å®ç°çš„éªŒæ”¶æ ‡å‡†ã€‘\n"
        for i, criterion in enumerate(file_criteria, 1):
            crit_id = criterion.get("id", f"AC-{i}")
            crit_name = criterion.get("name", criterion.get("title", ""))
            crit_desc = criterion.get("description", "")
            prompt += f"{i}. [{crit_id}] {crit_name}\n"
            if crit_desc:
                prompt += f"   è¦æ±‚: {crit_desc}\n"
        prompt += "\nã€å…³é”®ã€‘ç”Ÿæˆçš„ä»£ç å¿…é¡»å®Œæ•´å®ç°ä¸Šè¿°æ¯ä¸ªéªŒæ”¶æ ‡å‡†çš„åŠŸèƒ½ï¼\n"

    result, _ = await call_llm_json(
        llm,
        [
            {"role": "system", "content": "ä½ æ˜¯äº¤ä»˜å·¥ç¨‹å¸ˆï¼Œç”Ÿæˆå•ä¸ªä»£ç æ–‡ä»¶ã€‚"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.25,
        label=f"generate_file:{file_path}",
        verbose=verbose,
    )

    # Claude Code style: Validate and fix imports after generation
    if result and module_registry:
        from ._code_context import (
            validate_imports_strict,
            fix_invalid_imports,
        )

        content = result.get("content", "")
        if content:
            validation = validate_imports_strict(
                content,
                file_path,
                module_registry,
                workspace_files=workspace_files,
            )

            if not validation.valid:
                # Auto-fix invalid imports
                fixed_content = fix_invalid_imports(content, validation)
                result["content"] = fixed_content

                # Log the fixes
                if verbose:
                    for inv in validation.invalid_imports:
                        fix = validation.suggested_fixes.get(inv["import"])
                        if fix:
                            print(f"  [AutoFix] {inv['import']} -> {fix}")
                        else:
                            print(f"  [Warning] Invalid import: {inv['import']} - {inv['reason']}")

    return result


# ---------------------------------------------------------------------------
# File Sorting
# ---------------------------------------------------------------------------
def sort_files_by_dependency(files_plan: list[dict[str, Any]]) -> list[dict[str, Any]]:
    """Sort files by dependency order using topological sort.

    Args:
        files_plan: List of file plans

    Returns:
        list: Sorted file plans
    """
    if not files_plan:
        return []

    # Build dependency graph
    path_to_file = {f["path"]: f for f in files_plan}
    in_degree = {f["path"]: 0 for f in files_plan}
    dependents: dict[str, list[str]] = {f["path"]: [] for f in files_plan}

    for f in files_plan:
        for dep in f.get("dependencies", []):
            if dep in path_to_file:
                in_degree[f["path"]] += 1
                dependents[dep].append(f["path"])

    # Topological sort with priority
    result: list[dict[str, Any]] = []
    queue = sorted(
        [p for p, d in in_degree.items() if d == 0],
        key=lambda p: path_to_file[p].get("priority", 99),
    )

    while queue:
        path = queue.pop(0)
        result.append(path_to_file[path])

        for dep_path in dependents[path]:
            in_degree[dep_path] -= 1
            if in_degree[dep_path] == 0:
                priority = path_to_file[dep_path].get("priority", 99)
                insert_idx = len(queue)
                for i, p in enumerate(queue):
                    if path_to_file[p].get("priority", 99) > priority:
                        insert_idx = i
                        break
                queue.insert(insert_idx, dep_path)

    # Add remaining files (cycles)
    remaining = [f for f in files_plan if f["path"] not in [r["path"] for r in result]]
    result.extend(sorted(remaining, key=lambda f: f.get("priority", 99)))

    return result


# ---------------------------------------------------------------------------
# Scaffold Commands - LLM-driven package management
# ---------------------------------------------------------------------------
def _build_scaffold_commands(
    config: dict[str, Any],
    scaffold_type: str,
    is_initialized: bool,
    installed_packages: set[str],
) -> tuple[list[str], set[str]]:
    """Build scaffold commands based on LLM-provided configuration.

    The LLM provides:
    - init_command: Project initialization (run once)
    - install_template: Template for installing packages ({packages} placeholder)
    - packages: List of required packages

    Args:
        config: Scaffold config from blueprint (e.g., frontend or backend section)
        scaffold_type: "frontend" or "backend" (for logging)
        is_initialized: Whether project is already initialized
        installed_packages: Set of already installed packages

    Returns:
        Tuple of (list of commands to execute, set of packages to track)
    """
    commands: list[str] = []
    packages_to_track: set[str] = set()

    if not config:
        return commands, packages_to_track

    # Handle legacy format (single "command" field)
    if "command" in config and "packages" not in config:
        # Legacy: just run the command if not initialized
        if not is_initialized:
            commands.append(config["command"])
        return commands, packages_to_track

    # New format with explicit init_command, install_template, packages
    init_cmd = config.get("init_command")
    install_template = config.get("install_template")
    packages = config.get("packages", [])

    # Convert packages to set for tracking
    required_packages = set(packages) if isinstance(packages, list) else set()
    packages_to_track = required_packages

    if not is_initialized:
        # First time: run init command if provided
        if init_cmd:
            commands.append(init_cmd)

        # Install all packages
        if install_template and required_packages:
            install_cmd = install_template.replace("{packages}", ' '.join(sorted(required_packages)))
            commands.append(install_cmd)
    else:
        # Already initialized: only install new packages
        new_packages = required_packages - installed_packages
        if install_template and new_packages:
            install_cmd = install_template.replace("{packages}", ' '.join(sorted(new_packages)))
            commands.append(install_cmd)

    return commands, packages_to_track


async def run_scaffold_commands(
    runtime_workspace: "RuntimeWorkspace",
    scaffold_config: dict[str, Any],
    llm: Any,
    *,
    scaffold_initialized: dict[str, bool] | None = None,
    installed_packages: dict[str, set[str]] | None = None,
    verbose: bool = False,
) -> dict[str, Any]:
    """Execute scaffold initialization commands with incremental package installation.

    Args:
        runtime_workspace: RuntimeWorkspace instance
        scaffold_config: Scaffold configuration
        llm: LLM for result judgment
        scaffold_initialized: Dict tracking which scaffold types are initialized
        installed_packages: Dict tracking installed packages per scaffold type
        verbose: Whether to print debug info

    Returns:
        dict: Execution result with new_packages info
    """
    result = {
        "success": True,
        "commands_executed": [],
        "outputs": [],
        "errors": [],
        "new_packages": {"frontend": set(), "backend": set()},
        "initialized": {"frontend": False, "backend": False},
    }

    scaffold_initialized = scaffold_initialized or {}
    installed_packages = installed_packages or {"frontend": set(), "backend": set()}

    async def execute_cmd(cmd: str, description: str, timeout: int = 300) -> bool:
        from ._observability import get_logger
        logger = get_logger()
        logger.info(f"[SCAFFOLD] {description}: {cmd}")
        result["commands_executed"].append(cmd)

        try:
            exec_result = runtime_workspace.execute_command(cmd, timeout=timeout)
            output = exec_result.get("output", "")
            error = exec_result.get("error", "")
            result["outputs"].append(output)

            if exec_result.get("success"):
                logger.info(f"[SCAFFOLD] âœ“ {description}å®Œæˆ")
                return True
            else:
                result["errors"].append(f"{description}å¤±è´¥: {error[:100]}")
                logger.warn(f"[SCAFFOLD] âš  {description}å¤±è´¥")
                return False
        except Exception as e:
            result["errors"].append(str(e))
            return False

    # Frontend scaffold
    frontend_cfg = scaffold_config.get("frontend")
    if frontend_cfg:
        is_init = scaffold_initialized.get("frontend", False)
        commands, new_pkgs = _build_scaffold_commands(
            frontend_cfg,
            "frontend",
            is_init,
            installed_packages.get("frontend", set()),
        )
        for cmd in commands:
            desc = "å‰ç«¯å¢é‡å®‰è£…" if is_init else "å‰ç«¯åˆå§‹åŒ–"
            await execute_cmd(cmd, desc)
        if new_pkgs:
            result["new_packages"]["frontend"] = new_pkgs
        if not is_init and commands:
            result["initialized"]["frontend"] = True
            # Fix permissions after frontend initialization
            # This ensures Claude Code (node user) can modify the files
            await execute_cmd(
                "chown -R node:node /workspace/frontend 2>/dev/null || true",
                "å‰ç«¯æƒé™ä¿®å¤"
            )

        # Post-init commands only run on first initialization
        if not is_init:
            for post_cmd in frontend_cfg.get("post_init", []):
                await execute_cmd(post_cmd, "å‰ç«¯ post_init", timeout=600)

    # Backend scaffold
    backend_cfg = scaffold_config.get("backend")
    if backend_cfg:
        is_init = scaffold_initialized.get("backend", False)
        commands, new_pkgs = _build_scaffold_commands(
            backend_cfg,
            "backend",
            is_init,
            installed_packages.get("backend", set()),
        )
        for cmd in commands:
            desc = "åç«¯å¢é‡å®‰è£…" if is_init else "åç«¯åˆå§‹åŒ–"
            await execute_cmd(cmd, desc)
        if new_pkgs:
            result["new_packages"]["backend"] = new_pkgs
        if not is_init and commands:
            result["initialized"]["backend"] = True
            # Fix permissions after backend initialization
            # This ensures Claude Code (node user) can modify the files
            await execute_cmd(
                "chown -R node:node /workspace/backend 2>/dev/null || true",
                "åç«¯æƒé™ä¿®å¤"
            )

        # Post-init commands only run on first initialization
        if not is_init:
            for post_cmd in backend_cfg.get("post_init", []):
                await execute_cmd(post_cmd, "åç«¯ post_init", timeout=600)

    return result


# ---------------------------------------------------------------------------
# Claude Code File Generation
# ---------------------------------------------------------------------------
async def generate_file_with_claude_code(
    requirement: dict[str, Any],
    blueprint: dict[str, Any],
    file_plan: dict[str, Any],
    workspace_dir: str,
    generated_files: dict[str, str],
    file_criteria: list[dict[str, Any]] | None = None,
    contextual_notes: str | None = None,
    *,
    runtime_workspace: Any | None = None,
    verbose: bool = False,
) -> dict[str, Any]:
    """Generate a single file using Claude Code CLI.

    Args:
        requirement: Requirement dict
        blueprint: Blueprint dict
        file_plan: File plan dict with path, description, action
        workspace_dir: Workspace directory path
        generated_files: Already generated files for context
        file_criteria: Acceptance criteria this file should implement
        contextual_notes: Additional context notes
        runtime_workspace: RuntimeWorkspace for reading files from container
        verbose: Whether to print debug info

    Returns:
        dict: File result with path and content
    """
    from ._claude_code import claude_code_edit
    from pathlib import Path

    file_path = file_plan.get("path", "")
    file_desc = file_plan.get("description", "")
    action = file_plan.get("action", "create")

    # Build the prompt for Claude Code
    prompt_parts = []

    # Task description
    if action == "modify":
        prompt_parts.append(f"ä¿®æ”¹æ–‡ä»¶ {file_path}:")
    else:
        prompt_parts.append(f"åˆ›å»ºæ–‡ä»¶ {file_path}:")

    prompt_parts.append(f"\næ–‡ä»¶æè¿°: {file_desc}")

    # Requirement context
    prompt_parts.append(f"\n\néœ€æ±‚: {requirement.get('title', '')}")
    if requirement.get("description"):
        prompt_parts.append(f"\n{requirement.get('description')[:500]}")

    # Blueprint context
    if blueprint.get("recommended_stack"):
        prompt_parts.append(f"\n\næŠ€æœ¯æ ˆ: {blueprint.get('recommended_stack')}")
    if blueprint.get("deliverable_pitch"):
        prompt_parts.append(f"\nç›®æ ‡: {blueprint.get('deliverable_pitch')}")

    # Acceptance criteria
    if file_criteria:
        prompt_parts.append("\n\nå¿…é¡»æ»¡è¶³çš„éªŒæ”¶æ ‡å‡†:")
        for i, criterion in enumerate(file_criteria, 1):
            crit_id = criterion.get("id", f"AC-{i}")
            crit_name = criterion.get("name", criterion.get("title", ""))
            crit_desc = criterion.get("description", "")
            prompt_parts.append(f"\n{i}. [{crit_id}] {crit_name}")
            if crit_desc:
                prompt_parts.append(f"\n   è¦æ±‚: {crit_desc}")

    # Dependencies context
    dependencies = file_plan.get("dependencies", [])
    if dependencies and generated_files:
        prompt_parts.append("\n\nç›¸å…³ä¾èµ–æ–‡ä»¶:")
        for dep in dependencies[:3]:
            if dep in generated_files:
                content = generated_files[dep]
                prompt_parts.append(f"\n--- {dep} ---\n{content[:2000]}")

    # Additional context
    if contextual_notes:
        prompt_parts.append(f"\n\nä¸Šä¸‹æ–‡:\n{contextual_notes[:1000]}")

    prompt = "".join(prompt_parts)

    # Call Claude Code
    try:
        result = await claude_code_edit(
            prompt=prompt,
            workspace=workspace_dir,
            timeout=120,  # 2 minutes per file
        )

        # Parse the response
        response_text = ""
        if result.content:
            for block in result.content:
                if hasattr(block, "get"):
                    response_text += block.get("text", "")
                elif hasattr(block, "text"):
                    response_text += block.text

        # Read the generated file content
        # Priority: 1) runtime_workspace (container), 2) local path, 3) extract from response
        content = ""

        # Try reading from container via runtime_workspace
        if runtime_workspace:
            try:
                content = await runtime_workspace.read_file(file_path)
                if content:
                    return {
                        "path": file_path,
                        "content": content,
                        "summary": f"Claude Code ç”Ÿæˆ (å®¹å™¨): {response_text[:100]}",
                    }
            except Exception as read_err:
                if verbose:
                    logger.debug(f"[ClaudeCode] ä»å®¹å™¨è¯»å–å¤±è´¥ {file_path}: {read_err}")

        # Fallback: try local path
        full_path = Path(workspace_dir) / file_path
        if full_path.exists():
            content = full_path.read_text(encoding="utf-8")
            return {
                "path": file_path,
                "content": content,
                "summary": f"Claude Code ç”Ÿæˆ: {response_text[:100]}",
            }

        # Fallback: try to extract code from the response
        if "```" in response_text:
            import re
            code_match = re.search(r"```(?:\w+)?\n(.*?)```", response_text, re.DOTALL)
            if code_match:
                content = code_match.group(1)
                # Write to container if runtime_workspace available
                if runtime_workspace:
                    try:
                        await runtime_workspace.write_file(file_path, content)
                    except Exception:
                        pass  # Ignore write errors
                else:
                    # Fallback: write locally
                    full_path.parent.mkdir(parents=True, exist_ok=True)
                    full_path.write_text(content, encoding="utf-8")
                return {
                    "path": file_path,
                    "content": content,
                    "summary": f"ä»å“åº”æå–: {response_text[:100]}",
                }

        return {
            "path": file_path,
            "content": "",
            "summary": f"Claude Code æœªç”Ÿæˆæ–‡ä»¶: {response_text[:200]}",
        }

    except Exception as e:
        if verbose:
            from ._observability import get_logger
            get_logger().warn(f"[ClaudeCode] ç”Ÿæˆ {file_path} å¤±è´¥: {e}")
        return {
            "path": file_path,
            "content": "",
            "summary": f"ç”Ÿæˆå¤±è´¥: {str(e)}",
        }


# ---------------------------------------------------------------------------
# Autonomous Claude Code Implementation (NEW - Recommended)
# ---------------------------------------------------------------------------
async def implement_requirement_autonomous(
    requirement: dict[str, Any],
    blueprint: dict[str, Any],
    all_criteria: list[dict[str, Any]] | None = None,
    workspace_dir: str | None = None,
    contextual_notes: str | None = None,
    feedback: str = "",
    failed_criteria: list[dict[str, Any]] | None = None,
    previous_errors: list[str] | None = None,
    architecture_contract: dict[str, Any] | None = None,
    *,
    verbose: bool = False,
) -> dict[str, Any]:
    """Implement a requirement by delegating full control to Claude Code.

    Unlike stepwise mode, this gives Claude Code complete autonomy to:
    - Explore the existing codebase
    - Decide which files to create or modify
    - Implement the solution holistically

    Args:
        requirement: Requirement dict with id, title, description
        blueprint: Blueprint with recommended_stack, deliverable_pitch
        all_criteria: Acceptance criteria to satisfy
        workspace_dir: Workspace directory path
        contextual_notes: Additional context
        feedback: Feedback from previous round
        failed_criteria: List of failed acceptance criteria
        previous_errors: List of validation errors from previous round
        architecture_contract: Architecture contract with API endpoints
        verbose: Whether to print debug info

    Returns:
        dict: Implementation result with files and developer_summary
    """
    from ._claude_code import claude_code_edit
    from ._observability import get_logger
    from pathlib import Path

    logger = get_logger()
    req_id = requirement.get("id", "REQ-???")

    if verbose:
        logger.info(f"[{req_id}] ğŸ¤– è‡ªä¸»æ¨¡å¼: Claude Code å…¨æƒè´Ÿè´£å®ç°")

    # Build comprehensive prompt for Claude Code
    prompt_parts = []

    # 1. Task header
    prompt_parts.append(f"# ä»»åŠ¡: å®ç°éœ€æ±‚ {req_id}")
    prompt_parts.append(f"\n## éœ€æ±‚: {requirement.get('title', '')}")
    if requirement.get("description"):
        prompt_parts.append(f"\n{requirement.get('description')}")

    # 2. Technology stack
    if blueprint.get("recommended_stack"):
        prompt_parts.append(f"\n\n## æŠ€æœ¯æ ˆ\n{blueprint.get('recommended_stack')}")

    # 3. Goal
    if blueprint.get("deliverable_pitch"):
        prompt_parts.append(f"\n\n## ç›®æ ‡\n{blueprint.get('deliverable_pitch')}")

    # 4. Acceptance criteria (CRITICAL)
    if all_criteria:
        prompt_parts.append("\n\n## éªŒæ”¶æ ‡å‡† (å¿…é¡»å…¨éƒ¨æ»¡è¶³)")
        for i, criterion in enumerate(all_criteria, 1):
            crit_id = criterion.get("id", f"AC-{i}")
            crit_name = criterion.get("name", criterion.get("title", ""))
            crit_desc = criterion.get("description", "")
            prompt_parts.append(f"\n{i}. **[{crit_id}]** {crit_name}")
            if crit_desc:
                prompt_parts.append(f"\n   - {crit_desc}")

    # 5. Architecture contract (if available)
    if architecture_contract:
        endpoints = architecture_contract.get("api_endpoints", [])
        if endpoints:
            prompt_parts.append("\n\n## API æ¶æ„å¥‘çº¦")
            for ep in endpoints[:10]:  # Limit to first 10
                method = ep.get("method", "GET")
                path = ep.get("path", "")
                desc = ep.get("description", "")
                prompt_parts.append(f"\n- `{method} {path}`: {desc}")

    # 6. Previous round feedback (if any)
    if feedback or failed_criteria or previous_errors:
        prompt_parts.append("\n\n## âš ï¸ ä¸Šä¸€è½®é—®é¢˜ (å¿…é¡»ä¿®å¤)")
        if feedback:
            prompt_parts.append(f"\nåé¦ˆ: {feedback}")
        if failed_criteria:
            prompt_parts.append("\næœªé€šè¿‡çš„éªŒæ”¶æ ‡å‡†:")
            for fc in failed_criteria:
                prompt_parts.append(f"\n- {fc.get('name', '')}: {fc.get('reason', '')}")
        if previous_errors:
            prompt_parts.append("\nä»£ç é”™è¯¯:")
            for err in previous_errors[:5]:
                prompt_parts.append(f"\n- {err}")

    # 7. Additional context
    if contextual_notes:
        prompt_parts.append(f"\n\n## ä¸Šä¸‹æ–‡\n{contextual_notes[:2000]}")

    # 8. Instructions
    prompt_parts.append("""

## å®ç°æŒ‡å—

è¯·ä½ ï¼š
1. **é¦–å…ˆæ¢ç´¢ä»£ç åº“**ï¼Œäº†è§£ç°æœ‰ç»“æ„å’Œæ¨¡å¼
2. **è‡ªä¸»å†³å®š**éœ€è¦åˆ›å»ºæˆ–ä¿®æ”¹å“ªäº›æ–‡ä»¶
3. **å®Œæ•´å®ç°**éœ€æ±‚ï¼Œç¡®ä¿æ‰€æœ‰éªŒæ”¶æ ‡å‡†éƒ½èƒ½é€šè¿‡
4. **éµå¾ªç°æœ‰ä»£ç é£æ ¼**å’Œé¡¹ç›®çº¦å®š
5. **ç¡®ä¿å¯¼å…¥æ­£ç¡®**ï¼Œé¿å…å¾ªç¯å¯¼å…¥

ä½ æœ‰å®Œå…¨çš„è‡ªä¸»æƒæ¥å†³å®šæœ€ä½³å®ç°æ–¹æ¡ˆã€‚å¼€å§‹å§ï¼
""")

    prompt = "".join(prompt_parts)

    if verbose:
        logger.debug(f"[{req_id}] Prompt é•¿åº¦: {len(prompt)} å­—ç¬¦")

    # Call Claude Code with extended timeout
    try:
        result = await claude_code_edit(
            prompt=prompt,
            workspace=workspace_dir,
            timeout=600,  # 10 minutes for full requirement
        )

        # Parse the response
        response_text = ""
        if result.content:
            for block in result.content:
                if hasattr(block, "get"):
                    response_text += block.get("text", "")
                elif hasattr(block, "text"):
                    response_text += block.text

        if verbose:
            logger.info(f"[{req_id}] Claude Code å®Œæˆ: {response_text[:200]}...")

        # Scan workspace for generated/modified files
        files_output = []
        if workspace_dir:
            ws_path = Path(workspace_dir)
            # Get all source files
            for ext in ["py", "ts", "js", "vue", "tsx", "jsx", "json"]:
                for f in ws_path.rglob(f"*.{ext}"):
                    if "node_modules" in str(f) or "__pycache__" in str(f):
                        continue
                    rel_path = str(f.relative_to(ws_path))
                    try:
                        content = f.read_text(encoding="utf-8")
                        files_output.append({
                            "path": rel_path,
                            "content": content,
                        })
                    except Exception:
                        pass

        return {
            "files": files_output,
            "developer_summary": f"Claude Code è‡ªä¸»å®ç°: {response_text[:500]}",
            "mode": "autonomous",
        }

    except Exception as e:
        logger.error(f"[{req_id}] Claude Code æ‰§è¡Œå¤±è´¥: {e}")
        return {
            "files": [],
            "developer_summary": f"å®ç°å¤±è´¥: {str(e)}",
            "mode": "autonomous",
        }


# ---------------------------------------------------------------------------
# Stepwise Generation (Legacy)
# ---------------------------------------------------------------------------
async def stepwise_generate_files(
    llm: Any,
    requirement: dict[str, Any],
    blueprint: dict[str, Any],
    contextual_notes: str | None = None,
    runtime_workspace: "RuntimeWorkspace | None" = None,
    feedback: str = "",
    failed_criteria: list[dict[str, Any]] | None = None,
    previous_errors: list[str] | None = None,
    skeleton_context: dict[str, Any] | None = None,
    all_criteria: list[dict[str, Any]] | None = None,
    workspace_dir: "Any | None" = None,
    code_guard: "Any | None" = None,
    *,
    verbose: bool = False,
) -> dict[str, Any]:
    """Generate files step by step based on blueprint.

    Args:
        llm: LLM model instance
        requirement: Requirement dict
        blueprint: Blueprint dict
        contextual_notes: Context notes
        runtime_workspace: RuntimeWorkspace instance
        feedback: Feedback from previous round (QA/validation issues)
        failed_criteria: List of failed acceptance criteria
        previous_errors: List of validation errors from previous round
        skeleton_context: Skeleton context with file_ownership and shared_files
        all_criteria: All acceptance criteria for the requirement
        workspace_dir: Workspace directory for module scanning
        code_guard: CodeGuardManager instance for real-time validation
        verbose: Whether to print debug info

    Returns:
        dict: Implementation result compatible with implement_requirement
    """
    files_plan = blueprint.get("files_plan", [])
    if not files_plan:
        raise ValueError("Blueprint ç¼ºå°‘ files_plan å­—æ®µ")

    from ._observability import get_logger
    logger = get_logger()

    sorted_files = sort_files_by_dependency(files_plan)
    if verbose:
        logger.debug(f"[STEPWISE] æ–‡ä»¶ç”Ÿæˆé¡ºåº: {[f['path'] for f in sorted_files]}")

    # Build module registry for import validation (Claude Code style)
    module_registry: dict[str, list[dict[str, str]]] = {}
    if workspace_dir:
        from pathlib import Path
        from ._code_context import scan_workspace_files, build_module_registry
        ws_path = Path(workspace_dir) if not isinstance(workspace_dir, Path) else workspace_dir
        if ws_path.exists():
            existing_files = scan_workspace_files(ws_path)
            module_registry = build_module_registry(existing_files)
            if verbose and module_registry:
                logger.debug(f"[STEPWISE] æ‰«æåˆ° {len(module_registry)} ä¸ªç°æœ‰æ¨¡å—")

    # Build feedback context from previous round errors
    # Using structured error classification (æ¡†æ¶æ™®é€‚åŒ– + Agentç‰¹åŒ–)
    feedback_context = ""
    if feedback:
        feedback_context += f"\nã€ä¸Šä¸€è½®åé¦ˆã€‘\n{feedback}\n"
    if failed_criteria:
        feedback_context += "\nã€æœªé€šè¿‡çš„éªŒæ”¶æ ‡å‡†ã€‘\n"
        for criterion in failed_criteria:
            feedback_context += f"- {criterion.get('name', '')}: {criterion.get('reason', '')}\n"
    if previous_errors:
        # Use the framework's error classification system
        from ._execution import _classify_errors, ErrorCategory

        classified = _classify_errors(previous_errors)

        # Show raw errors first (limited)
        feedback_context += "\nã€ä¸Šä¸€è½®ä»£ç é”™è¯¯ - å¿…é¡»ä¿®å¤ã€‘\n"
        for err in previous_errors[:10]:
            feedback_context += f"- {err}\n"

        # ===================================================================
        # Agent Specialization Layer (Agentç‰¹åŒ–)
        # Provide domain-specific guidance based on error categories
        # ===================================================================

        # 1. CIRCULAR IMPORT - Most critical, blocks everything
        if classified[ErrorCategory.CIRCULAR_IMPORT]:
            feedback_context += (
                "\nã€ä¸¥é‡ï¼šå¾ªç¯å¯¼å…¥é”™è¯¯ã€‘\n"
                "è¿™æ˜¯å¯¼è‡´é¡¹ç›®æ— æ³•è¿è¡Œçš„æ ¹æœ¬åŸå› ï¼\n"
                "\nä¿®å¤ç­–ç•¥ï¼ˆé€‰æ‹©å…¶ä¸€ï¼‰ï¼š\n"
                "ç­–ç•¥A - æœ€å°åŒ– __init__.pyï¼š\n"
                "```python\n"
                "# app/__init__.py\n"
                "__all__ = ['models', 'schemas', 'routers']\n"
                "# ä¸è¦å¯¼å…¥å…·ä½“ç±»ï¼è®©ç”¨æˆ·ä»å­æ¨¡å—å¯¼å…¥\n"
                "```\n"
                "\nç­–ç•¥B - ä½¿ç”¨ TYPE_CHECKINGï¼š\n"
                "```python\n"
                "from typing import TYPE_CHECKING\n"
                "if TYPE_CHECKING:\n"
                "    from .models.member import Member  # åªç”¨äºç±»å‹æç¤º\n"
                "```\n"
            )

        # 2. TYPE CHECK ERRORS - Common with mypy/tsc
        elif classified[ErrorCategory.TYPE_CHECK]:
            type_errors = classified[ErrorCategory.TYPE_CHECK]
            feedback_context += (
                f"\nã€ç±»å‹æ£€æŸ¥é”™è¯¯ã€‘æ£€æµ‹åˆ° {len(type_errors)} ä¸ªç±»å‹é”™è¯¯\n"
                "\nå¸¸è§é—®é¢˜å’Œä¿®å¤æ–¹æ³•ï¼š\n"
                "\n1. SQLAlchemy æ¨¡å‹ç±»å‹æ³¨è§£ï¼š\n"
                "```python\n"
                "# âŒ é”™è¯¯\n"
                "class User(Base):\n"
                "    name: str = Column(String)  # Column ä¸æ˜¯ str\n"
                "\n"
                "# âœ… æ­£ç¡® - ä½¿ç”¨ Mapped\n"
                "from sqlalchemy.orm import Mapped, mapped_column\n"
                "class User(Base):\n"
                "    name: Mapped[str] = mapped_column(String)\n"
                "```\n"
                "\n2. Optional ç±»å‹å¤„ç†ï¼š\n"
                "```python\n"
                "# âŒ é”™è¯¯\n"
                "def get_user(id: int) -> User:  # å¯èƒ½è¿”å› None\n"
                "\n"
                "# âœ… æ­£ç¡®\n"
                "def get_user(id: int) -> User | None:\n"
                "```\n"
                "\n3. Pydantic v2 è¿ç§»ï¼š\n"
                "```python\n"
                "# âŒ é”™è¯¯ - Pydantic v1 è¯­æ³•\n"
                "from pydantic import BaseSettings\n"
                "\n"
                "# âœ… æ­£ç¡® - Pydantic v2 è¯­æ³•\n"
                "from pydantic_settings import BaseSettings\n"
                "```\n"
            )

        # 3. IMPORT MISSING - Symbol not exported
        elif classified[ErrorCategory.IMPORT_MISSING]:
            import_errors = classified[ErrorCategory.IMPORT_MISSING]
            feedback_context += "\nã€å¯¼å…¥ç¼ºå¤±é”™è¯¯ã€‘\n"
            for err_info in import_errors[:3]:  # Show first 3
                symbol = err_info.get("symbol", "Unknown")
                package = err_info.get("package", "Unknown")
                feedback_context += (
                    f"\næ— æ³•ä» `{package}` å¯¼å…¥ `{symbol}`\n"
                    f"\nä¿®å¤æ–¹æ³•ï¼ˆé€‰æ‹©å…¶ä¸€ï¼‰ï¼š\n"
                    f"1. åœ¨ `{package.replace('.', '/')}/__init__.py` æ·»åŠ å¯¼å‡ºï¼š\n"
                    f"   ```python\n"
                    f"   from .æ¨¡å—å import {symbol}\n"
                    f"   ```\n"
                    f"2. ä¿®æ”¹å¯¼å…¥è¯­å¥ä½¿ç”¨å®Œæ•´è·¯å¾„ï¼š\n"
                    f"   ```python\n"
                    f"   from {package}.æ¨¡å—å import {symbol}\n"
                    f"   ```\n"
                )

        # 4. MODULE NOT FOUND - Missing dependency
        elif classified[ErrorCategory.MODULE_NOT_FOUND]:
            module_errors = classified[ErrorCategory.MODULE_NOT_FOUND]
            feedback_context += "\nã€ç¼ºå¤±ä¾èµ–é”™è¯¯ã€‘\n"
            for err_info in module_errors[:3]:
                module = err_info.get("module", "Unknown")
                feedback_context += f"- ç¼ºå¤±æ¨¡å—: {module}\n"
            feedback_context += (
                "\nå¿…é¡»æ›´æ–°ä¾èµ–æ–‡ä»¶ï¼š\n"
                "- Python: åœ¨ requirements.txt æ·»åŠ ä¾èµ–\n"
                "- Node.js: åœ¨ package.json çš„ dependencies æ·»åŠ \n"
                "- å°†ä¾èµ–æ–‡ä»¶åŠ å…¥å¾…ä¿®æ”¹æ–‡ä»¶åˆ—è¡¨ï¼\n"
            )

        # 5. SYNTAX ERRORS
        elif classified[ErrorCategory.SYNTAX]:
            feedback_context += (
                "\nã€è¯­æ³•é”™è¯¯ã€‘\n"
                "ä»£ç ç”Ÿæˆä¸å®Œæ•´ï¼Œè¯·æ£€æŸ¥ï¼š\n"
                "1. å­—ç¬¦ä¸²å¼•å·æ˜¯å¦æˆå¯¹\n"
                "2. æ‹¬å· () [] {} æ˜¯å¦é…å¯¹\n"
                "3. ä»£ç æ˜¯å¦è¢«æˆªæ–­\n"
            )

        # 6. INIT MISSING - Database/file not initialized
        elif classified[ErrorCategory.INIT_MISSING]:
            init_errors = classified[ErrorCategory.INIT_MISSING]
            feedback_context += "\nã€åˆå§‹åŒ–ç¼ºå¤±é”™è¯¯ã€‘\n"
            for err_info in init_errors:
                file_path = err_info.get("file", "")
                if "db" in file_path.lower() or "database" in file_path.lower():
                    feedback_context += (
                        "\næ•°æ®åº“æœªåˆå§‹åŒ–ï¼\n"
                        "\nä¿®å¤æ–¹æ³• - åœ¨åº”ç”¨å¯åŠ¨æ—¶åˆ›å»ºè¡¨ï¼š\n"
                        "```python\n"
                        "# FastAPI with lifespan\n"
                        "from contextlib import asynccontextmanager\n"
                        "from .database import engine, Base\n"
                        "\n"
                        "@asynccontextmanager\n"
                        "async def lifespan(app: FastAPI):\n"
                        "    Base.metadata.create_all(bind=engine)\n"
                        "    yield\n"
                        "\n"
                        "app = FastAPI(lifespan=lifespan)\n"
                        "```\n"
                        "\næˆ–åœ¨ main.py é¡¶éƒ¨ç›´æ¥è°ƒç”¨ï¼š\n"
                        "```python\n"
                        "from .database import engine, Base\n"
                        "Base.metadata.create_all(bind=engine)\n"
                        "```\n"
                    )
                    break

    generated_files: dict[str, str] = {}
    summaries: list[str] = []

    # Build criteria lookup by ID
    criteria_by_id: dict[str, dict[str, Any]] = {}
    if all_criteria:
        for c in all_criteria:
            cid = c.get("id", "")
            if cid:
                criteria_by_id[cid] = c

    for i, file_plan in enumerate(sorted_files):
        file_path = file_plan["path"]
        action = file_plan.get("action", "create")
        is_scaffold = file_plan.get("is_scaffold", False)

        # Handle scaffold files - use template content directly
        if is_scaffold and file_plan.get("content_template"):
            logger.info(f"[STEPWISE] ({i + 1}/{len(sorted_files)}) [Scaffold] åˆ›å»ºåŸºç¡€æ–‡ä»¶: {file_path}")
            content = file_plan["content_template"]
            generated_files[file_path] = content

            # Write to workspace
            if runtime_workspace and runtime_workspace.is_initialized:
                try:
                    runtime_workspace.write_file(file_path, content)
                    logger.debug(f"[STEPWISE] Scaffold æ–‡ä»¶å†™å…¥å®¹å™¨: {file_path}")
                except Exception as e:
                    logger.warn(f"[STEPWISE] Scaffold å†™å…¥å¤±è´¥: {file_path} - {e}")
            elif workspace_dir:
                full_path = workspace_dir / file_path
                full_path.parent.mkdir(parents=True, exist_ok=True)
                full_path.write_text(content, encoding="utf-8")
                logger.debug(f"[STEPWISE] Scaffold æ–‡ä»¶å†™å…¥ä¸»æœº: {file_path}")

            continue  # Skip LLM generation for scaffold files

        logger.info(f"[STEPWISE] ({i + 1}/{len(sorted_files)}) {'ä¿®æ”¹' if action == 'modify' else 'ç”Ÿæˆ'}: {file_path}")

        # Get criteria for this specific file
        file_criteria_ids = file_plan.get("criteria_ids", [])
        file_criteria = [criteria_by_id[cid] for cid in file_criteria_ids if cid in criteria_by_id]
        # If no specific criteria assigned, use failed criteria
        if not file_criteria and failed_criteria:
            file_criteria = failed_criteria

        # Combine contextual notes with feedback
        combined_context = contextual_notes or ""
        if feedback_context:
            combined_context += feedback_context

        # CodeGuard real-time validation with retry
        max_retries = 2 if code_guard else 1
        code_guard_feedback = ""

        for retry in range(max_retries):
            # Add CodeGuard feedback to context for retry
            retry_context = combined_context if combined_context else ""
            if code_guard_feedback:
                retry_context += f"\n\n[CodeGuard æ£€æµ‹åˆ°é—®é¢˜ï¼Œè¯·ä¿®å¤]\n{code_guard_feedback}"

            # Choose generation method based on USE_CLAUDE_CODE flag
            if USE_CLAUDE_CODE and workspace_dir:
                # Use Claude Code for file generation
                if verbose:
                    logger.info(f"[ClaudeCode] ä½¿ç”¨ Claude Code ç”Ÿæˆ: {file_path}")
                result = await generate_file_with_claude_code(
                    requirement=requirement,
                    blueprint=blueprint,
                    file_plan=file_plan,
                    workspace_dir=str(workspace_dir),
                    generated_files=generated_files,
                    file_criteria=file_criteria if file_criteria else None,
                    contextual_notes=retry_context if retry_context else None,
                    runtime_workspace=runtime_workspace,
                    verbose=verbose,
                )
            else:
                # Use traditional LLM generation
                result = await generate_single_file(
                    llm=llm,
                    requirement=requirement,
                    blueprint=blueprint,
                    file_plan=file_plan,
                    generated_files=generated_files,
                    contextual_notes=retry_context if retry_context else None,
                    runtime_workspace=runtime_workspace,
                    skeleton_context=skeleton_context,
                    file_criteria=file_criteria if file_criteria else None,
                    module_registry=module_registry if module_registry else None,
                    verbose=verbose,
                )

            content = result.get("content", "")
            if not content:
                break

            # CodeGuard real-time validation
            if code_guard and code_guard.config.should_validate_file(file_path):
                cg_result = code_guard.validate_content(file_path, content, is_new_file=True)
                # Check for ERROR level issues
                has_errors = any(
                    issue.severity.name == "ERROR"
                    for issue in cg_result.issues
                )
                if has_errors and retry < max_retries - 1:
                    # Format errors for retry
                    code_guard_feedback = code_guard.format_warnings(cg_result)
                    if verbose:
                        logger.warn(f"[STEPWISE] CodeGuard æ£€æµ‹åˆ°é”™è¯¯ï¼Œé‡è¯• {file_path}")
                    continue  # Retry with feedback
                elif cg_result.issues and verbose:
                    # Log warnings even if not retrying
                    warnings = code_guard.format_warnings(cg_result)
                    if warnings:
                        logger.warn(f"[STEPWISE] CodeGuard ({file_path}):\n{warnings}")

            # Validation passed or max retries reached
            break

        content = result.get("content", "")
        if content:
            generated_files[file_path] = content
            summaries.append(f"- {file_path}: {result.get('summary', '')[:100]}")

            # Update module registry with newly generated file (for subsequent files to reference)
            if module_registry is not None:
                from ._code_context import (
                    extract_python_exports,
                    extract_typescript_exports,
                    extract_vue_exports,
                )
                exports: list[dict[str, str]] = []
                if file_path.endswith(".py"):
                    exports = extract_python_exports(content)
                elif file_path.endswith((".ts", ".tsx", ".js", ".jsx")):
                    exports = extract_typescript_exports(content)
                elif file_path.endswith(".vue"):
                    exports = extract_vue_exports(content)
                if exports:
                    module_registry[file_path] = exports
                    # Add @ alias for frontend files
                    if file_path.startswith("frontend/src/"):
                        alias_path = "@/" + file_path[13:]
                        module_registry[alias_path] = exports
        else:
            logger.warn(f"[STEPWISE] è­¦å‘Š: {file_path} ç”Ÿæˆå†…å®¹ä¸ºç©º")

    return {
        "summary": f"åˆ†æ­¥ç”Ÿæˆäº† {len(generated_files)} ä¸ªæ–‡ä»¶:\n" + "\n".join(summaries),
        "project_type": blueprint.get("artifact_type", "fullstack"),
        "files": [{"path": p, "content": c} for p, c in generated_files.items()],
        "setup_commands": [],
        "run_command": "",
        "entry_point": "",
    }


# ---------------------------------------------------------------------------
# LLM-Driven Unified Skeleton Analysis (metadata only, no content generation)
# ---------------------------------------------------------------------------
SKELETON_ANALYSIS_PROMPT = """åˆ†æä»¥ä¸‹é¡¹ç›®éœ€æ±‚ï¼Œè¯†åˆ«éœ€è¦é¢„å…ˆåˆ›å»ºçš„å…±äº«ä»£ç éª¨æ¶æ–‡ä»¶ã€‚

ã€é‡è¦ã€‘åªåˆ†æå¹¶è¿”å›æ–‡ä»¶å…ƒä¿¡æ¯ï¼Œä¸è¦ç”Ÿæˆå…·ä½“ä»£ç å†…å®¹ã€‚ä»£ç å°†ç”± Claude Code åœ¨å®¹å™¨ä¸­ç”Ÿæˆã€‚

## é¡¹ç›®éœ€æ±‚åˆ—è¡¨
{requirements_json}

## æ¶æ„å¥‘çº¦ï¼ˆå¦‚æœ‰ï¼‰
{architecture_contract}

## ä»»åŠ¡
åˆ†ææ‰€æœ‰éœ€æ±‚ï¼Œè¯†åˆ«ï¼š
1. å…±äº«çš„æ•°æ®å®ä½“ï¼ˆå¤šä¸ªéœ€æ±‚éƒ½éœ€è¦çš„ï¼‰
2. å…±äº«çš„APIæ¨¡å—/è·¯ç”±ï¼ˆå¤šä¸ªéœ€æ±‚éƒ½éœ€è¦çš„ï¼‰
3. å…±äº«çš„å·¥å…·å‡½æ•°æˆ–æœåŠ¡

## è¾“å‡ºæ ¼å¼
```json
{{
  "analysis": {{
    "shared_entities": [
      {{"name": "å®ä½“å", "used_by": ["REQ-001", "REQ-002"], "fields": ["å­—æ®µ1", "å­—æ®µ2"]}}
    ],
    "shared_modules": [
      {{"name": "æ¨¡å—å", "type": "router|service|util", "used_by": ["REQ-001"], "description": "æ¨¡å—æè¿°"}}
    ]
  }},
  "skeleton_files": [
    {{
      "path": "æ–‡ä»¶è·¯å¾„ï¼ˆç›¸å¯¹äºé¡¹ç›®æ ¹ç›®å½•ï¼‰",
      "description": "æ–‡ä»¶ç”¨é€”è¯´æ˜",
      "shared_by": ["REQ-001", "REQ-002"],
      "todo_sections": ["éœ€è¦å®ç°çš„åŠŸèƒ½1", "éœ€è¦å®ç°çš„åŠŸèƒ½2"]
    }}
  ],
  "file_ownership": {{
    "REQ-001": ["æ–‡ä»¶è·¯å¾„1", "æ–‡ä»¶è·¯å¾„2"],
    "REQ-002": ["æ–‡ä»¶è·¯å¾„3"]
  }},
  "reasoning": "ä¸ºä»€ä¹ˆè¿™æ ·åˆ’åˆ†éª¨æ¶"
}}
```

## é‡è¦åŸåˆ™
1. åªè¯†åˆ«è¢«å¤šä¸ªéœ€æ±‚å…±äº«çš„éª¨æ¶æ–‡ä»¶
2. æ¯ä¸ªéœ€æ±‚ç‹¬æœ‰çš„æ–‡ä»¶ä¸éœ€è¦é¢„å®šä¹‰éª¨æ¶
3. ä¸è¦ç”Ÿæˆå…·ä½“ä»£ç å†…å®¹ï¼ˆtemplateå­—æ®µï¼‰ï¼Œåªæè¿°æ–‡ä»¶ç”¨é€”å’Œéœ€è¦å®ç°çš„åŠŸèƒ½
4. todo_sections åˆ—å‡ºæ¯ä¸ªéœ€æ±‚éœ€è¦åœ¨è¯¥æ–‡ä»¶ä¸­å®ç°çš„åŠŸèƒ½ç‚¹
5. ä¸è¦å‡è®¾ç‰¹å®šçš„ç¼–ç¨‹è¯­è¨€æˆ–æ¡†æ¶ï¼Œæ ¹æ®éœ€æ±‚æ¨æ–­
"""


async def analyze_skeleton_requirements(
    llm: Any,
    requirements: list[dict[str, Any]],
    architecture_contract: dict[str, Any] | None = None,
    *,
    verbose: bool = False,
) -> dict[str, Any] | None:
    """Use LLM to analyze requirements and determine skeleton structure.

    Args:
        llm: LLM model instance
        requirements: List of requirement dicts
        architecture_contract: Architecture contract (optional)
        verbose: Whether to print debug info

    Returns:
        dict: Skeleton analysis result, or None if failed
    """
    from ._llm_utils import call_llm_json
    from ._observability import get_logger

    logger = get_logger()

    # Build requirements summary
    req_summary = []
    for req in requirements:
        req_summary.append({
            "id": req.get("id", ""),
            "title": req.get("title", ""),
            "type": req.get("type", ""),
            "description": req.get("description", "")[:500],
        })

    # Format architecture contract
    contract_str = "æ— "
    if architecture_contract:
        contract_str = json.dumps(architecture_contract, ensure_ascii=False, indent=2)

    prompt = SKELETON_ANALYSIS_PROMPT.format(
        requirements_json=json.dumps(req_summary, ensure_ascii=False, indent=2),
        architecture_contract=contract_str,
    )

    try:
        result, _ = await call_llm_json(
            llm,
            [
                {"role": "system", "content": "ä½ æ˜¯è½¯ä»¶æ¶æ„å¸ˆï¼Œå–„äºåˆ†æéœ€æ±‚å¹¶è®¾è®¡ä»£ç éª¨æ¶ç»“æ„ã€‚"},
                {"role": "user", "content": prompt},
            ],
            temperature=0.2,
            label="skeleton_analysis",
            verbose=verbose,
        )
        return result
    except Exception as exc:
        if verbose:
            logger.debug(f"[SKELETON] Failed to analyze skeleton: {exc}")
        return None


async def analyze_unified_skeleton(
    llm: Any,
    requirements: list[dict[str, Any]],
    architecture_contract: dict[str, Any] | None = None,
    *,
    verbose: bool = False,
) -> dict[str, Any]:
    """Analyze requirements to identify shared skeleton files.

    NOTE: This function only analyzes and returns metadata.
    Actual file creation is done by Claude Code in the container.

    This function:
    1. Uses LLM to analyze all requirements
    2. Identifies shared entities and modules
    3. Returns skeleton file metadata for Claude Code to create

    Args:
        llm: LLM model instance
        requirements: List of all requirement dicts
        architecture_contract: Architecture contract (optional)
        verbose: Whether to print debug info

    Returns:
        dict: {
            "skeleton_files": list of file metadata (path, description, shared_by, todo_sections),
            "file_ownership": mapping of req_id to owned files,
            "shared_files": list of files shared by multiple requirements,
            "analysis": shared entities and modules analysis
        }
    """
    from ._observability import get_logger

    logger = get_logger()

    # Skip if only one requirement
    if len(requirements) <= 1:
        if verbose:
            logger.debug("[SKELETON] Skipped: only one requirement")
        return {"skeleton_files": [], "file_ownership": {}, "shared_files": [], "analysis": {}}

    # Analyze skeleton requirements (metadata only, no content generation)
    if verbose:
        logger.info("[SKELETON] åˆ†æéœ€æ±‚ï¼Œè¯†åˆ«å…±äº«éª¨æ¶...")

    analysis = await analyze_skeleton_requirements(
        llm, requirements, architecture_contract, verbose=verbose
    )

    if not analysis or not analysis.get("skeleton_files"):
        if verbose:
            logger.debug("[SKELETON] No shared skeleton needed")
        return {"skeleton_files": [], "file_ownership": {}, "shared_files": [], "analysis": {}}

    # Extract skeleton file metadata (no file writing here)
    skeleton_files = analysis.get("skeleton_files", [])
    file_ownership = analysis.get("file_ownership", {})
    shared_files: list[str] = []

    # Collect shared files
    for file_spec in skeleton_files:
        file_path = file_spec.get("path", "")
        shared_by = file_spec.get("shared_by", [])
        if file_path and len(shared_by) > 1:
            shared_files.append(file_path)

    result = {
        "skeleton_files": skeleton_files,  # Metadata only: path, description, shared_by, todo_sections
        "file_ownership": file_ownership,
        "shared_files": shared_files,
        "analysis": analysis.get("analysis", {}),
    }

    if verbose:
        logger.info(f"[SKELETON] è¯†åˆ«äº† {len(skeleton_files)} ä¸ªéª¨æ¶æ–‡ä»¶, {len(shared_files)} ä¸ªå…±äº«æ–‡ä»¶")
        for sf in skeleton_files:
            logger.debug(f"[SKELETON]   - {sf.get('path')}: {sf.get('description', '')[:50]}")

    return result


# Keep old function name as alias for compatibility
async def generate_unified_skeleton(
    llm: Any,
    requirements: list[dict[str, Any]],
    workspace_dir: "Any",  # Path - now ignored
    architecture_contract: dict[str, Any] | None = None,
    runtime_workspace: "Any | None" = None,  # RuntimeWorkspace - now ignored
    *,
    verbose: bool = False,
) -> dict[str, Any]:
    """Deprecated: Use analyze_unified_skeleton instead.

    This function now only analyzes and returns metadata.
    workspace_dir and runtime_workspace parameters are ignored.
    """
    return await analyze_unified_skeleton(
        llm, requirements, architecture_contract, verbose=verbose
    )


def get_owned_files(
    file_ownership: dict[str, list[str]],
    requirement_id: str,
) -> list[str]:
    """Get list of files owned by a specific requirement.

    Args:
        file_ownership: Mapping of requirement ID to owned file paths
        requirement_id: The requirement ID to look up

    Returns:
        list: File paths owned by this requirement
    """
    return file_ownership.get(requirement_id, [])


def is_shared_file(
    shared_files: list[str],
    file_path: str,
) -> bool:
    """Check if a file is shared by multiple requirements.

    Args:
        shared_files: List of shared file paths
        file_path: File path to check

    Returns:
        bool: True if the file is shared
    """
    # Normalize paths for comparison
    normalized = file_path.lstrip("./")
    return normalized in shared_files or file_path in shared_files


__all__ = [
    "extract_file_structure",
    "find_similar_text",
    "validate_python_syntax",
    "apply_edits",
    "design_requirement",
    "implement_requirement",
    "implement_requirement_autonomous",
    "generate_single_file",
    "sort_files_by_dependency",
    "run_scaffold_commands",
    "stepwise_generate_files",
    "analyze_skeleton_requirements",
    "generate_unified_skeleton",
    "get_owned_files",
    "is_shared_file",
    # Claude Code configuration
    "USE_CLAUDE_CODE",
    "CLAUDE_CODE_MODE",
]
