# -*- coding: utf-8 -*-
"""File editing tools inspired by Claude Code's Edit mechanism.

This module provides:
- Precise string replacement (old_string -> new_string)
- File reading with caching
- Edit validation (uniqueness check)
- Incremental file modification

Key principles (from Claude Code):
1. MUST read file before editing
2. old_string must be unique in file
3. Preserve existing code, only modify what's needed
4. No full file rewrites - use precise edits
"""
from __future__ import annotations

import hashlib
import json
import textwrap
from pathlib import Path
from typing import Any, TYPE_CHECKING

if TYPE_CHECKING:
    from agentscope.ones.code_guard import CodeGuardManager


# ---------------------------------------------------------------------------
# File State Manager
# ---------------------------------------------------------------------------
class FileStateManager:
    """Manages file state across multiple requirements.

    Tracks which files have been read, modified, and by which requirement.
    Prevents accidental overwrites and enables incremental editing.
    """

    def __init__(self, workspace_dir: Path):
        """Initialize file state manager.

        Args:
            workspace_dir: Root directory for all files
        """
        self.workspace_dir = Path(workspace_dir)
        self._file_cache: dict[str, str] = {}  # path -> content
        self._file_hashes: dict[str, str] = {}  # path -> hash
        self._read_by: dict[str, set[str]] = {}  # path -> set of req_ids
        self._modified_by: dict[str, list[str]] = {}  # path -> list of req_ids (ordered)
        self._locked_files: set[str] = set()  # files locked after passing QA

    def read_file(self, file_path: str, req_id: str | None = None) -> str | None:
        """Read file content and cache it.

        Args:
            file_path: Relative path to file
            req_id: Requirement ID requesting the read

        Returns:
            str: File content, or None if file doesn't exist
        """
        full_path = self.workspace_dir / file_path

        # Return cached content if available and file unchanged
        if file_path in self._file_cache:
            if full_path.exists():
                current_hash = self._compute_hash(full_path.read_text(encoding="utf-8"))
                if current_hash == self._file_hashes.get(file_path):
                    if req_id:
                        self._read_by.setdefault(file_path, set()).add(req_id)
                    return self._file_cache[file_path]

        # Read from disk
        if not full_path.exists():
            return None

        content = full_path.read_text(encoding="utf-8")
        self._file_cache[file_path] = content
        self._file_hashes[file_path] = self._compute_hash(content)

        if req_id:
            self._read_by.setdefault(file_path, set()).add(req_id)

        return content

    def write_file(self, file_path: str, content: str, req_id: str | None = None) -> bool:
        """Write file content and update cache.

        Args:
            file_path: Relative path to file
            content: New file content
            req_id: Requirement ID making the write

        Returns:
            bool: True if write succeeded
        """
        if file_path in self._locked_files:
            from ._observability import get_logger
            get_logger().warn(f"[FileEditor] æ–‡ä»¶å·²é”å®šï¼Œè·³è¿‡å†™å…¥: {file_path}")
            return False

        full_path = self.workspace_dir / file_path
        full_path.parent.mkdir(parents=True, exist_ok=True)
        full_path.write_text(content, encoding="utf-8")

        # Update cache
        self._file_cache[file_path] = content
        self._file_hashes[file_path] = self._compute_hash(content)

        if req_id:
            self._modified_by.setdefault(file_path, []).append(req_id)

        return True

    def lock_file(self, file_path: str) -> None:
        """Lock a file to prevent further modifications.

        Args:
            file_path: Relative path to file
        """
        self._locked_files.add(file_path)

    def unlock_file(self, file_path: str) -> None:
        """Unlock a file to allow modifications.

        Args:
            file_path: Relative path to file
        """
        self._locked_files.discard(file_path)

    def is_locked(self, file_path: str) -> bool:
        """Check if file is locked.

        Args:
            file_path: Relative path to file

        Returns:
            bool: True if locked
        """
        return file_path in self._locked_files

    def refresh_file(self, file_path: str, req_id: str | None = None) -> str | None:
        """Force refresh file content from disk, bypassing cache.

        CRITICAL: Always call this before generating edits to avoid
        using stale content that may have been modified by other requirements.

        Args:
            file_path: Relative path to file
            req_id: Requirement ID requesting the refresh

        Returns:
            str: Fresh file content, or None if file doesn't exist
        """
        full_path = self.workspace_dir / file_path

        if not full_path.exists():
            # File doesn't exist - remove from cache if present
            self._file_cache.pop(file_path, None)
            self._file_hashes.pop(file_path, None)
            return None

        # Always read from disk, update cache
        content = full_path.read_text(encoding="utf-8")
        self._file_cache[file_path] = content
        self._file_hashes[file_path] = self._compute_hash(content)

        if req_id:
            self._read_by.setdefault(file_path, set()).add(req_id)

        return content

    def get_file_history(self, file_path: str) -> dict[str, Any]:
        """Get modification history for a file.

        Args:
            file_path: Relative path to file

        Returns:
            dict: History info with read_by and modified_by
        """
        return {
            "read_by": list(self._read_by.get(file_path, [])),
            "modified_by": self._modified_by.get(file_path, []),
            "is_locked": file_path in self._locked_files,
        }

    def list_directory(self, dir_path: str = "") -> list[str]:
        """List files in a directory (like tree command).

        Args:
            dir_path: Relative directory path (empty for root)

        Returns:
            list: List of file paths relative to workspace
        """
        target = self.workspace_dir / dir_path if dir_path else self.workspace_dir
        if not target.exists():
            return []

        files = []
        for p in sorted(target.rglob("*")):
            if p.is_file() and not any(part.startswith(".") for part in p.parts):
                files.append(str(p.relative_to(self.workspace_dir)))

        return files

    def tree(self, dir_path: str = "", max_depth: int = 3) -> str:
        """Generate tree-like directory listing.

        Args:
            dir_path: Relative directory path
            max_depth: Maximum depth to traverse

        Returns:
            str: Tree-formatted string
        """
        target = self.workspace_dir / dir_path if dir_path else self.workspace_dir
        if not target.exists():
            return f"(directory not found: {dir_path})"

        lines = [str(target.name) + "/"]
        self._tree_recursive(target, "", lines, max_depth, 0)
        return "\n".join(lines)

    def _tree_recursive(
        self,
        path: Path,
        prefix: str,
        lines: list[str],
        max_depth: int,
        current_depth: int
    ) -> None:
        """Recursive helper for tree generation."""
        if current_depth >= max_depth:
            return

        entries = sorted(path.iterdir(), key=lambda x: (not x.is_dir(), x.name))
        entries = [e for e in entries if not e.name.startswith(".")]

        for i, entry in enumerate(entries):
            is_last = i == len(entries) - 1
            connector = "â””â”€â”€ " if is_last else "â”œâ”€â”€ "

            if entry.is_dir():
                lines.append(f"{prefix}{connector}{entry.name}/")
                new_prefix = prefix + ("    " if is_last else "â”‚   ")
                self._tree_recursive(entry, new_prefix, lines, max_depth, current_depth + 1)
            else:
                lines.append(f"{prefix}{connector}{entry.name}")

    @staticmethod
    def _compute_hash(content: str) -> str:
        """Compute hash of content for change detection."""
        return hashlib.md5(content.encode("utf-8")).hexdigest()

    def get_available_modules(self, tech_stack: str = "") -> dict[str, list[str]]:
        """Extract available modules from project structure.

        Analyzes the workspace to find importable modules based on tech stack.

        Args:
            tech_stack: Technology stack hint (e.g., "python", "fastapi", "react")

        Returns:
            dict: Categorized modules {"python": [...], "javascript": [...]}
        """
        modules: dict[str, list[str]] = {"python": [], "javascript": []}
        tech_lower = tech_stack.lower() if tech_stack else ""

        # Detect project type from files
        is_python = (self.workspace_dir / "requirements.txt").exists() or \
                   (self.workspace_dir / "pyproject.toml").exists() or \
                   "python" in tech_lower or "fastapi" in tech_lower or "django" in tech_lower

        is_js = (self.workspace_dir / "package.json").exists() or \
               "react" in tech_lower or "vue" in tech_lower or "vite" in tech_lower

        if is_python:
            modules["python"] = self._scan_python_modules()

        if is_js:
            modules["javascript"] = self._scan_js_modules()

        return modules

    def _scan_python_modules(self) -> list[str]:
        """Scan for available Python modules in the workspace.

        Returns:
            list: Available Python import paths (e.g., ["app.models", "app.routers.users"])
        """
        python_modules = []
        # Common Python source directories
        src_dirs = ["app", "src", "backend/app", "backend/src", "."]

        for src_dir in src_dirs:
            src_path = self.workspace_dir / src_dir
            if not src_path.exists():
                continue

            for py_file in src_path.rglob("*.py"):
                # Skip __pycache__, venv, etc.
                if any(p.startswith((".", "__pycache__", "venv", "env", ".venv"))
                       for p in py_file.parts):
                    continue

                # Convert file path to module path
                try:
                    rel_path = py_file.relative_to(src_path)
                    # Remove .py extension
                    module_path = str(rel_path)[:-3].replace("/", ".").replace("\\", ".")
                    # Handle __init__.py
                    if module_path.endswith(".__init__"):
                        module_path = module_path[:-9]
                    elif module_path == "__init__":
                        continue

                    if src_dir not in [".", "src"]:
                        # Prepend directory name for subdirectories
                        base = src_dir.split("/")[-1]  # e.g., "app" from "backend/app"
                        if not module_path.startswith(base):
                            module_path = f"{base}.{module_path}" if module_path else base
                    elif src_dir == ".":
                        # Skip if it's just a standalone script
                        if "/" not in str(rel_path) and "\\" not in str(rel_path):
                            continue

                    if module_path and module_path not in python_modules:
                        python_modules.append(module_path)
                except ValueError:
                    continue

        return sorted(python_modules)

    def _scan_js_modules(self) -> list[str]:
        """Scan for available JavaScript/TypeScript modules in the workspace.

        Returns:
            list: Available JS/TS import paths (e.g., ["@/api/users", "@/utils/request"])
        """
        js_modules = []
        # Common frontend source directories
        src_dirs = ["src", "frontend/src", "client/src"]

        for src_dir in src_dirs:
            src_path = self.workspace_dir / src_dir
            if not src_path.exists():
                continue

            for ext in ["*.ts", "*.tsx", "*.js", "*.jsx", "*.vue"]:
                for file in src_path.rglob(ext):
                    # Skip node_modules, dist, etc.
                    if any(p.startswith((".", "node_modules", "dist", "build"))
                           for p in file.parts):
                        continue

                    try:
                        rel_path = file.relative_to(src_path)
                        # Remove extension
                        module_path = str(rel_path)
                        for suffix in [".tsx", ".ts", ".jsx", ".js", ".vue"]:
                            if module_path.endswith(suffix):
                                module_path = module_path[:-len(suffix)]
                                break

                        # Convert to @ alias format (common convention)
                        module_path = "@/" + module_path.replace("\\", "/")

                        # Skip index files (they can be imported by directory)
                        if module_path.endswith("/index"):
                            module_path = module_path[:-6]

                        if module_path and module_path not in js_modules:
                            js_modules.append(module_path)
                    except ValueError:
                        continue

        return sorted(js_modules)


def format_available_modules(modules: dict[str, list[str]]) -> str:
    """Format available modules for inclusion in LLM prompt.

    Args:
        modules: Dict of categorized modules from get_available_modules()

    Returns:
        str: Formatted module list for prompt
    """
    sections = []

    if modules.get("python"):
        py_mods = modules["python"]
        sections.append("ã€Python å¯ç”¨æ¨¡å—ã€‘\n" + "\n".join(f"  - {m}" for m in py_mods[:50]))
        if len(py_mods) > 50:
            sections[-1] += f"\n  ... å…± {len(py_mods)} ä¸ªæ¨¡å—"

    if modules.get("javascript"):
        js_mods = modules["javascript"]
        sections.append("ã€JavaScript/TypeScript å¯ç”¨æ¨¡å—ã€‘\n" + "\n".join(f"  - {m}" for m in js_mods[:50]))
        if len(js_mods) > 50:
            sections[-1] += f"\n  ... å…± {len(js_mods)} ä¸ªæ¨¡å—"

    return "\n\n".join(sections) if sections else ""


# ---------------------------------------------------------------------------
# Edit Operations
# ---------------------------------------------------------------------------
class EditOperation:
    """Represents a single edit operation."""

    def __init__(self, old_string: str, new_string: str, replace_all: bool = False):
        """Initialize edit operation.

        Args:
            old_string: Text to find and replace
            new_string: Replacement text
            replace_all: If True, replace all occurrences
        """
        self.old_string = old_string
        self.new_string = new_string
        self.replace_all = replace_all

    def validate(self, content: str) -> tuple[bool, str]:
        """Validate that edit can be applied.

        Args:
            content: File content to check against

        Returns:
            tuple: (is_valid, error_message)
        """
        if not self.old_string:
            return False, "old_string cannot be empty"

        count = content.count(self.old_string)

        if count == 0:
            return False, f"old_string not found in file"

        if count > 1 and not self.replace_all:
            return False, f"old_string appears {count} times (must be unique or use replace_all)"

        return True, ""

    def apply(self, content: str) -> str:
        """Apply edit to content.

        Args:
            content: Original file content

        Returns:
            str: Modified content
        """
        if self.replace_all:
            return content.replace(self.old_string, self.new_string)
        else:
            return content.replace(self.old_string, self.new_string, 1)


def apply_edits(content: str, edits: list[EditOperation]) -> tuple[str, list[str]]:
    """Apply multiple edits to content.

    Args:
        content: Original file content
        edits: List of edit operations

    Returns:
        tuple: (modified_content, list_of_errors)
    """
    errors = []
    result = content

    for i, edit in enumerate(edits):
        is_valid, error = edit.validate(result)
        if not is_valid:
            errors.append(f"Edit {i + 1}: {error}")
            continue

        result = edit.apply(result)

    return result, errors


# ---------------------------------------------------------------------------
# LLM-Driven Edit Generation
# ---------------------------------------------------------------------------
async def generate_file_edits(
    llm: Any,
    file_path: str,
    existing_content: str,
    requirement: dict[str, Any],
    criteria: list[dict[str, Any]] | None = None,
    failed_criteria: list[dict[str, Any]] | None = None,
    file_manager: "FileStateManager | None" = None,
    blueprint: dict[str, Any] | None = None,
    *,
    verbose: bool = False,
) -> list[dict[str, Any]]:
    """Generate edit operations for a file using LLM.

    Instead of generating entire file, generate precise edits.

    Args:
        llm: LLM model instance
        file_path: Path to file being edited
        existing_content: Current file content
        requirement: Requirement dict
        criteria: Acceptance criteria to implement
        failed_criteria: Previously failed criteria to fix
        file_manager: File state manager for scanning available modules
        blueprint: Blueprint dict with tech stack info
        verbose: Whether to print debug info

    Returns:
        list: List of edit dicts with 'old', 'new', 'replace_all' keys
    """
    from ._llm_utils import call_llm_json

    # Build criteria context
    criteria_text = ""
    if criteria:
        criteria_text = "\nã€éœ€è¦å®ç°çš„éªŒæ”¶æ ‡å‡†ã€‘\n"
        for c in criteria:
            criteria_text += f"- {c.get('id', '')}: {c.get('title', c.get('name', ''))}\n"

    if failed_criteria:
        criteria_text += "\nã€ä¸Šè½®å¤±è´¥çš„æ ‡å‡† - å¿…é¡»ä¿®å¤ã€‘\n"
        for c in failed_criteria:
            crit_id = c.get('id', '')
            crit_title = c.get('title', c.get('name', ''))
            # Use enhanced QA failure reason if available
            failure_reason = c.get('qa_failure_reason') or c.get('reason', 'æœªçŸ¥åŸå› ')
            recommendation = c.get('recommendation', '')
            criteria_text += f"- {crit_id}: {crit_title}\n"
            criteria_text += f"  âš ï¸ å¤±è´¥åŸå› : {failure_reason}\n"
            if recommendation:
                criteria_text += f"  ğŸ’¡ ä¿®å¤å»ºè®®: {recommendation}\n"

    # Build available modules text (CRITICAL: prevents importing non-existent modules)
    modules_text = ""
    if file_manager:
        tech_stack = blueprint.get('recommended_stack', '') if blueprint else ''
        available_modules = file_manager.get_available_modules(tech_stack)
        modules_text = format_available_modules(available_modules)

    prompt = textwrap.dedent(f"""
        ã€ä»»åŠ¡ã€‘ç¼–è¾‘æ–‡ä»¶: {file_path}

        ã€å½“å‰æ–‡ä»¶å†…å®¹ã€‘
        ```
        {existing_content[:8000]}
        ```

        ã€éœ€æ±‚ã€‘
        {requirement.get('title', '')}: {requirement.get('summary', requirement.get('description', ''))}
        {criteria_text}
        {modules_text}

        ã€è¾“å‡ºæ ¼å¼ã€‘
        è¿”å› JSON æ ¼å¼çš„ç¼–è¾‘æŒ‡ä»¤åˆ—è¡¨:
        ```json
        {{
            "edits": [
                {{
                    "old": "è¦æ›¿æ¢çš„åŸæ–‡æœ¬ï¼ˆå¿…é¡»åœ¨æ–‡ä»¶ä¸­èƒ½æ‰¾åˆ°ï¼‰",
                    "new": "æ›¿æ¢åçš„æ–°æ–‡æœ¬",
                    "replace_all": false,
                    "reason": "ä¸ºä»€ä¹ˆè¦åšè¿™ä¸ªä¿®æ”¹"
                }}
            ],
            "summary": "æœ¬æ¬¡ç¼–è¾‘çš„æ€»ç»“"
        }}
        ```

        ã€å…³é”®è§„åˆ™ - å¿…é¡»éµå®ˆã€‘
        1. old å­—æ®µå¿…é¡»æ˜¯æ–‡ä»¶ä¸­**å®é™…å­˜åœ¨**çš„æ–‡æœ¬ï¼Œå®Œå…¨åŒ¹é…
        2. old å­—æ®µåº”è¯¥è¶³å¤Ÿé•¿ä»¥ç¡®ä¿**å”¯ä¸€æ€§**ï¼ˆè‡³å°‘åŒ…å«ä¸€æ•´è¡Œï¼‰
        3. åªä¿®æ”¹éœ€è¦æ”¹çš„éƒ¨åˆ†ï¼Œ**ä¿ç•™å…¶ä»–åŠŸèƒ½ä»£ç **
        4. å¦‚æœéœ€è¦æ·»åŠ æ–°ä»£ç ï¼Œæ‰¾åˆ°åˆé€‚çš„ä½ç½®ï¼ˆå¦‚å‡½æ•°æœ«å°¾ã€import åŒºåŸŸï¼‰
        5. ä¸è¦åˆ é™¤å…¶ä»–éœ€æ±‚å·²å®ç°çš„åŠŸèƒ½
        6. å¦‚æœæ–‡ä»¶æ˜¯ç©ºçš„æˆ–éœ€è¦åˆ›å»ºæ–°å†…å®¹ï¼Œä½¿ç”¨ old: "" å’Œ new: "å®Œæ•´å†…å®¹"

        ã€æ¨¡å—å¯¼å…¥çº¦æŸ - ä¸¥æ ¼éµå®ˆã€‘
        - åªèƒ½å¯¼å…¥ä¸Šè¿°"å¯ç”¨æ¨¡å—"åˆ—è¡¨ä¸­å­˜åœ¨çš„æœ¬åœ°æ¨¡å—
        - ç¦æ­¢å¯¼å…¥ä¸å­˜åœ¨çš„æ¨¡å—ï¼ˆå¦‚ app.apiã€app.core ç­‰å‡è®¾çš„æ¨¡å—ï¼‰
        - ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ä¸å—æ­¤é™åˆ¶ï¼ˆå¦‚ fastapiã€sqlalchemy ç­‰ï¼‰
        - å¦‚æœéœ€è¦çš„æ¨¡å—ä¸å­˜åœ¨ï¼Œåœ¨æ–‡ä»¶ä¸­å®šä¹‰æ‰€éœ€åŠŸèƒ½æˆ–æ·»åŠ  TODO æ³¨é‡Š

        ã€æ¥å£è°ƒç”¨çº¦æŸ - ç¦æ­¢çŒœæµ‹ã€‘
        - åªèƒ½è°ƒç”¨åœ¨"å½“å‰æ–‡ä»¶å†…å®¹"æˆ–"ç›¸å…³æ–‡ä»¶"ä¸­æ˜ç¡®å®šä¹‰çš„å‡½æ•°/ç±»/æ–¹æ³•
        - å¦‚æœéœ€è¦è°ƒç”¨æœªå®šä¹‰çš„æ¥å£ï¼Œå¿…é¡»æ·»åŠ  TODO æ³¨é‡Šè¯´æ˜éœ€è¦ä»€ä¹ˆæ¥å£
        - ç¤ºä¾‹: // TODO: éœ€è¦ @/api/users æä¾› getUserById(id): Promise<User>

        ã€æ­£ç¡®ç¤ºä¾‹ã€‘
        old: "def get_member(db, member_id):\\n    pass  # TODO"
        new: "def get_member(db, member_id):\\n    return db.query(Member).filter(Member.id == member_id).first()"

        ã€é”™è¯¯ç¤ºä¾‹ã€‘
        old: "pass" (å¤ªçŸ­ï¼Œå¯èƒ½åŒ¹é…å¤šå¤„)
        old: "# TODO" (å¤ªçŸ­ï¼Œä¸å”¯ä¸€)
    """)

    result, _ = await call_llm_json(
        llm,
        [
            {"role": "system", "content": "ä½ æ˜¯ä»£ç ç¼–è¾‘ä¸“å®¶ï¼Œç”Ÿæˆç²¾ç¡®çš„æ–‡ä»¶ç¼–è¾‘æŒ‡ä»¤ã€‚"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.2,
        label=f"edit_file:{file_path}",
        verbose=verbose,
    )

    return result.get("edits", [])


async def generate_new_file(
    llm: Any,
    file_path: str,
    requirement: dict[str, Any],
    blueprint: dict[str, Any],
    criteria: list[dict[str, Any]] | None = None,
    context_files: dict[str, str] | None = None,
    file_manager: "FileStateManager | None" = None,
    *,
    verbose: bool = False,
) -> str:
    """Generate a new file from scratch.

    Only used when file doesn't exist yet.

    Args:
        llm: LLM model instance
        file_path: Path to new file
        requirement: Requirement dict
        blueprint: Blueprint dict with file description
        criteria: Acceptance criteria to implement
        context_files: Related files for context
        file_manager: File state manager for scanning available modules
        verbose: Whether to print debug info

    Returns:
        str: New file content
    """
    from ._llm_utils import call_llm_json

    # Get file description from blueprint
    file_desc = ""
    for fp in blueprint.get("files_plan", []):
        if fp.get("path") == file_path:
            file_desc = fp.get("description", "")
            break

    # Build context from related files
    context_text = ""
    if context_files:
        context_text = "\nã€ç›¸å…³æ–‡ä»¶å†…å®¹ï¼ˆä¾›å‚è€ƒï¼‰ã€‘\n"
        for path, content in list(context_files.items())[:5]:
            context_text += f"\n--- {path} ---\n{content[:2000]}\n"

    # Build criteria text
    criteria_text = ""
    if criteria:
        criteria_text = "\nã€æ­¤æ–‡ä»¶éœ€å®ç°çš„éªŒæ”¶æ ‡å‡†ã€‘\n"
        for c in criteria:
            criteria_text += f"- {c.get('id', '')}: {c.get('title', c.get('name', ''))}\n"
            if c.get('description'):
                criteria_text += f"  è¦æ±‚: {c.get('description')}\n"

    # Build available modules text (CRITICAL: prevents importing non-existent modules)
    modules_text = ""
    if file_manager:
        tech_stack = blueprint.get('recommended_stack', '')
        available_modules = file_manager.get_available_modules(tech_stack)
        modules_text = format_available_modules(available_modules)

    prompt = textwrap.dedent(f"""
        ã€ä»»åŠ¡ã€‘åˆ›å»ºæ–°æ–‡ä»¶: {file_path}

        ã€æ–‡ä»¶æè¿°ã€‘
        {file_desc}

        ã€éœ€æ±‚ã€‘
        {requirement.get('title', '')}: {requirement.get('summary', requirement.get('description', ''))}

        ã€æŠ€æœ¯æ ˆã€‘
        {blueprint.get('recommended_stack', '')}
        {context_text}
        {criteria_text}
        {modules_text}

        ã€è¾“å‡ºæ ¼å¼ã€‘
        è¿”å› JSON:
        ```json
        {{
            "content": "å®Œæ•´çš„æ–‡ä»¶å†…å®¹",
            "summary": "æ–‡ä»¶åŠŸèƒ½è¯´æ˜"
        }}
        ```

        ã€è¦æ±‚ã€‘
        1. ç”Ÿæˆå®Œæ•´ã€å¯è¿è¡Œçš„ä»£ç 
        2. éµå¾ªé¡¹ç›®å·²æœ‰çš„ä»£ç é£æ ¼
        3. å®ç°æ‰€æœ‰æŒ‡å®šçš„éªŒæ”¶æ ‡å‡†

        ã€æ¨¡å—å¯¼å…¥çº¦æŸ - ä¸¥æ ¼éµå®ˆã€‘
        - åªèƒ½å¯¼å…¥ä¸Šè¿°"å¯ç”¨æ¨¡å—"åˆ—è¡¨ä¸­å­˜åœ¨çš„æœ¬åœ°æ¨¡å—
        - ç¦æ­¢å¯¼å…¥ä¸å­˜åœ¨çš„æ¨¡å—ï¼ˆå¦‚ app.apiã€app.core ç­‰å‡è®¾çš„æ¨¡å—ï¼‰
        - ç¬¬ä¸‰æ–¹åº“å¯¼å…¥ä¸å—æ­¤é™åˆ¶ï¼ˆå¦‚ fastapiã€sqlalchemy ç­‰ï¼‰
        - å¦‚æœéœ€è¦çš„æ¨¡å—ä¸å­˜åœ¨ï¼Œåœ¨æ–‡ä»¶ä¸­å®šä¹‰æ‰€éœ€åŠŸèƒ½æˆ–æ·»åŠ  TODO æ³¨é‡Š

        ã€æ¥å£è°ƒç”¨çº¦æŸ - ç¦æ­¢çŒœæµ‹ã€‘
        - åªèƒ½è°ƒç”¨åœ¨"ç›¸å…³æ–‡ä»¶å†…å®¹"ä¸­æ˜ç¡®å®šä¹‰çš„å‡½æ•°/ç±»/æ–¹æ³•
        - å¦‚æœéœ€è¦è°ƒç”¨çš„æ¥å£ä¸åœ¨ä¸Šä¸‹æ–‡ä¸­ï¼Œå¿…é¡»æ·»åŠ  TODO æ³¨é‡Šè¯´æ˜éœ€è¦ä»€ä¹ˆæ¥å£
        - TODO æ ¼å¼: // TODO: éœ€è¦ [æ¨¡å—è·¯å¾„] æä¾› [æ¥å£ç­¾å]
        - ç¦æ­¢å‡è®¾æ¥å£çš„å‚æ•°ç±»å‹ã€è¿”å›å€¼ç±»å‹ã€åŒæ­¥/å¼‚æ­¥è¡Œä¸º
    """)

    result, _ = await call_llm_json(
        llm,
        [
            {"role": "system", "content": "ä½ æ˜¯ä»£ç ç”Ÿæˆä¸“å®¶ï¼Œç”Ÿæˆé«˜è´¨é‡çš„ä»£ç æ–‡ä»¶ã€‚"},
            {"role": "user", "content": prompt},
        ],
        temperature=0.25,
        label=f"new_file:{file_path}",
        verbose=verbose,
    )

    return result.get("content", "")


# ---------------------------------------------------------------------------
# High-Level Edit Flow
# ---------------------------------------------------------------------------
async def edit_or_create_file(
    llm: Any,
    file_path: str,
    requirement: dict[str, Any],
    blueprint: dict[str, Any],
    file_manager: FileStateManager,
    criteria: list[dict[str, Any]] | None = None,
    failed_criteria: list[dict[str, Any]] | None = None,
    *,
    code_guard: "CodeGuardManager | None" = None,
    verbose: bool = False,
) -> dict[str, Any]:
    """Edit existing file or create new one.

    Main entry point for file modifications. Follows Claude Code pattern:
    1. Read file first (or determine it's new)
    2. Generate edits (or full content for new files)
    3. Apply and validate edits
    4. Write result

    Args:
        llm: LLM model instance
        file_path: Path to file
        requirement: Requirement dict
        blueprint: Blueprint dict
        file_manager: FileStateManager instance
        criteria: Acceptance criteria to implement
        failed_criteria: Previously failed criteria
        code_guard: CodeGuardManager instance for anti-hallucination validation
        verbose: Whether to print debug info

    Returns:
        dict: Result with 'path', 'content', 'action', 'edits_applied', 'errors'
    """
    from ._observability import get_logger
    logger = get_logger()

    req_id = requirement.get("id", "unknown")
    result = {
        "path": file_path,
        "content": "",
        "action": "create",
        "edits_applied": 0,
        "errors": [],
        "summary": "",
        "code_guard_warnings": "",  # CodeGuard validation warnings
    }

    # Check if file is locked
    if file_manager.is_locked(file_path):
        result["errors"].append(f"æ–‡ä»¶å·²é”å®š: {file_path}")
        result["content"] = file_manager.read_file(file_path, req_id) or ""
        return result

    # Step 1: Read existing file (ALWAYS refresh from disk before edit)
    # This is CRITICAL to avoid edit conflicts when files are modified by other requirements
    existing_content = file_manager.refresh_file(file_path, req_id)

    # CodeGuard: Record the file read
    if code_guard and existing_content is not None:
        code_guard.record_file_read(file_path, existing_content)

    if existing_content is None:
        # New file - generate from scratch
        result["action"] = "create"
        if verbose:
            logger.debug(f"[FileEditor] åˆ›å»ºæ–°æ–‡ä»¶: {file_path}")

        # Get context from related files
        context_files = {}
        for fp in blueprint.get("files_plan", []):
            dep_path = fp.get("path", "")
            if dep_path != file_path:
                dep_content = file_manager.read_file(dep_path, req_id)
                if dep_content:
                    context_files[dep_path] = dep_content

        content = await generate_new_file(
            llm=llm,
            file_path=file_path,
            requirement=requirement,
            blueprint=blueprint,
            criteria=criteria,
            context_files=context_files if context_files else None,
            file_manager=file_manager,
            verbose=verbose,
        )

        result["content"] = content
        result["summary"] = f"åˆ›å»ºæ–°æ–‡ä»¶ {file_path}"

    else:
        # Existing file - generate edits
        result["action"] = "edit"
        if verbose:
            logger.debug(f"[FileEditor] ç¼–è¾‘ç°æœ‰æ–‡ä»¶: {file_path} ({len(existing_content)} å­—ç¬¦)")

        edits_data = await generate_file_edits(
            llm=llm,
            file_path=file_path,
            existing_content=existing_content,
            requirement=requirement,
            criteria=criteria,
            failed_criteria=failed_criteria,
            file_manager=file_manager,
            blueprint=blueprint,
            verbose=verbose,
        )

        # Convert to EditOperation objects
        edits = []
        for ed in edits_data:
            edits.append(EditOperation(
                old_string=ed.get("old", ""),
                new_string=ed.get("new", ""),
                replace_all=ed.get("replace_all", False),
            ))

        # Apply edits
        modified_content, errors = apply_edits(existing_content, edits)

        result["content"] = modified_content
        result["edits_applied"] = len(edits) - len(errors)
        result["errors"].extend(errors)
        result["summary"] = f"åº”ç”¨ {result['edits_applied']}/{len(edits)} ä¸ªç¼–è¾‘"

        if errors and verbose:
            for err in errors:
                logger.warn(f"[FileEditor] ç¼–è¾‘å¤±è´¥: {err}")

    # Step 2.5: CodeGuard validation (after content is generated, before writing)
    if code_guard and result["content"]:
        is_new_file = existing_content is None
        validation = code_guard.validate_content(
            file_path,
            result["content"],
            is_new_file=is_new_file,
        )

        # Format warnings
        warnings_text = code_guard.format_warnings(validation)
        if warnings_text:
            result["code_guard_warnings"] = warnings_text
            if verbose:
                logger.warn(f"[FileEditor] CodeGuard æ£€æµ‹åˆ°é—®é¢˜:\n{warnings_text}")

        # Check if write should be blocked
        if code_guard.should_block_write(validation):
            result["errors"].append("CodeGuard blocked write: file was not read first")
            if verbose:
                logger.error(f"[FileEditor] CodeGuard é˜»æ­¢å†™å…¥: {file_path}")
            return result

    # Step 3: Write result
    if result["content"]:
        success = file_manager.write_file(file_path, result["content"], req_id)
        if not success:
            result["errors"].append(f"å†™å…¥å¤±è´¥: {file_path}")
        else:
            # CodeGuard: Update registries after successful write
            if code_guard:
                code_guard.update_after_write(file_path, result["content"])

    return result


# ---------------------------------------------------------------------------
# Integration Helper
# ---------------------------------------------------------------------------
async def stepwise_edit_files(
    llm: Any,
    requirement: dict[str, Any],
    blueprint: dict[str, Any],
    file_manager: FileStateManager,
    all_criteria: list[dict[str, Any]] | None = None,
    failed_criteria: list[dict[str, Any]] | None = None,
    *,
    code_guard: "CodeGuardManager | None" = None,
    verbose: bool = False,
) -> dict[str, Any]:
    """Edit files step by step, reading before each edit.

    Replacement for stepwise_generate_files that uses edit mode.

    Args:
        llm: LLM model instance
        requirement: Requirement dict
        blueprint: Blueprint dict
        file_manager: FileStateManager instance
        all_criteria: All acceptance criteria for requirement
        failed_criteria: Failed criteria from previous round
        code_guard: CodeGuardManager instance for anti-hallucination validation
        verbose: Whether to print debug info

    Returns:
        dict: Implementation result with files list
    """
    from ._observability import get_logger
    logger = get_logger()

    files_plan = blueprint.get("files_plan", [])
    if not files_plan:
        raise ValueError("Blueprint ç¼ºå°‘ files_plan å­—æ®µ")

    req_id = requirement.get("id", "")

    # Log directory structure first (like tree command)
    if verbose:
        tree_output = file_manager.tree(max_depth=3)
        logger.debug(f"[FileEditor] å½“å‰ç›®å½•ç»“æ„:\n{tree_output}")

    # Build criteria lookup
    criteria_by_id: dict[str, dict[str, Any]] = {}
    if all_criteria:
        for c in all_criteria:
            cid = c.get("id", "")
            if cid:
                criteria_by_id[cid] = c

    results = []
    summaries = []

    for i, file_plan in enumerate(files_plan):
        file_path = file_plan["path"]

        # Get criteria for this file
        file_criteria_ids = file_plan.get("criteria_ids", [])
        file_criteria = [criteria_by_id[cid] for cid in file_criteria_ids if cid in criteria_by_id]

        # Use failed_criteria if no specific criteria
        if not file_criteria and failed_criteria:
            file_criteria = failed_criteria

        logger.info(f"[FileEditor] ({i + 1}/{len(files_plan)}) å¤„ç†: {file_path}")

        result = await edit_or_create_file(
            llm=llm,
            file_path=file_path,
            requirement=requirement,
            blueprint=blueprint,
            file_manager=file_manager,
            criteria=file_criteria if file_criteria else None,
            failed_criteria=failed_criteria,
            code_guard=code_guard,
            verbose=verbose,
        )

        results.append(result)
        action_text = "åˆ›å»º" if result["action"] == "create" else f"ç¼–è¾‘({result['edits_applied']}å¤„)"
        summaries.append(f"- {file_path}: {action_text} - {result.get('summary', '')[:80]}")

        if result["errors"]:
            for err in result["errors"]:
                logger.warn(f"[FileEditor] {file_path}: {err}")

    # Collect all files
    all_files = []
    code_guard_warnings_list = []
    for r in results:
        if r["content"]:
            all_files.append({"path": r["path"], "content": r["content"]})
        # Collect CodeGuard warnings
        if r.get("code_guard_warnings"):
            code_guard_warnings_list.append(f"[{r['path']}]\n{r['code_guard_warnings']}")

    result_dict = {
        "summary": f"[{req_id}] å¤„ç†äº† {len(all_files)} ä¸ªæ–‡ä»¶:\n" + "\n".join(summaries),
        "project_type": blueprint.get("artifact_type", "fullstack"),
        "files": all_files,
        "edit_results": results,
        "setup_commands": [],
        "run_command": "",
        "entry_point": "",
    }

    # Add CodeGuard warnings summary if any
    if code_guard_warnings_list:
        result_dict["code_guard_warnings"] = "\n\n".join(code_guard_warnings_list)

    return result_dict


__all__ = [
    "FileStateManager",
    "EditOperation",
    "apply_edits",
    "generate_file_edits",
    "generate_new_file",
    "edit_or_create_file",
    "stepwise_edit_files",
]
